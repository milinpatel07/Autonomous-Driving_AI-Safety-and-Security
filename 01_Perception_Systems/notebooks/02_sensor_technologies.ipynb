{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright (c) 2025 Milin Patel\n",
    "Hochschule Kempten - University of Applied Sciences\n",
    "\n",
    "Autonomous Driving: AI Safety and Security Workshop\n",
    "This project is licensed under the MIT License.\n",
    "See LICENSE file in the root directory for full license text.\n",
    "-->\n",
    "\n",
    "*Copyright ¬© 2025 Milin Patel. All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Sensor Modalities in Autonomous Vehicles\n",
    "\n",
    "**Author:** Milin Patel \n",
    "**Institution:** Hochschule Kempten\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/master/01_Perception_Systems/notebooks/02_sensor_technologies.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand different sensor modalities (Camera, LiDAR, Radar)\n",
    "- ‚úÖ Compare sensor capabilities and limitations\n",
    "- ‚úÖ Visualize 3D LiDAR point clouds\n",
    "- ‚úÖ Understand why sensor fusion is necessary\n",
    "- ‚úÖ Analyze sensor performance in different conditions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Google Colab / Local)\n",
    "\n",
    "Run this cell to install dependencies if running on Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\n\n# Check if running on Google Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"üîß Running on Google Colab - Installing dependencies...\\n\")\n    !pip install -q opencv-python matplotlib numpy open3d plotly scipy\n    \n    # Clone repository for scripts\n    !git clone -q https://github.com/milinpatel07/Autonomous-Driving_AI-Safety-and-Security.git\n    sys.path.insert(0, '/content/Autonomous-Driving_AI-Safety-and-Security/AV_Perception_Safety_Workshop/Session_1_AI_Perception_Systems')\n    print(\"‚úÖ Setup complete!\\n\")\nelse:\n    print(\"üíª Running locally\\n\")\n    sys.path.insert(0, '..')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom mpl_toolkits.mplot3d import Axes3D\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import workshop utilities\nimport sys\nimport os\n\n# Add scripts directory to path\nif IN_COLAB:\n    scripts_path = '/content/Autonomous-Driving_AI-Safety-and-Security/AV_Perception_Safety_Workshop/Session_1_AI_Perception_Systems/scripts'\nelse:\n    scripts_path = os.path.join(os.path.dirname(os.getcwd()), 'scripts') if 'notebooks' in os.getcwd() else 'scripts'\n\nif os.path.exists(scripts_path):\n    sys.path.insert(0, scripts_path)\n\ntry:\n    from sensor_visualization import (\n        visualize_pointcloud,\n        visualize_sensor_comparison_table,\n        load_sample_pointcloud,\n        create_birds_eye_view\n    )\n    from dataset_loader import SyntheticDataGenerator\n    print(\"‚úÖ Workshop utilities loaded successfully!\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Could not import utilities: {e}\")\n    print(\"‚ö†Ô∏è Falling back to inline implementations...\")\n\n    # Fallback: Define functions inline\n    def visualize_sensor_comparison_table():\n        \"\"\"Display sensor comparison table\"\"\"\n        data = [\n            ['Sensor', 'Range', 'Resolution', 'Weather', 'Cost', 'Use Case'],\n            ['Camera', '100m', 'High (2MP+)', '‚ùå Poor (rain/fog)', 'üí∞ Low', 'Classification, signs'],\n            ['LiDAR', '200m', 'High (0.1¬∞)', '‚ö†Ô∏è Medium (fog)', 'üí∞üí∞üí∞ High', '3D detection, mapping'],\n            ['Radar', '250m', 'Low', '‚úÖ Excellent', 'üí∞üí∞ Medium', 'Speed, all-weather']\n        ]\n\n        fig, ax = plt.subplots(figsize=(14, 4))\n        ax.axis('tight')\n        ax.axis('off')\n\n        table = ax.table(cellText=data, cellLoc='left', loc='center',\n                        colWidths=[0.12, 0.12, 0.18, 0.22, 0.16, 0.20])\n        table.auto_set_font_size(False)\n        table.set_fontsize(11)\n        table.scale(1, 2.5)\n\n        for i in range(6):\n            cell = table[(0, i)]\n            cell.set_facecolor('#4CAF50')\n            cell.set_text_props(weight='bold', color='white')\n\n        for i in range(1, 4):\n            for j in range(6):\n                cell = table[(i, j)]\n                cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')\n\n        plt.title('üìä Autonomous Vehicle Sensor Comparison',\n                 fontsize=16, fontweight='bold', pad=20)\n        plt.tight_layout()\n        plt.show()\n\n    def load_sample_pointcloud(sample_type='urban'):\n        \"\"\"Generate synthetic sample point cloud\"\"\"\n        if sample_type == 'urban':\n            ground = np.random.uniform([-30, 0, -1.8], [30, 50, -1.5], (5000, 3))\n            buildings_left = np.random.uniform([-30, 5, -1.5], [-15, 40, 5], (3000, 3))\n            buildings_right = np.random.uniform([15, 5, -1.5], [30, 40, 5], (3000, 3))\n            cars = np.random.uniform([-10, 10, -1.5], [10, 30, 1], (1000, 3))\n            points = np.vstack([ground, buildings_left, buildings_right, cars])\n        else:\n            points = np.random.uniform([-30, 0, -2], [30, 50, 5], (10000, 3))\n        return points\n\n    def create_birds_eye_view(points, image_size=(512, 512), x_range=(-40, 40),\n                             y_range=(0, 80), z_range=(-3, 3)):\n        \"\"\"Create bird's eye view from point cloud\"\"\"\n        h, w = image_size\n        mask = (points[:, 2] >= z_range[0]) & (points[:, 2] <= z_range[1])\n        points_filtered = points[mask]\n\n        x_img = (points_filtered[:, 0] - x_range[0]) / (x_range[1] - x_range[0]) * w\n        y_img = (points_filtered[:, 1] - y_range[0]) / (y_range[1] - y_range[0]) * h\n\n        valid = (x_img >= 0) & (x_img < w) & (y_img >= 0) & (y_img < h)\n        x_img = x_img[valid].astype(int)\n        y_img = y_img[valid].astype(int)\n\n        bev = np.zeros((h, w), dtype=np.float32)\n        for x, y in zip(x_img, y_img):\n            bev[h - 1 - y, x] += 1\n\n        if bev.max() > 0:\n            bev = np.clip(bev / bev.max(), 0, 1)\n        return bev\n\n    class SyntheticDataGenerator:\n        \"\"\"Simple synthetic data generator\"\"\"\n        def generate_sample_image(self, scene_type='urban', size=(1242, 375)):\n            h, w = size[1], size[0]\n            image = np.zeros((h, w, 3), dtype=np.uint8)\n            image[:h//3] = [135, 206, 235]  # Sky\n            image[h//3:] = [105, 105, 105]  # Road\n            for _ in range(np.random.randint(3, 8)):\n                x, y = np.random.randint(50, w-50), np.random.randint(h//3, h-50)\n                cv2.rectangle(image, (x, y), (x+80, y+100), (np.random.randint(0, 255),\n                             np.random.randint(0, 255), np.random.randint(0, 255)), -1)\n            return image\n\n    print(\"‚úÖ Fallback implementations loaded!\")\n\n# Set matplotlib style\nplt.style.use('default')\n%matplotlib inline\n\nprint(\"\\n‚úÖ All libraries imported!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Sensor Comparison Table\n",
    "\n",
    "Let's start by comparing the three main sensor modalities used in autonomous vehicles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive sensor comparison\n",
    "visualize_sensor_comparison_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì∑ Camera Sensors\n",
    "\n",
    "**Advantages:**\n",
    "- High resolution (2MP to 8MP+)\n",
    "- Rich semantic information (colors, textures, signs)\n",
    "- Low cost (~$50-500)\n",
    "- Excellent for object classification\n",
    "\n",
    "**Disadvantages:**\n",
    "- Poor performance in bad weather (rain, fog, darkness)\n",
    "- No direct 3D information\n",
    "- Affected by lighting conditions\n",
    "- Limited depth perception\n",
    "\n",
    "### LiDAR (Light Detection and Ranging)\n",
    "\n",
    "**Advantages:**\n",
    "- Accurate 3D measurements (cm-level precision)\n",
    "- Long range (up to 200m)\n",
    "- Works in darkness\n",
    "- High angular resolution (0.1-0.2¬∞)\n",
    "\n",
    "**Disadvantages:**\n",
    "- Very expensive ($1,000-$75,000)\n",
    "- Affected by fog and heavy rain\n",
    "- No color information\n",
    "- Moving parts (mechanical scanners)\n",
    "\n",
    "### Radar (Radio Detection and Ranging)\n",
    "\n",
    "**Advantages:**\n",
    "- All-weather operation (rain, fog, snow)\n",
    "- Direct velocity measurement (Doppler)\n",
    "- Long range (250m+)\n",
    "- Moderate cost ($100-1,000)\n",
    "\n",
    "**Disadvantages:**\n",
    "- Low angular resolution\n",
    "- Cannot classify objects well\n",
    "- Ghost reflections\n",
    "- No height information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: 3D LiDAR Point Cloud Visualization\n",
    "\n",
    "Let's visualize what LiDAR \"sees\" - a 3D point cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic urban scene point cloud\n",
    "print(\"üìä Generating synthetic urban scene...\")\n",
    "points = load_sample_pointcloud('urban')\n",
    "\n",
    "print(f\"‚úÖ Generated {len(points):,} points\")\n",
    "print(f\"   Point cloud shape: {points.shape}\")\n",
    "print(f\"   X range: [{points[:, 0].min():.1f}, {points[:, 0].max():.1f}] meters\")\n",
    "print(f\"   Y range: [{points[:, 1].min():.1f}, {points[:, 1].max():.1f}] meters\")\n",
    "print(f\"   Z range: [{points[:, 2].min():.1f}, {points[:, 2].max():.1f}] meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize point cloud (2D projection)\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Top view (X-Y)\n",
    "ax1 = fig.add_subplot(131)\n",
    "scatter = ax1.scatter(points[:, 0], points[:, 1], c=points[:, 2], \n",
    "                     cmap='viridis', s=0.5, alpha=0.6)\n",
    "ax1.set_xlabel('X (meters)', fontsize=12)\n",
    "ax1.set_ylabel('Y (meters)', fontsize=12)\n",
    "ax1.set_title('Top View (Bird\\'s Eye)', fontsize=14, fontweight='bold')\n",
    "ax1.set_aspect('equal')\n",
    "plt.colorbar(scatter, ax=ax1, label='Height (m)')\n",
    "\n",
    "# Side view (Y-Z)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(points[:, 1], points[:, 2], c=points[:, 0], \n",
    "           cmap='plasma', s=0.5, alpha=0.6)\n",
    "ax2.set_xlabel('Y (meters)', fontsize=12)\n",
    "ax2.set_ylabel('Z (meters)', fontsize=12)\n",
    "ax2.set_title('Side View', fontsize=14, fontweight='bold')\n",
    "ax2.axhline(y=0, color='r', linestyle='--', label='Ground')\n",
    "ax2.legend()\n",
    "\n",
    "# Front view (X-Z)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(points[:, 0], points[:, 2], c=points[:, 1], \n",
    "           cmap='coolwarm', s=0.5, alpha=0.6)\n",
    "ax3.set_xlabel('X (meters)', fontsize=12)\n",
    "ax3.set_ylabel('Z (meters)', fontsize=12)\n",
    "ax3.set_title('Front View', fontsize=14, fontweight='bold')\n",
    "ax3.axhline(y=0, color='r', linestyle='--', label='Ground')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° The point cloud shows a 3D representation of the environment.\")\n",
    "print(\"   Each point represents a laser reflection from a surface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive 3D Visualization\n\nNow let's create an interactive 3D view! (Note: Close the window to continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D matplotlib visualization\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Subsample for performance\n",
    "subsample = points[::5]  # Every 5th point\n",
    "\n",
    "# Color by height\n",
    "colors = (subsample[:, 2] - subsample[:, 2].min()) / (subsample[:, 2].max() - subsample[:, 2].min())\n",
    "\n",
    "scatter = ax.scatter(subsample[:, 0], subsample[:, 1], subsample[:, 2],\n",
    "                    c=colors, cmap='viridis', s=1, alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('X (m)', fontsize=12)\n",
    "ax.set_ylabel('Y (m)', fontsize=12)\n",
    "ax.set_zlabel('Z (m)', fontsize=12)\n",
    "ax.set_title('Interactive 3D Point Cloud\\n(Rotate with mouse)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# Set equal aspect ratio\n",
    "max_range = np.array([\n",
    "    subsample[:, 0].max()-subsample[:, 0].min(),\n",
    "    subsample[:, 1].max()-subsample[:, 1].min(),\n",
    "    subsample[:, 2].max()-subsample[:, 2].min()\n",
    "]).max() / 2.0\n",
    "\n",
    "mid_x = (subsample[:, 0].max()+subsample[:, 0].min()) * 0.5\n",
    "mid_y = (subsample[:, 1].max()+subsample[:, 1].min()) * 0.5\n",
    "mid_z = (subsample[:, 2].max()+subsample[:, 2].min()) * 0.5\n",
    "\n",
    "ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Height (normalized)', shrink=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## üó∫Ô∏è Part 3: Bird's Eye View (BEV)\n\nBird's eye view is commonly used for planning and navigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bird's eye view\n",
    "bev = create_birds_eye_view(\n",
    "    points,\n",
    "    image_size=(512, 512),\n",
    "    x_range=(-40, 40),\n",
    "    y_range=(0, 80),\n",
    "    z_range=(-2, 5)\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# BEV grayscale\n",
    "axes[0].imshow(bev, cmap='gray')\n",
    "axes[0].set_title('Bird\\'s Eye View (Grayscale)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('X (lateral)')\n",
    "axes[0].set_ylabel('Y (forward)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# BEV with color\n",
    "axes[1].imshow(bev, cmap='viridis')\n",
    "axes[1].set_title('Bird\\'s Eye View (Colored)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('X (lateral)')\n",
    "axes[1].set_ylabel('Y (forward)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Bird's eye view is used for:\")\n",
    "print(\"   - Path planning\")\n",
    "print(\"   - Obstacle detection\")\n",
    "print(\"   - Parking maneuvers\")\n",
    "print(\"   - Lane keeping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## üì∑ Part 4: Camera vs LiDAR Comparison\n\nLet's compare what camera and LiDAR \"see\" in the same scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic camera image\n",
    "gen = SyntheticDataGenerator()\n",
    "camera_image = gen.generate_sample_image('urban', size=(1242, 375))\n",
    "\n",
    "# Visualize both modalities\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Camera image\n",
    "axes[0, 0].imshow(camera_image)\n",
    "axes[0, 0].set_title('üì∑ Camera View (RGB Image)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "axes[0, 0].text(10, 30, 'Rich semantic info\\nColors, textures, signs', \n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "               fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "# LiDAR top view\n",
    "axes[0, 1].scatter(points[:, 0], points[:, 1], c=points[:, 2], \n",
    "                  cmap='viridis', s=0.3, alpha=0.6)\n",
    "axes[0, 1].set_title('üåê LiDAR Top View', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('X (m)')\n",
    "axes[0, 1].set_ylabel('Y (m)')\n",
    "axes[0, 1].set_aspect('equal')\n",
    "axes[0, 1].text(-35, 45, 'Precise 3D geometry\\nNo color info', \n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "               fontsize=10, color='blue', fontweight='bold')\n",
    "\n",
    "# Bird's eye view\n",
    "axes[1, 0].imshow(bev, cmap='viridis')\n",
    "axes[1, 0].set_title('üó∫Ô∏è Bird\\'s Eye View (LiDAR)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "axes[1, 0].text(20, 40, 'Top-down view\\nUsed for planning', \n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "               fontsize=10, color='purple', fontweight='bold')\n",
    "\n",
    "# 3D side view\n",
    "axes[1, 1].scatter(points[:, 1], points[:, 2], c=points[:, 0], \n",
    "                  cmap='plasma', s=0.3, alpha=0.6)\n",
    "axes[1, 1].set_title('üåê LiDAR Side View', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Y (m)')\n",
    "axes[1, 1].set_ylabel('Z (m)')\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2, label='Ground')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].text(5, 4, 'Height information\\nGround detection', \n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "               fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## ‚òÅÔ∏è Part 5: Weather Conditions Impact\n\nDifferent sensors behave differently in various weather conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather impact simulation\n",
    "conditions = ['Clear', 'Rain', 'Fog', 'Night', 'Snow']\n",
    "camera_performance = [95, 40, 30, 20, 35]\n",
    "lidar_performance = [95, 75, 50, 95, 60]\n",
    "radar_performance = [90, 95, 85, 90, 90]\n",
    "\n",
    "x = np.arange(len(conditions))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width, camera_performance, width, label='üì∑ Camera', color='#FF6B6B')\n",
    "bars2 = ax.bar(x, lidar_performance, width, label='üåê LiDAR', color='#4ECDC4')\n",
    "bars3 = ax.bar(x + width, radar_performance, width, label='üì° Radar', color='#45B7D1')\n",
    "\n",
    "ax.set_xlabel('Weather Condition', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Performance (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Sensor Performance in Different Weather Conditions', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(conditions)\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{int(height)}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"   ‚úÖ Radar: Best all-weather performance\")\n",
    "print(\"   ‚ö†Ô∏è Camera: Struggles in rain, fog, and night\")\n",
    "print(\"   üåü LiDAR: Good overall, but affected by fog\")\n",
    "print(\"\\nüí° This is why sensor FUSION is critical for safe autonomous driving!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Why Sensor Fusion?\n",
    "\n",
    "By combining multiple sensors, we get the best of all worlds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor fusion benefits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Individual sensor performance\n",
    "scenarios = ['Object\\nDetection', 'Distance\\nEstimation', 'All-Weather\\nOperation']\n",
    "camera_scores = [90, 50, 40]\n",
    "lidar_scores = [70, 95, 60]\n",
    "radar_scores = [50, 85, 95]\n",
    "fusion_scores = [95, 95, 90]\n",
    "\n",
    "x = np.arange(len(scenarios))\n",
    "width = 0.2\n",
    "\n",
    "axes[0].bar(x - 1.5*width, camera_scores, width, label='Camera', color='#FF6B6B')\n",
    "axes[0].bar(x - 0.5*width, lidar_scores, width, label='LiDAR', color='#4ECDC4')\n",
    "axes[0].bar(x + 0.5*width, radar_scores, width, label='Radar', color='#45B7D1')\n",
    "axes[0].bar(x + 1.5*width, fusion_scores, width, label='Fusion', color='#95E1D3')\n",
    "axes[0].set_ylabel('Performance', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Performance Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(scenarios)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 100)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Redundancy\n",
    "sensors_used = [1, 2, 3]\n",
    "reliability = [85, 95, 99.5]\n",
    "axes[1].plot(sensors_used, reliability, 'o-', linewidth=3, markersize=10, color='#F38181')\n",
    "axes[1].fill_between(sensors_used, reliability, alpha=0.3, color='#F38181')\n",
    "axes[1].set_xlabel('Number of Sensors', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('System Reliability (%)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Redundancy Benefits', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(sensors_used)\n",
    "axes[1].set_ylim(80, 100)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "for i, (x, y) in enumerate(zip(sensors_used, reliability)):\n",
    "    axes[1].text(x, y+0.5, f'{y}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Coverage improvement\n",
    "categories = ['Camera Only', 'LiDAR Only', 'Radar Only', 'All Fused']\n",
    "coverage = [65, 70, 60, 95]\n",
    "colors_bar = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E1D3']\n",
    "axes[2].barh(categories, coverage, color=colors_bar)\n",
    "axes[2].set_xlabel('Environment Coverage (%)', fontsize=11, fontweight='bold')\n",
    "axes[2].set_title('Detection Coverage', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlim(0, 100)\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "for i, v in enumerate(coverage):\n",
    "    axes[2].text(v + 2, i, f'{v}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Sensor Fusion Benefits:\")\n",
    "print(\"   1. üîÑ Redundancy: If one sensor fails, others continue working\")\n",
    "print(\"   2. üìà Better Performance: Combine strengths of each sensor\")\n",
    "print(\"   3. üåê Wider Coverage: Detect objects in more conditions\")\n",
    "print(\"   4. ‚úÖ Higher Reliability: Reduce false positives/negatives\")\n",
    "print(\"   5. üõ°Ô∏è Safety: Critical for ISO 26262 compliance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Three Main Sensors:**\n",
    " - üì∑ Camera: Rich semantic info, weather-dependent\n",
    " - LiDAR: Accurate 3D, expensive, fog-sensitive\n",
    " - Radar: All-weather, low resolution\n",
    "\n",
    "2. **Point Clouds:**\n",
    " - Represent 3D environment as collection of points\n",
    " - Each point has (x, y, z) coordinates + intensity\n",
    " - Can be visualized in different views (top, side, BEV)\n",
    "\n",
    "3. **Sensor Fusion is Essential:**\n",
    " - No single sensor is perfect\n",
    " - Combining sensors improves reliability and coverage\n",
    " - Critical for safety-critical autonomous driving\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 3:** Implement object detection with deep learning\n",
    "- **Notebook 5:** Learn how to fuse sensor data\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Self-Check Questions\n",
    "\n",
    "1. Which sensor works best in fog?\n",
    "2. Why is LiDAR expensive?\n",
    "3. What is a bird's eye view used for?\n",
    "4. Name three benefits of sensor fusion.\n",
    "5. Which sensor provides the best object classification?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answers</summary>\n",
    "\n",
    "1. Radar (radio waves penetrate fog)\n",
    "2. Complex mechanical scanners, laser components, precision optics\n",
    "3. Path planning, parking, lane keeping, obstacle detection\n",
    "4. Redundancy, better performance, wider coverage\n",
    "5. Camera (can see colors, textures, signs)\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Notebook Complete! Proceed to Notebook 3: Object Detection Demo**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
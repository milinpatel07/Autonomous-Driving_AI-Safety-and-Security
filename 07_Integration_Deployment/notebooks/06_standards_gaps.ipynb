{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright (c) 2025 Milin Patel\n",
    "Hochschule Kempten - University of Applied Sciences\n",
    "\n",
    "Autonomous Driving: AI Safety and Security Workshop\n",
    "This project is licensed under the MIT License.\n",
    "-->\n",
    "\n",
    "*Copyright Â© 2025 Milin Patel. All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 24: Standards Gaps and Open Problems in AV Safety\n",
    "\n",
    "**Session 5: Standards Integration and Deployment**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/08_Advanced_Topics/notebooks/24_Standards_Gaps_Open_Problems.ipynb)\n",
    "\n",
    "**Author:** Milin Patel\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Identify limitations of current safety standards for AI/ML systems\n",
    "- Understand what ISO 26262 cannot handle for autonomous systems\n",
    "- Recognize SOTIF gaps and \"unknown unknowns\" problem\n",
    "- Explore unresolved research questions in AV safety\n",
    "- Learn about emerging standards and future directions\n",
    "- Develop critical thinking about standards limitations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction: Why Standards Are Incomplete\n",
    "\n",
    "### The Challenge of AI/ML Systems\n",
    "\n",
    "**Traditional Software** (what ISO 26262 was designed for):\n",
    "- Deterministic behavior\n",
    "- Formally verifiable\n",
    "- Complete requirements specification possible\n",
    "- Failure modes enumerable\n",
    "\n",
    "**AI/ML Systems** (what AVs use):\n",
    "- Non-deterministic (probabilistic)\n",
    "- Not formally verifiable (black box)\n",
    "- Complete specification impossible (unknown unknowns)\n",
    "- Failure modes unbounded (long tail)\n",
    "\n",
    "### Current Standards Were Not Designed for AI\n",
    "\n",
    "| Standard | Original Purpose | When Published | AI Considered? |\n",
    "|----------|-----------------|----------------|----------------|\n",
    "| **ISO 26262** | E/E systems with deterministic SW | 2011 (2018 ed 2) | Minimal |\n",
    "| **ISO 21448** | Performance limitations | 2022 | Yes, but gaps |\n",
    "| **ISO/SAE 21434** | Cybersecurity | 2021 | Partially |\n",
    "\n",
    "**Result**: Standards provide **partial guidance** but leave major gaps\n",
    "\n",
    "### This Notebook's Goal\n",
    "\n",
    "**Not to criticize standards** (they're incredibly valuable!)\n",
    "\n",
    "**But to**:\n",
    "1. Understand their limitations\n",
    "2. Identify what's still missing\n",
    "3. Recognize active research areas\n",
    "4. Prepare for future standards\n",
    "\n",
    "**Critical thinking**: Standards are tools, not complete solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize standards coverage of AI/ML challenges\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Chart 1: Standards applicability to AI challenges\n",
    "challenges = ['Random HW\\nFaults', 'SW Bugs', 'Spec\\nInsufficiencies', 'Unknown\\nEdge Cases', 'Adversarial\\nAttacks', 'Distribution\\nShift']\n",
    "iso_26262 = [95, 85, 20, 10, 5, 5]  # Coverage percentage\n",
    "iso_21448 = [10, 15, 80, 50, 15, 60]\n",
    "iso_21434 = [5, 10, 5, 5, 85, 10]\n",
    "\n",
    "x = np.arange(len(challenges))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, iso_26262, width, label='ISO 26262', color='blue', alpha=0.7)\n",
    "ax1.bar(x, iso_21448, width, label='ISO 21448', color='green', alpha=0.7)\n",
    "ax1.bar(x + width, iso_21434, width, label='ISO/SAE 21434', color='red', alpha=0.7)\n",
    "\n",
    "ax1.set_ylabel('Coverage (%)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Standards Coverage of AV Challenges', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(challenges, fontsize=9)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.axhline(y=80, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='80% threshold')\n",
    "\n",
    "# Chart 2: Gap severity\n",
    "gap_areas = ['ML\\nVerification', 'Unknown\\nUnknowns', 'ODD\\nBoundaries', 'Online\\nLearning', 'Multi-Agent\\nInteraction']\n",
    "severity = [95, 90, 70, 85, 75]  # How severe the gap is\n",
    "colors = ['darkred', 'red', 'orange', 'red', 'orange']\n",
    "\n",
    "bars = ax2.barh(gap_areas, severity, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Gap Severity (Higher = More Critical)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Most Critical Standards Gaps', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "ax2.set_xlim(0, 100)\n",
    "\n",
    "for bar, sev in zip(bars, severity):\n",
    "    ax2.text(sev + 2, bar.get_y() + bar.get_height()/2, \n",
    "             f'{sev}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸  Key Insight: Standards cover traditional failures well, AI-specific challenges poorly\")\n",
    "print(\"   - ISO 26262: Great for HW/SW bugs, weak on AI behavior\")\n",
    "print(\"   - ISO 21448: Addresses performance limits, but 'unknown unknowns' remain\")\n",
    "print(\"   - ISO/SAE 21434: Covers traditional attacks, less on AI-specific exploits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ISO 26262 Gaps for AI/ML\n",
    "\n",
    "### What ISO 26262 Assumes\n",
    "\n",
    "1. **Requirements are Complete**: You can specify all correct behaviors\n",
    "2. **Behavior is Deterministic**: Same input â†’ same output\n",
    "3. **Verification is Possible**: Can test all critical paths\n",
    "4. **Failure Modes are Enumerable**: FMEA covers all failures\n",
    "\n",
    "### Why These Assumptions Fail for AI\n",
    "\n",
    "#### Gap 1: Non-Deterministic Behavior\n",
    "\n",
    "**Problem**:\n",
    "- Neural networks have stochastic elements (dropout, random initialization)\n",
    "- Same input can produce different outputs (especially in ensembles)\n",
    "- ISO 26262 requires deterministic behavior for ASIL D\n",
    "\n",
    "**No Clear Solution**: \n",
    "- Use deterministic mode at inference? (Hurts performance)\n",
    "- Accept non-determinism? (Violates standard)\n",
    "\n",
    "#### Gap 2: Cannot Verify Neural Networks Formally\n",
    "\n",
    "**Problem**:\n",
    "- ISO 26262 requires verification that SW meets requirements\n",
    "- Neural networks: ~100M parameters, non-linear, non-interpretable\n",
    "- **Cannot prove** \"This network will never misclassify a pedestrian\"\n",
    "\n",
    "**Current Workaround**:\n",
    "- Statistical testing on large datasets\n",
    "- But: No mathematical proof possible\n",
    "- Standards don't specify acceptable test size\n",
    "\n",
    "#### Gap 3: Data Quality Not Addressed\n",
    "\n",
    "**Problem**:\n",
    "- ISO 26262 focuses on code quality (MISRA-C, code reviews)\n",
    "- For AI: **Data quality is more critical than code quality**\n",
    "- No guidance on:\n",
    " - Dataset representativeness\n",
    " - Label quality assessment\n",
    " - Bias detection and mitigation\n",
    " - Train/val/test split strategies\n",
    "\n",
    "**ISO 26262 says**: \"Code shall be reviewed\"\n",
    "\n",
    "**Doesn't say**: \"Training data shall represent ODD\"\n",
    "\n",
    "#### Gap 4: Runtime Adaptation\n",
    "\n",
    "**Problem**:\n",
    "- ISO 26262 assumes SW is frozen after validation\n",
    "- Modern AVs use:\n",
    " - Over-the-air (OTA) updates\n",
    " - Online learning (continuous improvement)\n",
    " - Model fine-tuning\n",
    "\n",
    "**Questions**:\n",
    "- If model updated OTA, does it need re-certification?\n",
    "- How to validate online learning (can't test beforehand)?\n",
    "- What if update makes system worse?\n",
    "\n",
    "**Standard is silent on this**\n",
    "\n",
    "#### Gap 5: Diagnostic Coverage for AI\n",
    "\n",
    "**ISO 26262 Requirement**:\n",
    "- Must achieve specific diagnostic coverage (e.g., 99% for ASIL D)\n",
    "- Detect faults before they cause harm\n",
    "\n",
    "**For AI**:\n",
    "- How to measure diagnostic coverage of a neural network?\n",
    "- What's a \"fault\" in a neural network? (Dead neuron? Miscalibration?)\n",
    "- No consensus on metrics\n",
    "\n",
    "**Current Practice**: \n",
    "- Use uncertainty quantification as \"diagnostic\"\n",
    "- But: No proof this achieves required coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISO 26262 gaps for AI\n",
    "gaps_26262 = {\n",
    "    'Gap': [\n",
    "        'Non-deterministic behavior',\n",
    "        'Formal verification impossible',\n",
    "        'Data quality not addressed',\n",
    "        'Runtime adaptation (OTA, online learning)',\n",
    "        'Diagnostic coverage for AI',\n",
    "        'Long-tail failure modes',\n",
    "        'Adversarial robustness'\n",
    "    ],\n",
    "    'ISO 26262 Assumption': [\n",
    "        'Deterministic SW',\n",
    "        'Formal methods apply',\n",
    "        'Code quality = system quality',\n",
    "        'SW frozen post-validation',\n",
    "        'Faults enumerable',\n",
    "        'FMEA covers all failures',\n",
    "        'Random faults only'\n",
    "    ],\n",
    "    'Reality for AI': [\n",
    "        'Stochastic (dropout, ensembles)',\n",
    "        'Black box, 100M+ parameters',\n",
    "        'Data quality >> code quality',\n",
    "        'Continuous updates needed',\n",
    "        'AI \"faults\" not well-defined',\n",
    "        'Unbounded edge cases',\n",
    "        'Adversarial attacks exist'\n",
    "    ],\n",
    "    'Current Workaround': [\n",
    "        'Use deterministic inference (performance loss)',\n",
    "        'Statistical testing (no proof)',\n",
    "        'Ad-hoc data validation',\n",
    "        'Re-validate each OTA (expensive)',\n",
    "        'Uncertainty as proxy (unproven)',\n",
    "        'SOTIF helps (but incomplete)',\n",
    "        'Use ISO/SAE 21434 (partial)'\n",
    "    ],\n",
    "    'Status': [\n",
    "        'Open problem',\n",
    "        'Active research',\n",
    "        'ISO TR 5469 emerging',\n",
    "        'No consensus',\n",
    "        'Open problem',\n",
    "        'SOTIF addresses (partially)',\n",
    "        'Emerging research'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_26262 = pd.DataFrame(gaps_26262)\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"ISO 26262 GAPS FOR AI/ML SYSTEMS\")\n",
    "print(\"=\"*140 + \"\\n\")\n",
    "display(df_26262)\n",
    "\n",
    "print(\"\\nðŸš¨ Key Insight: ISO 26262 was designed for traditional SW, not AI\")\n",
    "print(\"   - Core assumptions (determinism, formal verification) don't hold\")\n",
    "print(\"   - Current practice: Workarounds, no consensus\")\n",
    "print(\"   - Need: AI-specific safety standard (ISO TR 5469 is first step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ISO 21448 (SOTIF) Gaps\n",
    "\n",
    "### What SOTIF Addresses Well\n",
    "\n",
    "âœ… **Performance Limitations**: Sensor range, algorithm accuracy\n",
    "\n",
    "âœ… **Known Triggering Conditions**: Rain, fog, glare\n",
    "\n",
    "âœ… **Scenario-Based Testing**: Identify hazardous scenarios\n",
    "\n",
    "### The \"Unknown Unknowns\" Problem\n",
    "\n",
    "#### SOTIF Four Quadrants\n",
    "\n",
    "```\n",
    " Known Scenarios Unknown Scenarios\n",
    " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Safe Performance â”‚ Known Safe â”‚ Unknown Safe â”‚\n",
    " â”‚ â”‚ (goal: expand) â”‚\n",
    " â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n",
    "Unsafe Performanceâ”‚ Known Unsafe â”‚ Unknown Unsafe â”‚\n",
    " â”‚ (mitigate!) â”‚ (THE PROBLEM!) â”‚\n",
    "```\n",
    "\n",
    "**SOTIF Process**:\n",
    "1. Identify known unsafe scenarios â†’ Fix\n",
    "2. Discover unknown unsafe â†’ Move to known unsafe â†’ Fix\n",
    "3. Repeat until \"unknown unsafe\" is acceptably small\n",
    "\n",
    "**The Gap**: **How do you know when \"unknown unsafe\" is small enough?**\n",
    "\n",
    "#### Gap 1: No Quantitative Exit Criteria\n",
    "\n",
    "**Problem**:\n",
    "- SOTIF says: \"Reduce unknown unsafe scenarios\"\n",
    "- Doesn't say: \"To what level?\"\n",
    "- No metric for \"how much is enough testing?\"\n",
    "\n",
    "**Current Practice**:\n",
    "- Drive millions of miles\n",
    "- Hope you've found most edge cases\n",
    "- **No proof you're done**\n",
    "\n",
    "**What's Needed**:\n",
    "- Statistical confidence metric\n",
    "- \"With 95% confidence, residual risk < X\"\n",
    "- Standard doesn't provide this\n",
    "\n",
    "#### Gap 2: Adversarial Scenarios\n",
    "\n",
    "**SOTIF Assumption**: Scenarios occur naturally\n",
    "\n",
    "**Reality**: Adversaries create scenarios intentionally\n",
    "- Adversarial stop signs\n",
    "- Intentional sensor spoofing\n",
    "- Coordinated multi-vehicle attacks\n",
    "\n",
    "**Gap**: SOTIF doesn't cover intentional triggering conditions\n",
    "\n",
    "**Overlap with ISO/SAE 21434**, but:\n",
    "- 21434 focuses on digital attacks\n",
    "- Physical adversarial examples (e.g., stickers on signs) fall between standards\n",
    "\n",
    "#### Gap 3: Multi-Agent Interactions\n",
    "\n",
    "**Problem**:\n",
    "- SOTIF focuses on single vehicle performance\n",
    "- Real world: Multiple AVs + human drivers + cyclists + pedestrians\n",
    "- **Emergent behaviors** from interactions\n",
    "\n",
    "**Example Scenario**:\n",
    "- AV #1 is too cautious (stops for shadows)\n",
    "- AV #2 following, also stops\n",
    "- Human driver behind AV #2, frustrated, overtakes dangerously\n",
    "- **Crash caused by interaction, not single failure**\n",
    "\n",
    "**Gap**: SOTIF doesn't model multi-agent systems\n",
    "\n",
    "#### Gap 4: Continuous Deployment\n",
    "\n",
    "**SOTIF Assumes**: Validation â†’ Deployment â†’ Done\n",
    "\n",
    "**Reality**: \n",
    "- OTA updates monthly\n",
    "- Fleet learning (update model based on field data)\n",
    "- ODD expansion over time\n",
    "\n",
    "**Gap**: How to apply SOTIF to continuously evolving systems?\n",
    "\n",
    "**Questions**:\n",
    "- Does each OTA update need full SOTIF process?\n",
    "- How to validate fleet learning (happens post-deployment)?\n",
    "- When can you expand ODD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOTIF gaps visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Chart 1: SOTIF quadrants with uncertainty\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Known safe\n",
    "known_safe = Rectangle((0, 5), 5, 5, fill=True, alpha=0.5, color='green', label='Known Safe')\n",
    "ax1.add_patch(known_safe)\n",
    "\n",
    "# Known unsafe\n",
    "known_unsafe = Rectangle((0, 0), 5, 5, fill=True, alpha=0.5, color='red', label='Known Unsafe')\n",
    "ax1.add_patch(known_unsafe)\n",
    "\n",
    "# Unknown safe\n",
    "unknown_safe = Rectangle((5, 5), 5, 5, fill=True, alpha=0.3, color='lightgreen', label='Unknown Safe')\n",
    "ax1.add_patch(unknown_safe)\n",
    "\n",
    "# Unknown unsafe - THE PROBLEM\n",
    "unknown_unsafe = Rectangle((5, 0), 5, 5, fill=True, alpha=0.7, color='darkred', label='Unknown Unsafe')\n",
    "ax1.add_patch(unknown_unsafe)\n",
    "\n",
    "# Question mark\n",
    "ax1.text(7.5, 2.5, '?', fontsize=100, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Labels\n",
    "ax1.text(2.5, 7.5, 'Validated\\nScenarios', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(2.5, 2.5, 'Known\\nHazards', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "ax1.text(7.5, 7.5, 'Probably\\nSafe', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(7.5, 0.5, 'How big is\\nthis region?', ha='center', va='bottom', fontsize=9, fontweight='bold', color='yellow')\n",
    "\n",
    "ax1.set_xlabel('Scenario Knowledge', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Safety', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('SOTIF Quadrants: The \"Unknown Unsafe\" Problem', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks([2.5, 7.5])\n",
    "ax1.set_xticklabels(['Known', 'Unknown'])\n",
    "ax1.set_yticks([2.5, 7.5])\n",
    "ax1.set_yticklabels(['Unsafe', 'Safe'])\n",
    "\n",
    "# Chart 2: Gap severity for SOTIF\n",
    "sotif_gaps = ['Exit Criteria\\n(When done?)', 'Adversarial\\nScenarios', 'Multi-Agent\\nInteractions', \n",
    "              'Continuous\\nDeployment', 'Rare Event\\nValidation']\n",
    "severity = [90, 80, 75, 85, 95]\n",
    "colors = ['darkred', 'red', 'orange', 'red', 'darkred']\n",
    "\n",
    "bars = ax2.barh(sotif_gaps, severity, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Gap Severity', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('SOTIF Critical Gaps', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "ax2.set_xlim(0, 100)\n",
    "\n",
    "for bar, sev in zip(bars, severity):\n",
    "    ax2.text(sev + 2, bar.get_y() + bar.get_height()/2, \n",
    "             f'{sev}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâ“ Key Insight: SOTIF's biggest gap is 'unknown unknowns'\")\n",
    "print(\"   - No way to measure size of 'unknown unsafe' region\")\n",
    "print(\"   - No exit criteria (when is validation done?)\")\n",
    "print(\"   - Cannot prove absence of rare edge cases\")\n",
    "print(\"   - This is a FUNDAMENTAL limitation, not just implementation gap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Open Research Problems\n",
    "\n",
    "### Problem 1: Formal Verification of Neural Networks\n",
    "\n",
    "**Goal**: Prove mathematical properties of neural networks\n",
    "\n",
    "**Example Property**: \"For all inputs representing pedestrians, output confidence > 0.95\"\n",
    "\n",
    "**Current State**:\n",
    "- Possible for small networks (<1000 neurons)\n",
    "- Tools: Reluplex, Marabou, CROWN\n",
    "- **Problem**: Doesn't scale to ImageNet-size networks (100M parameters)\n",
    "- **Timeline**: 10+ years until practical\n",
    "\n",
    "**Implication**: Cannot formally verify AV perception systems yet\n",
    "\n",
    "### Problem 2: Measuring Dataset Representativeness\n",
    "\n",
    "**Question**: \"Is my training dataset representative of the ODD?\"\n",
    "\n",
    "**Why It Matters**:\n",
    "- If dataset biased, model will fail in deployment\n",
    "- Example: Trained only on sunny days â†’ fails in rain\n",
    "\n",
    "**Current State**:\n",
    "- No quantitative metric\n",
    "- Heuristics: \"Collect data from different cities, times, weather\"\n",
    "- **Cannot prove** \"This dataset covers 99% of ODD\"\n",
    "\n",
    "**Research Directions**:\n",
    "- Coverage metrics for high-dimensional spaces\n",
    "- Distribution shift detection\n",
    "- Active learning to find gaps\n",
    "\n",
    "### Problem 3: Safety Validation at Scale\n",
    "\n",
    "**Kalra & Paddock (2016)**: Need 11 billion miles to prove 20% safer\n",
    "\n",
    "**Current Approaches**:\n",
    "- Simulation (CARLA, SUMO)\n",
    "- Scenario-based testing\n",
    "- Accelerated testing\n",
    "\n",
    "**Gaps**:\n",
    "- **Simulation fidelity**: Real world always has surprises\n",
    "- **Scenario coverage**: Cannot enumerate all scenarios\n",
    "- **Statistical confidence**: No accepted methodology\n",
    "\n",
    "**Open Problem**: How to achieve statistical proof of safety without billions of real miles?\n",
    "\n",
    "### Problem 4: Continuous Learning and Safety\n",
    "\n",
    "**Desired**: AV improves over time (fleet learning)\n",
    "\n",
    "**Problem**: Learning can make system worse\n",
    "- Catastrophic forgetting\n",
    "- Distribution shift in field data\n",
    "- Adversarial data poisoning\n",
    "\n",
    "**Question**: How to allow online learning while maintaining safety guarantees?\n",
    "\n",
    "**Current State**:\n",
    "- Most AVs don't use online learning (too risky)\n",
    "- Tesla Autopilot does, but incidents suggest problems\n",
    "- No safety standard for online learning\n",
    "\n",
    "### Problem 5: Multi-Agent Safety\n",
    "\n",
    "**Scenario**: 1000 AVs in same city\n",
    "\n",
    "**Questions**:\n",
    "- How do they interact?\n",
    "- Can emergent traffic jams occur?\n",
    "- What if all AVs make same mistake (e.g., avoid phantom obstacle)?\n",
    "- Adversary exploits predictable AV behavior?\n",
    "\n",
    "**Current State**:\n",
    "- Game theory models exist\n",
    "- But: No validation methodology\n",
    "- No standard for multi-AV safety\n",
    "\n",
    "### Problem 6: Ethical Decision-Making\n",
    "\n",
    "**Trolley Problem for AVs**:\n",
    "- Unavoidable crash\n",
    "- Must choose: Hit pedestrian? Or swerve into wall (injure passenger)?\n",
    "\n",
    "**Questions**:\n",
    "- Who decides the ethical algorithm?\n",
    "- Should it be standardized?\n",
    "- Can passengers override? (Leads to safety issues)\n",
    "- Legal liability?\n",
    "\n",
    "**Current State**:\n",
    "- Germany has ethics commission recommendations\n",
    "- No international consensus\n",
    "- Standards are silent on this\n",
    "\n",
    "### Problem 7: Long-Term Reliability\n",
    "\n",
    "**Question**: Do neural networks degrade over time?\n",
    "\n",
    "**Concerns**:\n",
    "- Hardware aging (bit flips in weights?)\n",
    "- Accumulation of small errors\n",
    "- Distribution shift as world changes\n",
    "\n",
    "**Example**:\n",
    "- AV trained in 2020\n",
    "- In 2030: New car designs, new traffic patterns, new road markings\n",
    "- Does model still work?\n",
    "\n",
    "**Gap**: No long-term reliability models for AI systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open research problems matrix\n",
    "problems_data = {\n",
    "    'Research Problem': [\n",
    "        'Formal verification of NNs',\n",
    "        'Dataset representativeness',\n",
    "        'Safety validation at scale',\n",
    "        'Continuous learning safety',\n",
    "        'Multi-agent interactions',\n",
    "        'Ethical decision-making',\n",
    "        'Long-term reliability'\n",
    "    ],\n",
    "    'Why It Matters': [\n",
    "        'Cannot prove correctness',\n",
    "        'Biased training â†’ field failures',\n",
    "        'Need 11B miles (impossible)',\n",
    "        'Learning can degrade safety',\n",
    "        'Fleet behavior unpredictable',\n",
    "        'Crash liability unclear',\n",
    "        'Unknown degradation over years'\n",
    "    ],\n",
    "    'Current State': [\n",
    "        'Only small networks (<1K neurons)',\n",
    "        'Heuristics only',\n",
    "        'Simulation + hope',\n",
    "        'Mostly avoided (too risky)',\n",
    "        'Game theory, no validation',\n",
    "        'Regional guidelines, no standard',\n",
    "        'No models exist'\n",
    "    ],\n",
    "    'Severity (1-10)': [9, 8, 10, 8, 7, 6, 7],\n",
    "    'Timeline to Solution': [\n",
    "        '10+ years',\n",
    "        '5-10 years',\n",
    "        '10+ years',\n",
    "        '5+ years',\n",
    "        '10+ years',\n",
    "        'Unknown (political)',\n",
    "        '10+ years'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_problems = pd.DataFrame(problems_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"OPEN RESEARCH PROBLEMS IN AV SAFETY (2025)\")\n",
    "print(\"=\"*140 + \"\\n\")\n",
    "display(df_problems)\n",
    "\n",
    "print(\"\\nðŸ”¬ Key Insight: Many fundamental problems remain unsolved\")\n",
    "print(\"   - Most critical: Safety validation at scale (severity 10/10)\")\n",
    "print(\"   - Timeline: 5-10+ years for solutions\")\n",
    "print(\"   - Current practice: Workarounds, not solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize research problem severity and timeline\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Chart 1: Severity comparison\n",
    "problems_short = ['NN\\nVerification', 'Dataset\\nRep.', 'Validation\\nScale', 'Online\\nLearning', \n",
    "                  'Multi-Agent', 'Ethics', 'Long-term\\nReliability']\n",
    "severity = df_problems['Severity (1-10)'].values\n",
    "colors_sev = ['red' if s >= 8 else 'orange' if s >= 7 else 'yellow' for s in severity]\n",
    "\n",
    "bars = ax1.bar(problems_short, severity, color=colors_sev, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Severity (1-10)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Open Problem Severity', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim(0, 11)\n",
    "ax1.axhline(y=8, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Critical Threshold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, sev in zip(bars, severity):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "             f'{sev}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Chart 2: Timeline to solution\n",
    "timeline_categories = ['< 5 years', '5-10 years', '10+ years', 'Unknown']\n",
    "counts = [0, 2, 4, 1]  # Based on data\n",
    "colors_time = ['green', 'yellow', 'red', 'gray']\n",
    "\n",
    "bars = ax2.bar(timeline_categories, counts, color=colors_time, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Number of Problems', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Timeline to Solutions', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{count}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâ³ Key Insight: Most problems are 10+ years from solution\")\n",
    "print(\"   - Validation at scale: Most critical (10/10) and longest timeline\")\n",
    "print(\"   - This explains why AV deployment is slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Emerging Standards and Future Directions\n",
    "\n",
    "### Standards in Development\n",
    "\n",
    "#### ISO/IEC TR 5469:2024 - AI and Functional Safety\n",
    "\n",
    "**Status**: Published (Technical Report, not full standard yet)\n",
    "\n",
    "**Coverage**:\n",
    "- Data management for AI\n",
    "- Training, validation, test set requirements\n",
    "- Uncertainty quantification\n",
    "- Explainability (XAI)\n",
    "- Monitoring and retraining\n",
    "\n",
    "**Limitations**:\n",
    "- Technical Report = guidance, not requirements\n",
    "- Doesn't address formal verification gap\n",
    "- No exit criteria for validation\n",
    "\n",
    "**Timeline**: May become full standard by 2027-2028\n",
    "\n",
    "#### ISO 34503:2023 - Test Scenarios for ADS\n",
    "\n",
    "**Status**: Published\n",
    "\n",
    "**Coverage**:\n",
    "- ODD specification framework\n",
    "- Scenario taxonomy\n",
    "- Data formats for scenarios\n",
    "\n",
    "**Limitations**:\n",
    "- Describes scenarios, doesn't say how to test\n",
    "- No coverage metrics\n",
    "\n",
    "#### UL 4600 - Standard for Autonomous Products\n",
    "\n",
    "**Publisher**: Underwriters Laboratories (US)\n",
    "\n",
    "**Status**: Published 2020, updated 2023\n",
    "\n",
    "**Approach**: \n",
    "- Safety case based (not prescriptive)\n",
    "- \"Prove your system is safe\" vs \"Follow these steps\"\n",
    "\n",
    "**Coverage**:\n",
    "- Risk assessment\n",
    "- Validation methods\n",
    "- Data quality\n",
    "- Runtime monitoring\n",
    "\n",
    "**Limitation**: \n",
    "- US-centric\n",
    "- Less adoption in Europe/Asia\n",
    "\n",
    "### What's Still Missing\n",
    "\n",
    "#### 1. Quantitative Validation Exit Criteria\n",
    "\n",
    "**Need**: \"Test X miles in Y scenarios â†’ 95% confidence of Z safety level\"\n",
    "\n",
    "**Challenge**: No consensus on statistical methodology\n",
    "\n",
    "#### 2. Online Learning Standard\n",
    "\n",
    "**Need**: How to safely update models post-deployment\n",
    "\n",
    "**Challenge**: Validation before learning is impossible\n",
    "\n",
    "#### 3. AI Diagnostic Coverage Metrics\n",
    "\n",
    "**Need**: Equivalent of ISO 26262 diagnostic coverage for AI\n",
    "\n",
    "**Challenge**: What's a \"fault\" in a neural network?\n",
    "\n",
    "#### 4. Multi-Agent Safety Standard\n",
    "\n",
    "**Need**: Safety when many AVs interact\n",
    "\n",
    "**Challenge**: Emergent behaviors hard to predict\n",
    "\n",
    "### Industry Feedback on Standards\n",
    "\n",
    "**Common Complaints**:\n",
    "1. **Too Slow**: Standards take 5-10 years to develop; tech moves faster\n",
    "2. **Too Prescriptive**: \"Do X\" doesn't work for AI (need outcome-based)\n",
    "3. **Fragmented**: Need to comply with 26262 + 21448 + 21434 + regional regulations\n",
    "4. **Gaps**: Key problems (like validation scale) not addressed\n",
    "\n",
    "**Suggestions**:\n",
    "1. **Agile Standards**: Update every 2-3 years, not 10\n",
    "2. **Outcome-Based**: Define safety goals, not implementation\n",
    "3. **Harmonization**: Single global standard (unlikely politically)\n",
    "4. **AI-Specific**: Need ISO 26262-like standard designed for AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standards timeline and coverage\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Chart 1: Standards development timeline\n",
    "standards_timeline = {\n",
    "    'Standard': ['ISO 26262\\n(Ed 1)', 'ISO 26262\\n(Ed 2)', 'ISO/SAE\\n21434', 'ISO 21448\\n(SOTIF)', \n",
    "                 'ISO/IEC TR\\n5469', 'ISO 34503', 'UL 4600', 'ISO 26262\\nfor AI?'],\n",
    "    'Year': [2011, 2018, 2021, 2022, 2024, 2023, 2020, 2028],\n",
    "    'Status': ['Published', 'Published', 'Published', 'Published', 'Published (TR)', 'Published', 'Published', 'Proposed']\n",
    "}\n",
    "\n",
    "years = standards_timeline['Year']\n",
    "y_pos = np.arange(len(standards_timeline['Standard']))\n",
    "colors_timeline = ['blue', 'blue', 'red', 'green', 'purple', 'orange', 'brown', 'gray']\n",
    "\n",
    "ax1.barh(y_pos, [2025 - y for y in years], left=years, color=colors_timeline, alpha=0.6, edgecolor='black')\n",
    "\n",
    "for i, (year, std) in enumerate(zip(years, standards_timeline['Standard'])):\n",
    "    ax1.text(year - 0.5, i, str(year), va='center', ha='right', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(standards_timeline['Standard'], fontsize=9)\n",
    "ax1.set_xlabel('Year', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('AV Safety Standards Timeline', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim(2010, 2030)\n",
    "ax1.axvline(x=2025, color='red', linestyle='--', linewidth=2, label='Today')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Chart 2: Standards coverage of AI challenges (updated with new standards)\n",
    "ai_challenges = ['Data\\nQuality', 'Online\\nLearning', 'NN\\nVerification', 'Multi-Agent', 'Validation\\nScale']\n",
    "iso_tr_5469 = [70, 40, 20, 10, 30]  # New AI standard\n",
    "ul_4600 = [60, 30, 15, 25, 40]\n",
    "combined = [80, 50, 25, 30, 45]  # Using all standards together\n",
    "\n",
    "x = np.arange(len(ai_challenges))\n",
    "width = 0.25\n",
    "\n",
    "ax2.bar(x - width, iso_tr_5469, width, label='ISO/IEC TR 5469', color='purple', alpha=0.7)\n",
    "ax2.bar(x, ul_4600, width, label='UL 4600', color='brown', alpha=0.7)\n",
    "ax2.bar(x + width, combined, width, label='All Standards Combined', color='green', alpha=0.7)\n",
    "\n",
    "ax2.set_ylabel('Coverage (%)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Emerging Standards Coverage of AI Challenges', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(ai_challenges, fontsize=9)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.axhline(y=80, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='80% threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“‹ Key Insight: New standards emerging, but gaps remain\")\n",
    "print(\"   - ISO/IEC TR 5469: Best AI coverage so far (data quality, online learning)\")\n",
    "print(\"   - Even combined: <50% coverage of validation scale, multi-agent\")\n",
    "print(\"   - Need: 5-10 more years of standards development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Summary and Critical Thinking\n",
    "\n",
    "### Main Takeaways\n",
    "\n",
    "#### 1. Standards Are Necessary But Insufficient\n",
    "\n",
    "âœ… **Standards provide**:\n",
    "- Common terminology\n",
    "- Process frameworks\n",
    "- Best practices\n",
    "- Regulatory acceptance\n",
    "\n",
    "âŒ **Standards don't provide**:\n",
    "- Solutions to fundamental AI problems\n",
    "- Quantitative validation methods\n",
    "- Formal verification for AI\n",
    "- Exit criteria for testing\n",
    "\n",
    "**Conclusion**: Use standards, but don't assume compliance = safety\n",
    "\n",
    "#### 2. Biggest Gaps\n",
    "\n",
    "**Severity Ranking**:\n",
    "1. **Validation at scale** (10/10) - Cannot prove safety with finite testing\n",
    "2. **NN formal verification** (9/10) - Cannot prove correctness mathematically\n",
    "3. **Unknown unknowns** (9/10) - Cannot bound residual risk\n",
    "4. **Online learning** (8/10) - Cannot safely update post-deployment\n",
    "5. **Multi-agent** (7/10) - Cannot predict fleet interactions\n",
    "\n",
    "#### 3. Timeline to Solutions\n",
    "\n",
    "**Realistic Assessment**:\n",
    "- **Data quality standards**: 2-5 years (ISO TR 5469 progress)\n",
    "- **ODD frameworks**: Mostly solved (ISO 34503)\n",
    "- **Validation methodology**: 10+ years (fundamental statistics problem)\n",
    "- **Formal verification**: 10+ years (scalability challenge)\n",
    "- **Multi-agent safety**: 10+ years (complexity challenge)\n",
    "\n",
    "**Implication**: Current standards are interim solutions; expect major revisions by 2030-2035\n",
    "\n",
    "#### 4. What This Means for Practitioners\n",
    "\n",
    "**If you're developing AVs**:\n",
    "1. **Follow standards** (regulatory requirement, best practices)\n",
    "2. **Go beyond standards** (they're not sufficient)\n",
    "3. **Document gaps** (for safety case, explain what's not covered)\n",
    "4. **Stay conservative** (narrow ODD, extensive testing)\n",
    "5. **Monitor research** (solutions emerging, be ready to adopt)\n",
    "\n",
    "**If you're a regulator**:\n",
    "1. **Standards are baseline** (not complete safety proof)\n",
    "2. **Demand transparency** (what's NOT covered?)\n",
    "3. **Require safety cases** (argumentation, not just compliance)\n",
    "4. **Allow iteration** (tech evolving, regulations must too)\n",
    "\n",
    "**If you're a researcher**:\n",
    "1. **Work on gaps** (huge opportunity, societal impact)\n",
    "2. **Collaborate with industry** (practical constraints matter)\n",
    "3. **Engage with standards bodies** (influence future standards)\n",
    "\n",
    "### The Path Forward\n",
    "\n",
    "#### Short Term (2025-2030)\n",
    "- **ISO/IEC TR 5469** becomes full standard\n",
    "- Better data quality practices\n",
    "- Improved scenario-based testing\n",
    "- Harmonization of regional regulations\n",
    "\n",
    "#### Medium Term (2030-2035)\n",
    "- AI-specific safety standard (\"ISO 26262 for AI\")\n",
    "- Quantitative validation methods\n",
    "- Online learning guidelines\n",
    "- Multi-agent safety frameworks\n",
    "\n",
    "#### Long Term (2035+)\n",
    "- Formal verification tools for large NNs\n",
    "- Statistical proof of safety\n",
    "- Global harmonized standard\n",
    "- Level 5 enablement (maybe)\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "**Standards reflect our current best understanding** â€” they're iterative, not perfect.\n",
    "\n",
    "**The gaps are not failures** â€” they're honest acknowledgment of hard problems.\n",
    "\n",
    "**Safety is an ongoing process** â€” not a one-time certification.\n",
    "\n",
    "**Critical thinking required** â€” Don't blindly follow standards; understand their limitations.\n",
    "\n",
    "**The field is young** â€” AV safety engineering is only ~15 years old. Traditional automotive took 100+ years to mature.\n",
    "\n",
    "**We're making progress** â€” Slowly, but standards, research, and practice are improving.\n",
    "\n",
    "**Patience needed** â€” This is a marathon, not a sprint. Safety cannot be rushed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive summary\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Chart 1: Standards maturity by topic\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "topics = ['HW Faults', 'SW Bugs', 'Cybersecurity', 'ODD Spec', 'Data Quality', \n",
    "          'Scenario Testing', 'NN Verification', 'Validation Scale', 'Online Learning', 'Multi-Agent']\n",
    "maturity = [95, 90, 75, 80, 60, 65, 15, 25, 30, 20]  # Out of 100\n",
    "colors_mat = ['green' if m >= 70 else 'yellow' if m >= 50 else 'orange' if m >= 30 else 'red' for m in maturity]\n",
    "\n",
    "bars = ax1.barh(topics, maturity, color=colors_mat, alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Standards Maturity (%)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Standards Maturity by Topic (2025)', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlim(0, 100)\n",
    "ax1.axvline(x=70, color='green', linestyle='--', linewidth=1, alpha=0.5, label='Mature')\n",
    "ax1.axvline(x=50, color='yellow', linestyle='--', linewidth=1, alpha=0.5, label='Developing')\n",
    "ax1.axvline(x=30, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='Immature')\n",
    "ax1.legend(loc='lower right', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, mat in zip(bars, maturity):\n",
    "    ax1.text(mat + 2, bar.get_y() + bar.get_height()/2, \n",
    "             f'{mat}%', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Chart 2: Gap severity vs timeline\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "gap_names = ['Validation\\nScale', 'NN\\nVerif.', 'Unknown\\nUnknowns', 'Online\\nLearn', 'Multi-\\nAgent']\n",
    "severity_short = [10, 9, 9, 8, 7]\n",
    "timeline_years = [15, 12, 15, 7, 12]\n",
    "\n",
    "scatter = ax2.scatter(timeline_years, severity_short, s=[s*50 for s in severity_short], \n",
    "                     c=severity_short, cmap='Reds', alpha=0.7, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, name in enumerate(gap_names):\n",
    "    ax2.annotate(name, (timeline_years[i], severity_short[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Est. Years to Solution', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Severity (1-10)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Gap Severity vs. Timeline', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 18)\n",
    "ax2.set_ylim(6, 11)\n",
    "\n",
    "# Chart 3: Standards evolution roadmap\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "periods = ['2025-2030', '2030-2035', '2035+']\n",
    "current = [60, 0, 0]  # Current standards\n",
    "emerging = [20, 30, 0]  # Emerging standards\n",
    "future = [0, 20, 40]  # Future standards\n",
    "\n",
    "x = np.arange(len(periods))\n",
    "width = 0.6\n",
    "\n",
    "ax3.bar(x, current, width, label='Current Standards', color='blue', alpha=0.7)\n",
    "ax3.bar(x, emerging, width, bottom=current, label='Emerging Standards', color='green', alpha=0.7)\n",
    "ax3.bar(x, future, width, bottom=np.array(current)+np.array(emerging), label='Future Standards', color='gold', alpha=0.7)\n",
    "\n",
    "ax3.set_ylabel('Relative Contribution (%)', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Standards Evolution Roadmap', fontsize=12, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(periods)\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Chart 4: Recommendations summary\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "\n",
    "recommendations_text = \"\"\"\n",
    "KEY RECOMMENDATIONS\n",
    "\n",
    "For DEVELOPERS:\n",
    "âœ“ Follow standards (baseline), but go beyond\n",
    "âœ“ Document gaps explicitly in safety case\n",
    "âœ“ Conservative ODD, extensive testing\n",
    "âœ“ Monitor research for emerging solutions\n",
    "\n",
    "For REGULATORS:\n",
    "âœ“ Standards = baseline, not complete proof\n",
    "âœ“ Demand transparency on limitations\n",
    "âœ“ Require safety cases (argumentation)\n",
    "âœ“ Allow iterative improvement\n",
    "\n",
    "For RESEARCHERS:\n",
    "âœ“ Focus on critical gaps (validation scale, NN verification)\n",
    "âœ“ Collaborate with industry on practical constraints\n",
    "âœ“ Engage with standards bodies\n",
    "\n",
    "BOTTOM LINE:\n",
    "Standards are necessary tools, not complete solutions.\n",
    "Many fundamental problems remain open.\n",
    "Safety is an ongoing process, not a one-time certification.\n",
    "Critical thinking required - understand limitations.\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, recommendations_text, transform=ax4.transAxes,\n",
    "         fontsize=9, verticalalignment='top', family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))\n",
    "\n",
    "plt.suptitle('Standards Gaps and Open Problems: Complete Summary', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"WORKSHOP COMPLETE: STANDARDS GAPS AND OPEN PROBLEMS\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nðŸŽ¯ Main Lessons:\")\n",
    "print(\"   1. Standards provide frameworks, not complete solutions\")\n",
    "print(\"   2. Biggest gaps: Validation scale, NN verification, unknown unknowns\")\n",
    "print(\"   3. Timeline: 5-15 years for major advances\")\n",
    "print(\"   4. Use standards, but understand their limitations\")\n",
    "print(\"   5. Safety is ongoing process, not one-time certification\")\n",
    "print(\"\\nðŸ“š This completes Session 5 and the entire workshop!\")\n",
    "print(\"   You now have comprehensive understanding of AV safety:\")\n",
    "print(\"   - Technical: Perception, uncertainty, validation\")\n",
    "print(\"   - Standards: ISO 26262, 21448, 21434, emerging standards\")\n",
    "print(\"   - Practical: Deployment challenges, ODD, limitations\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References and Further Reading\n",
    "\n",
    "### Standards (Current)\n",
    "- **ISO 26262:2018** - Road vehicles â€” Functional safety\n",
    "- **ISO 21448:2022** - Road vehicles â€” Safety of the intended functionality (SOTIF)\n",
    "- **ISO/SAE 21434:2021** - Road vehicles â€” Cybersecurity engineering\n",
    "\n",
    "### Standards (Emerging)\n",
    "- **ISO/IEC TR 5469:2024** - Artificial intelligence â€” Functional safety and AI systems\n",
    "- **ISO 34503:2023** - Road vehicles â€” Test scenarios for automated driving systems\n",
    "- **UL 4600:2023** - Standard for Safety for the Evaluation of Autonomous Products\n",
    "\n",
    "### Research Papers on Standards Gaps\n",
    "- Koopman, P., & Wagner, M. (2017). \"Autonomous Vehicle Safety: An Interdisciplinary Challenge\" *IEEE ITAS*.\n",
    "- Burton, S., et al. (2020). \"Confidence Arguments for Evidence of Performance in Machine Learning\" *SafeComp 2020*.\n",
    "- Borg, M., et al. (2019). \"Safely Entering the Deep: A Review of Verification and Validation for ML\" *arXiv:1812.10577*.\n",
    "\n",
    "### Research Papers on Open Problems\n",
    "- Kalra, N., & Paddock, S. M. (2016). \"Driving to safety\" *Transportation Research Part A*.\n",
    "- Amodei, D., et al. (2016). \"Concrete Problems in AI Safety\" *arXiv:1606.06565*.\n",
    "- Huang, X., et al. (2020). \"A Survey of Safety and Trustworthiness of Deep Neural Networks\" *CSUR*, 53(6).\n",
    "\n",
    "### Industry Perspectives\n",
    "- Waymo Safety Report (2024)\n",
    "- SAE International: \"Challenges in Autonomous Vehicle Testing and Validation\"\n",
    "- McKinsey: \"The Future of Mobility - Standards and Regulation\"\n",
    "\n",
    "---\n",
    "\n",
    "*Copyright Â© 2025 Milin Patel. All Rights Reserved.*\n",
    "\n",
    "**End of Session 5 and Workshop**\n",
    "\n",
    "Thank you for participating in this comprehensive workshop on Autonomous Driving: AI, Safety, and Security!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright (c) 2025 Milin Patel\n",
    "Hochschule Kempten - University of Applied Sciences\n",
    "\n",
    "Autonomous Driving: AI Safety and Security Workshop\n",
    "This project is licensed under the MIT License.\n",
    "-->\n",
    "\n",
    "*Copyright Â© 2025 Milin Patel. All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 20: Explainability (XAI) for Safety Certification\n",
    "\n",
    "**Session 5: Standards Integration and Deployment**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/master/08_Advanced_Topics/notebooks/20_Explainability_XAI.ipynb)\n",
    "\n",
    "**Author:** Milin Patel \n",
    "**Institution:** Hochschule Kempten\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand why explainability is mandatory for AV certification\n",
    "- Learn XAI methods: LIME, SHAP, GradCAM, attention visualization\n",
    "- Apply saliency maps to perception models\n",
    "- Document AI decisions for regulators\n",
    "- Understand XAI requirements in ISO/IEC TR 5469\n",
    "- Navigate accuracy vs interpretability trade-offs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import Image, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Explainability Matters for AV Certification\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Deep neural networks are **black boxes**:\n",
    "- 100 million+ parameters in perception models\n",
    "- Non-linear transformations across layers\n",
    "- Impossible to trace decision path manually\n",
    "\n",
    "**Regulators ask:** \"Why did the vehicle brake here?\"\n",
    "**Traditional answer:** \"The neural network output was 0.87\"\n",
    "**Regulator response:** \"That's not an explanation. Rejected.\"\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **Legal Liability**: When crashes occur, courts need to understand why AI made decisions\n",
    "2. **Certification**: Regulators (NHTSA, UNECE) require explainable AI per ISO/IEC TR 5469\n",
    "3. **Debugging**: Engineers need to fix failures, not just detect them\n",
    "4. **Trust**: Public acceptance requires understanding\n",
    "5. **Standards Compliance**: ISO 26262 requires failure analysis - can't analyze what you can't explain\n",
    "\n",
    "### Real Example: Uber ATG Crash (2018)\n",
    "\n",
    "- Perception system detected pedestrian 6 seconds before impact\n",
    "- System classified object as: vehicle â†’ bicycle â†’ unknown â†’ pedestrian\n",
    "- **Critical question**: Why did classification keep changing?\n",
    "- **Without XAI**: Cannot answer, cannot fix\n",
    "- **With XAI**: Could visualize which image features caused confusion\n",
    "\n",
    "### Regulatory Requirements\n",
    "\n",
    "**ISO/IEC TR 5469 (AI Functional Safety):**\n",
    "- Section 6.4: AI systems must provide \"rationale for decisions\"\n",
    "- Section 7.2: \"Interpretability shall be documented\"\n",
    "\n",
    "**EU AI Act (2024):**\n",
    "- High-risk AI (includes AVs) must provide explanations\n",
    "- Right to explanation for impacted individuals\n",
    "\n",
    "**NHTSA Standing General Order:**\n",
    "- Crash reports must explain AV decisions before crash\n",
    "\n",
    "**Key Insight:** XAI is not optional - it's legally required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. XAI Methods for Perception\n",
    "\n",
    "### Method 1: Saliency Maps (Gradient-Based)\n",
    "\n",
    "**Question:** Which pixels influenced the decision most?\n",
    "\n",
    "**Method:**\n",
    "1. Forward pass: input image â†’ prediction\n",
    "2. Compute gradient: âˆ‚(prediction)/âˆ‚(input pixels)\n",
    "3. Visualize: bright pixels = high influence\n",
    "\n",
    "**Interpretation:** Highlights regions model \"looks at\"\n",
    "\n",
    "**Limitation:** Shows correlation, not causation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple saliency map computation\n",
    "def compute_saliency_map(model, image, target_class):\n",
    "    \"\"\"\n",
    "    Compute saliency map showing which pixels influence prediction.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained neural network\n",
    "        image: Input image tensor (requires_grad=True)\n",
    "        target_class: Class index to explain\n",
    "    \n",
    "    Returns:\n",
    "        Saliency map (same size as image)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Get score for target class\n",
    "    score = output[0, target_class]\n",
    "    \n",
    "    # Backward pass\n",
    "    score.backward()\n",
    "    \n",
    "    # Gradient magnitude\n",
    "    saliency = image.grad.abs().max(dim=1)[0]\n",
    "    \n",
    "    return saliency.detach().cpu().numpy()\n",
    "\n",
    "print(\"âœ“ Saliency map function defined\")\n",
    "print(\"  Usage: Shows which pixels the model focuses on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: GradCAM (Gradient-weighted Class Activation Mapping)\n",
    "\n",
    "**Improvement over saliency:** Shows **where** in feature maps model looks\n",
    "\n",
    "**How it works:**\n",
    "1. Get gradients flowing into last convolutional layer\n",
    "2. Weight each feature map by gradient importance\n",
    "3. Sum weighted feature maps\n",
    "4. Overlay heatmap on original image\n",
    "\n",
    "**Advantage:** \n",
    "- Higher resolution than simple saliency\n",
    "- Shows object-level attention\n",
    "- Works for any CNN architecture\n",
    "\n",
    "**Real-world use:** \n",
    "- Waymo uses GradCAM to debug perception failures\n",
    "- Shows exactly which part of object was detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradCAM implementation (simplified)\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Gradient-weighted Class Activation Mapping for CNN explainability.\n",
    "    \n",
    "    Shows which regions of the image the model focuses on for a specific prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_full_backward_hook(self.save_gradient)\n",
    "    \n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, image, target_class):\n",
    "        \"\"\"\n",
    "        Generate Class Activation Map.\n",
    "        \n",
    "        Returns:\n",
    "            Heatmap showing important regions (H x W)\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        output = self.model(image)\n",
    "        \n",
    "        # Backward pass for target class\n",
    "        self.model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "        \n",
    "        # Weight activations by gradients\n",
    "        weights = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # ReLU and normalize\n",
    "        cam = torch.relu(cam)\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "        \n",
    "        return cam[0, 0].cpu().numpy()\n",
    "\n",
    "print(\"âœ“ GradCAM class defined\")\n",
    "print(\"  Usage: Shows object-level attention in perception models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: LIME (Local Interpretable Model-Agnostic Explanations)\n",
    "\n",
    "**Idea:** Explain complex model by fitting simple model locally\n",
    "\n",
    "**Algorithm:**\n",
    "1. Perturb input (e.g., mask image regions)\n",
    "2. Get predictions on perturbed inputs\n",
    "3. Fit linear model: perturbations â†’ predictions\n",
    "4. Linear weights = feature importance\n",
    "\n",
    "**Advantage:**\n",
    "- Works for any model (model-agnostic)\n",
    "- Provides local explanations\n",
    "- Human-interpretable features\n",
    "\n",
    "**Disadvantage:**\n",
    "- Computationally expensive (many forward passes)\n",
    "- Explanations can be unstable\n",
    "\n",
    "**AV Application:** \n",
    "- Explain why pedestrian was missed\n",
    "- Test: if we mask this region, does detection fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "**Based on:** Game theory (Shapley values)\n",
    "\n",
    "**Idea:** How much does each feature contribute to prediction?\n",
    "\n",
    "**Properties:**\n",
    "- **Additivity:** Individual contributions sum to final prediction\n",
    "- **Consistency:** If feature becomes more important, SHAP value increases\n",
    "- **Uniqueness:** Only one valid explanation\n",
    "\n",
    "**AV Application:**\n",
    "- Decompose detection score: 0.87 = 0.3 (shape) + 0.4 (color) + 0.17 (context)\n",
    "- Regulators accept because mathematically principled\n",
    "\n",
    "**Limitation:** Exponentially expensive (need approximations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: Attention Visualization (for Transformers)\n",
    "\n",
    "Modern perception uses Vision Transformers (ViT).\n",
    "\n",
    "**Advantage:** Attention weights are built-in explanations!\n",
    "\n",
    "**How:**\n",
    "- Extract attention weights from each head\n",
    "- Visualize which image patches attend to which\n",
    "- See what model \"looks at\"\n",
    "\n",
    "**Example:** Tesla FSD Beta uses attention visualization internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Practical: Explaining Pedestrian Detection\n",
    "\n",
    "### Scenario\n",
    "\n",
    "Your AV perception system detects a pedestrian with confidence 0.91.\n",
    "**Regulator asks:** \"Why this detection?\"\n",
    "\n",
    "### Step-by-step XAI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate detection explanation\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Original image (simulated)\n",
    "axes[0].set_title('1. Original Image', fontweight='bold')\n",
    "axes[0].text(0.5, 0.5, 'Pedestrian\\nCrossing', ha='center', va='center', \n",
    "            fontsize=14, bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Saliency map\n",
    "axes[1].set_title('2. Saliency Map\\n(Gradient)', fontweight='bold')\n",
    "saliency = np.random.rand(10, 10)\n",
    "saliency[3:7, 4:6] = 0.9  # Bright region on pedestrian\n",
    "axes[1].imshow(saliency, cmap='hot', interpolation='bilinear')\n",
    "axes[1].text(5, 9, 'High gradient\\non person shape', ha='center', fontsize=9)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# GradCAM heatmap\n",
    "axes[2].set_title('3. GradCAM Heatmap\\n(Feature importance)', fontweight='bold')\n",
    "gradcam = np.zeros((10, 10))\n",
    "gradcam[3:7, 4:6] = 1.0  # Focus on pedestrian\n",
    "axes[2].imshow(gradcam, cmap='jet', interpolation='bilinear', alpha=0.7)\n",
    "axes[2].text(5, 9, 'Model focuses\\non upper body', ha='center', fontsize=9)\n",
    "axes[2].axis('off')\n",
    "\n",
    "# SHAP explanation\n",
    "axes[3].set_title('4. SHAP Values\\n(Feature contributions)', fontweight='bold')\n",
    "features = ['Human\\nShape', 'Vertical\\nPose', 'Legs\\nMoving', 'Context\\n(Crosswalk)', 'Background']\n",
    "shap_values = [0.35, 0.28, 0.18, 0.10, 0.00]\n",
    "colors = ['red' if v > 0.2 else 'orange' if v > 0.1 else 'gray' for v in shap_values]\n",
    "axes[3].barh(features, shap_values, color=colors)\n",
    "axes[3].set_xlabel('Contribution to\\nPedestrian Score (0.91)', fontsize=9)\n",
    "axes[3].set_xlim(0, 0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š XAI Explanation for Regulators:\")\n",
    "print(\"   Detection confidence: 0.91\")\n",
    "print(\"   Primary factors:\")\n",
    "print(\"   - Human shape match: +0.35\")\n",
    "print(\"   - Vertical pose: +0.28\")\n",
    "print(\"   - Leg movement detected: +0.18\")\n",
    "print(\"   - Located in crosswalk: +0.10\")\n",
    "print(\"   Total: 0.91 âœ“\")\n",
    "print(\"\\n   This explanation satisfies ISO/IEC TR 5469 requirements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Documenting AI Decisions for Certification\n",
    "\n",
    "### Required Documentation (ISO/IEC TR 5469)\n",
    "\n",
    "For each safety-critical AI component, document:\n",
    "\n",
    "1. **Model Architecture**\n",
    " - Layers, parameters, activations\n",
    " - Why this architecture chosen\n",
    "\n",
    "2. **Training Data**\n",
    " - Dataset composition\n",
    " - Bias analysis\n",
    " - ODD coverage\n",
    "\n",
    "3. **Decision Rationale**\n",
    " - XAI method used (GradCAM, SHAP, etc.)\n",
    " - Example explanations for test cases\n",
    " - Edge case analysis\n",
    "\n",
    "4. **Failure Modes**\n",
    " - Known failure scenarios\n",
    " - Why failures occur (with XAI)\n",
    " - Mitigation strategies\n",
    "\n",
    "5. **Validation Evidence**\n",
    " - Test coverage\n",
    " - Performance metrics\n",
    " - Uncertainty quantification\n",
    "\n",
    "### Example Documentation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: AI Decision Documentation Template\n",
    "documentation_template = \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "AI DECISION DOCUMENTATION (ISO/IEC TR 5469 Compliant)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SYSTEM: Pedestrian Detection Module\n",
    "VERSION: 2.3.1\n",
    "ASIL LEVEL: ASIL-D (ISO 26262)\n",
    "DATE: 2025-11-23\n",
    "AUTHOR: Milin Patel, Hochschule Kempten\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. MODEL ARCHITECTURE\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Base: YOLOv8-medium\n",
    "Parameters: 25.9M\n",
    "Input: 640x640 RGB\n",
    "Output: Bounding boxes + class probabilities\n",
    "Inference time: 23ms (average)\n",
    "\n",
    "Rationale: YOLOv8 chosen for real-time performance (<50ms) \n",
    "required by ASIL-D latency budget.\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "2. TRAINING DATA\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Dataset: nuScenes (1000 scenes) + KITTI (7481 images) + \n",
    "         Internal dataset (50K images)\n",
    "Pedestrian instances: 187,342\n",
    "ODD coverage: Urban (60%), Highway (25%), Residential (15%)\n",
    "Weather: Clear (70%), Rain (20%), Night (10%)\n",
    "Bias analysis: Gender balance 52%/48%, Age distribution verified\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "3. EXPLAINABILITY METHOD\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Primary: GradCAM (on backbone layer 4)\n",
    "Secondary: SHAP for feature importance\n",
    "Validation: Attention rollout for transformer heads\n",
    "\n",
    "Example Explanation (Test Case TC-PED-042):\n",
    "  Input: Pedestrian in crosswalk, daylight\n",
    "  Output: Confidence 0.94\n",
    "  \n",
    "  GradCAM Analysis:\n",
    "  - High activation on upper body (torso, head)\n",
    "  - Moderate activation on legs\n",
    "  - Low activation on background\n",
    "  \n",
    "  SHAP Decomposition:\n",
    "  - Body shape:  +0.38\n",
    "  - Upright pose: +0.29\n",
    "  - Motion:      +0.19\n",
    "  - Context:     +0.08\n",
    "  Total:         0.94 âœ“\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "4. KNOWN FAILURE MODES\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Failure FM-PED-01: Partial occlusion (>70% occluded)\n",
    "  Explanation: Insufficient visible pixels for shape matching\n",
    "  XAI Evidence: GradCAM shows no activation on occluded region\n",
    "  Mitigation: Multi-frame temporal integration\n",
    "  \n",
    "Failure FM-PED-02: Extreme lighting (direct sunlight)\n",
    "  Explanation: Overexposure saturates pixels, destroys edges\n",
    "  XAI Evidence: Saliency map shows uniform gradients\n",
    "  Mitigation: HDR image processing, sensor fusion with LiDAR\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "5. CERTIFICATION EVIDENCE\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Test Coverage: 98.7% of ODD scenarios\n",
    "Precision: 0.947 (on validation set)\n",
    "Recall: 0.923 (SOTIF target: >0.95)\n",
    "False Negative Rate: 0.077 â†’ ASIL-D requires <0.01\n",
    "  âš  ISSUE: Requires additional validation\n",
    "\n",
    "Uncertainty Quantification: MC Dropout (50 samples)\n",
    "  Mean epistemic uncertainty: 0.08\n",
    "  High uncertainty cases manually reviewed\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CERTIFICATION STATUS: Pending - FNR improvement required\n",
    "REVIEWER: [Safety Engineer Name]\n",
    "APPROVAL: [Regulatory Authority]\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(documentation_template)\n",
    "print(\"\\nâœ“ This template satisfies ISO/IEC TR 5469 Section 7.2\")\n",
    "print(\"  'Interpretability shall be documented'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Trade-offs: Accuracy vs Interpretability\n",
    "\n",
    "### The Dilemma\n",
    "\n",
    "**Simple models** (decision trees, linear regression):\n",
    "- Easy to explain\n",
    "- Lower accuracy\n",
    "- May not meet safety requirements\n",
    "\n",
    "**Complex models** (deep neural networks):\n",
    "- High accuracy\n",
    "- Hard to explain\n",
    "- Regulators uncomfortable\n",
    "\n",
    "### Industry Approaches\n",
    "\n",
    "**Option 1: Post-hoc explanation (most common)**\n",
    "- Use complex model for performance\n",
    "- Apply XAI (SHAP, GradCAM) for explanation\n",
    "- Document explanations for certification\n",
    "- **Used by:** Waymo, Cruise, Tesla\n",
    "\n",
    "**Option 2: Inherently interpretable models**\n",
    "- Use attention mechanisms (Transformers)\n",
    "- Attention weights = explanation\n",
    "- Trade some accuracy for interpretability\n",
    "- **Used by:** Some research groups\n",
    "\n",
    "**Option 3: Hybrid approach**\n",
    "- Complex model for difficult cases\n",
    "- Simple model for easy cases\n",
    "- Route based on scenario difficulty\n",
    "- **Emerging approach**\n",
    "\n",
    "### Certification Reality\n",
    "\n",
    "Regulators accept complex models **if**:\n",
    "1. XAI methods are validated\n",
    "2. Explanations are consistent\n",
    "3. Failure modes are understood\n",
    "4. Extensive testing is done\n",
    "5. Human oversight exists\n",
    "\n",
    "**Key Insight:** You don't need a simple model, you need good explanations of your complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "1. **XAI is mandatory** for AV certification per ISO/IEC TR 5469 and EU AI Act\n",
    "\n",
    "2. **Multiple XAI methods exist**: GradCAM, SHAP, LIME, saliency maps, attention\n",
    "\n",
    "3. **No single best method**: Use multiple for robustness\n",
    "\n",
    "4. **Documentation is critical**: Regulators need written explanations, not just visualizations\n",
    "\n",
    "5. **Post-hoc explanation works**: Don't sacrifice accuracy for interpretability\n",
    "\n",
    "6. **Explainability aids debugging**: Helps engineers fix failures\n",
    "\n",
    "7. **Trust requires transparency**: Public acceptance needs understandable AI\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- Selvaraju et al. (2017): \"Grad-CAM: Visual Explanations from Deep Networks\"\n",
    "- Lundberg & Lee (2017): \"A Unified Approach to Interpreting Model Predictions\" (SHAP)\n",
    "- Ribeiro et al. (2016): \"Why Should I Trust You? Explaining Predictions\" (LIME)\n",
    "- ISO/IEC TR 5469:2024: \"Artificial intelligence â€” Functional safety and AI systems\"\n",
    "- EU AI Act (2024): Requirements for high-risk AI systems\n",
    "\n",
    "---\n",
    "\n",
    "*Copyright Â© 2025 Milin Patel. All Rights Reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
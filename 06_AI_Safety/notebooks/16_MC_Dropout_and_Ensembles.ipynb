{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright (c) 2025 Milin Patel\n",
    "Hochschule Kempten - University of Applied Sciences\n",
    "\n",
    "Autonomous Driving: AI Safety and Security Workshop\n",
    "This project is licensed under the MIT License.\n",
    "See LICENSE file in the root directory for full license text.\n",
    "-->\n",
    "\n",
    "*Copyright ¬© 2025 Milin Patel. All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 16: Monte Carlo Dropout and Deep Ensembles\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/AV_Perception_Safety_Workshop/Session_4_Uncertainty_Estimation_and_Validation/notebooks/16_MC_Dropout_and_Ensembles.ipynb)\n",
    "\n",
    "**Session 4: Uncertainty Estimation and Validation**  \n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Monte Carlo (MC) Dropout for uncertainty estimation\n",
    "- Implement Deep Ensembles for robust predictions\n",
    "- Compare different uncertainty quantification methods\n",
    "- Apply to AV perception tasks\n",
    "- Analyze computational costs vs accuracy trade-offs\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous notebook, we learned about uncertainty types. Now we'll learn **how to quantify** them.\n",
    "\n",
    "**Two practical methods:**\n",
    "1. **Monte Carlo Dropout:** Cheap, single model, approximate Bayesian inference\n",
    "2. **Deep Ensembles:** More expensive, multiple models, strong empirical performance\n",
    "\n",
    "Both are widely used in production AV systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q torch torchvision matplotlib seaborn numpy scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Monte Carlo Dropout\n",
    "\n",
    "### 1.1 Theory\n",
    "\n",
    "**Key Insight (Gal & Ghahramani, 2016):** \n",
    "- Dropout during training = approximate Bayesian inference\n",
    "- Enable dropout at test time = sample from approximate posterior\n",
    "- Multiple forward passes = Monte Carlo sampling\n",
    "\n",
    "**Mathematical intuition:**\n",
    "```\n",
    "Regular prediction:    y = f(x; Œ∏)\n",
    "MC Dropout:            y‚ÇÅ, y‚ÇÇ, ..., y‚Çô = f(x; Œ∏ * mask‚ÇÅ), f(x; Œ∏ * mask‚ÇÇ), ..., f(x; Œ∏ * mask‚Çô)\n",
    "Mean prediction:       ≈∑ = (1/N) Œ£ y·µ¢\n",
    "Uncertainty:           œÉ¬≤ = (1/N) Œ£ (y·µ¢ - ≈∑)¬≤\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Single model (cheap to train)\n",
    "- ‚úÖ Easy to implement\n",
    "- ‚úÖ Captures epistemic uncertainty\n",
    "- ‚úÖ Can be added to existing models\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Slower inference (N forward passes)\n",
    "- ‚ùå Approximation quality depends on dropout rate\n",
    "- ‚ùå May underestimate uncertainty\n",
    "\n",
    "### 1.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropoutClassifier(nn.Module):\n",
    "    \"\"\"Neural network with MC Dropout for uncertainty estimation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=100, num_classes=2, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def predict_with_uncertainty(self, x, n_samples=50):\n",
    "        \"\"\"\n",
    "        Predict with uncertainty using MC Dropout.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            n_samples: Number of MC samples\n",
    "            \n",
    "        Returns:\n",
    "            mean_probs: Mean class probabilities\n",
    "            std_probs: Standard deviation of probabilities (epistemic uncertainty)\n",
    "            entropy: Predictive entropy (total uncertainty)\n",
    "        \"\"\"\n",
    "        self.train()  # Enable dropout!\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_samples):\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                predictions.append(probs)\n",
    "        \n",
    "        # Stack predictions: (n_samples, batch_size, num_classes)\n",
    "        predictions = torch.stack(predictions)\n",
    "        \n",
    "        # Mean prediction\n",
    "        mean_probs = predictions.mean(dim=0)\n",
    "        \n",
    "        # Epistemic uncertainty (variance)\n",
    "        std_probs = predictions.std(dim=0)\n",
    "        \n",
    "        # Predictive entropy (total uncertainty)\n",
    "        entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n",
    "        \n",
    "        return mean_probs, std_probs, entropy\n",
    "\n",
    "print(\"MC Dropout Classifier defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with OOD region\n",
    "def create_classification_dataset():\n",
    "    \"\"\"Create 2D classification dataset.\"\"\"\n",
    "    # Training data: moons dataset\n",
    "    X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "    \n",
    "    # Create OOD region (far from training data)\n",
    "    X_ood = np.random.uniform(-3, -1.5, size=(50, 2))\n",
    "    \n",
    "    return X, y, X_ood\n",
    "\n",
    "X_train, y_train, X_ood = create_classification_dataset()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', \n",
    "                     s=50, alpha=0.6, edgecolors='black', label='Training data')\n",
    "plt.scatter(X_ood[:, 0], X_ood[:, 1], c='red', marker='x', s=100, \n",
    "           linewidths=3, label='OOD region')\n",
    "plt.colorbar(scatter, label='Class')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Classification Dataset with OOD Region')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MC Dropout model\n",
    "def train_model(model, X_train, y_train, epochs=500, lr=0.01):\n",
    "    \"\"\"Train a classification model.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_tensor = torch.LongTensor(y_train).to(device)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train MC Dropout model\n",
    "print(\"Training MC Dropout model...\")\n",
    "mc_model = MCDropoutClassifier(input_dim=2, hidden_dim=100, num_classes=2, dropout_rate=0.2)\n",
    "losses = train_model(mc_model, X_train, y_train, epochs=500)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MC Dropout Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MC Dropout uncertainty\n",
    "def visualize_mc_dropout_uncertainty(model, X_train, y_train, X_ood, n_samples=50):\n",
    "    \"\"\"Visualize predictions and uncertainty from MC Dropout.\"\"\"\n",
    "    \n",
    "    # Create grid\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_tensor = torch.FloatTensor(grid_points).to(device)\n",
    "    \n",
    "    # Get predictions with uncertainty\n",
    "    mean_probs, std_probs, entropy = model.predict_with_uncertainty(grid_tensor, n_samples=n_samples)\n",
    "    mean_probs = mean_probs.cpu().numpy()\n",
    "    std_probs = std_probs.cpu().numpy()\n",
    "    entropy = entropy.cpu().numpy()\n",
    "    \n",
    "    # Reshape\n",
    "    pred_class = mean_probs[:, 1].reshape(xx.shape)\n",
    "    uncertainty = std_probs[:, 1].reshape(xx.shape)\n",
    "    entropy_map = entropy.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Predictions\n",
    "    ax = axes[0]\n",
    "    contour = ax.contourf(xx, yy, pred_class, levels=20, cmap='viridis', alpha=0.6)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', \n",
    "              s=50, edgecolors='black', linewidth=1.5)\n",
    "    ax.scatter(X_ood[:, 0], X_ood[:, 1], c='red', marker='x', s=100, linewidths=3)\n",
    "    plt.colorbar(contour, ax=ax, label='P(Class 1)')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title('Mean Predictions (50 MC samples)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Epistemic uncertainty (std)\n",
    "    ax = axes[1]\n",
    "    contour = ax.contourf(xx, yy, uncertainty, levels=20, cmap='Reds', alpha=0.8)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c='blue', s=30, alpha=0.3)\n",
    "    ax.scatter(X_ood[:, 0], X_ood[:, 1], c='black', marker='x', s=100, linewidths=3)\n",
    "    plt.colorbar(contour, ax=ax, label='Std Dev')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title('Epistemic Uncertainty (Std)\\nüî• High in OOD region!')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Predictive entropy\n",
    "    ax = axes[2]\n",
    "    contour = ax.contourf(xx, yy, entropy_map, levels=20, cmap='OrRd', alpha=0.8)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c='blue', s=30, alpha=0.3)\n",
    "    ax.scatter(X_ood[:, 0], X_ood[:, 1], c='black', marker='x', s=100, linewidths=3)\n",
    "    plt.colorbar(contour, ax=ax, label='Entropy')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title('Predictive Entropy (Total Uncertainty)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ Key Observations:\")\n",
    "    print(\"1. Uncertainty is HIGH in OOD region (red X's) ‚úÖ\")\n",
    "    print(\"2. Uncertainty is HIGH at decision boundary ‚úÖ\")\n",
    "    print(\"3. Uncertainty is LOW in well-covered regions ‚úÖ\")\n",
    "    print(\"\\n‚Üí MC Dropout successfully detects epistemic uncertainty!\")\n",
    "\n",
    "visualize_mc_dropout_uncertainty(mc_model, X_train, y_train, X_ood, n_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Ensembles\n",
    "\n",
    "### 2.1 Theory\n",
    "\n",
    "**Key Idea (Lakshminarayanan et al., 2017):**\n",
    "- Train M independent models with different initializations\n",
    "- Each model captures different aspects of the data\n",
    "- Disagreement between models = uncertainty\n",
    "\n",
    "**Mathematical formulation:**\n",
    "```\n",
    "Train M models:     f‚ÇÅ(x), f‚ÇÇ(x), ..., f‚Çò(x)\n",
    "Mean prediction:    ≈∑ = (1/M) Œ£ f·µ¢(x)\n",
    "Epistemic unc.:     œÉ¬≤_epistemic = (1/M) Œ£ (f·µ¢(x) - ≈∑)¬≤\n",
    "Aleatoric unc.:     œÉ¬≤_aleatoric = (1/M) Œ£ œÉ·µ¢¬≤(x)  [if models predict variance]\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Strong empirical performance\n",
    "- ‚úÖ Can capture both aleatoric and epistemic\n",
    "- ‚úÖ Simple and robust\n",
    "- ‚úÖ State-of-the-art in many benchmarks\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Expensive (M times the training cost)\n",
    "- ‚ùå M times the memory for deployment\n",
    "- ‚ùå M times slower inference\n",
    "\n",
    "**Production tip:** Often M=5 is enough for good results!\n",
    "\n",
    "### 2.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardClassifier(nn.Module):\n",
    "    \"\"\"Standard neural network for ensemble.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=100, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "class DeepEnsemble:\n",
    "    \"\"\"Deep Ensemble for uncertainty estimation.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_models=5, input_dim=2, hidden_dim=100, num_classes=2):\n",
    "        self.num_models = num_models\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(num_models):\n",
    "            # Different random initialization for each model\n",
    "            torch.manual_seed(42 + i)\n",
    "            model = StandardClassifier(input_dim, hidden_dim, num_classes)\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def train_ensemble(self, X_train, y_train, epochs=500, lr=0.01):\n",
    "        \"\"\"Train all models in the ensemble.\"\"\"\n",
    "        print(f\"Training ensemble of {self.num_models} models...\")\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            print(f\"\\nTraining model {i+1}/{self.num_models}...\")\n",
    "            losses = train_model(model, X_train, y_train, epochs=epochs, lr=lr)\n",
    "            print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    def predict_with_uncertainty(self, x):\n",
    "        \"\"\"\n",
    "        Predict with uncertainty using ensemble.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            mean_probs: Mean class probabilities\n",
    "            std_probs: Standard deviation (epistemic uncertainty)\n",
    "            entropy: Predictive entropy\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                predictions.append(probs)\n",
    "        \n",
    "        # Stack predictions: (num_models, batch_size, num_classes)\n",
    "        predictions = torch.stack(predictions)\n",
    "        \n",
    "        # Mean prediction\n",
    "        mean_probs = predictions.mean(dim=0)\n",
    "        \n",
    "        # Epistemic uncertainty (variance across models)\n",
    "        std_probs = predictions.std(dim=0)\n",
    "        \n",
    "        # Predictive entropy\n",
    "        entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n",
    "        \n",
    "        return mean_probs, std_probs, entropy\n",
    "\n",
    "print(\"Deep Ensemble class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble\n",
    "ensemble = DeepEnsemble(num_models=5, input_dim=2, hidden_dim=100, num_classes=2)\n",
    "ensemble.train_ensemble(X_train, y_train, epochs=500, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ensemble uncertainty\n",
    "def visualize_ensemble_uncertainty(ensemble, X_train, y_train, X_ood):\n",
    "    \"\"\"Visualize predictions and uncertainty from ensemble.\"\"\"\n",
    "    \n",
    "    # Create grid\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_tensor = torch.FloatTensor(grid_points).to(device)\n",
    "    \n",
    "    # Get predictions with uncertainty\n",
    "    mean_probs, std_probs, entropy = ensemble.predict_with_uncertainty(grid_tensor)\n",
    "    mean_probs = mean_probs.cpu().numpy()\n",
    "    std_probs = std_probs.cpu().numpy()\n",
    "    entropy = entropy.cpu().numpy()\n",
    "    \n",
    "    # Reshape\n",
    "    pred_class = mean_probs[:, 1].reshape(xx.shape)\n",
    "    uncertainty = std_probs[:, 1].reshape(xx.shape)\n",
    "    entropy_map = entropy.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Predictions\n",
    "    ax = axes[0]\n",
    "    contour = ax.contourf(xx, yy, pred_class, levels=20, cmap='viridis', alpha=0.6)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', \n",
    "              s=50, edgecolors='black', linewidth=1.5)\n",
    "    ax.scatter(X_ood[:, 0], X_ood[:, 1], c='red', marker='x', s=100, linewidths=3)\n",
    "    plt.colorbar(contour, ax=ax, label='P(Class 1)')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(f'Ensemble Mean Predictions ({ensemble.num_models} models)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Epistemic uncertainty\n",
    "    ax = axes[1]\n",
    "    contour = ax.contourf(xx, yy, uncertainty, levels=20, cmap='Reds', alpha=0.8)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c='blue', s=30, alpha=0.3)\n",
    "    ax.scatter(X_ood[:, 0], X_ood[:, 1], c='black', marker='x', s=100, linewidths=3)\n",
    "    plt.colorbar(contour, ax=ax, label='Std Dev')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title('Epistemic Uncertainty (Model Disagreement)\\nüî• High in OOD region!')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Predictive entropy\n",
    "    ax = axes[2]\n",
    "    contour = ax.contourf(xx, yy, entropy_map, levels=20, cmap='OrRd', alpha=0.8)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c='blue', s=30, alpha=0.3)\n",
    "    ax.scatter(X_ood[:, 0], X_ood[:, 1], c='black', marker='x', s=100, linewidths=3)\n",
    "    plt.colorbar(contour, ax=ax, label='Entropy')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title('Predictive Entropy (Total Uncertainty)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_ensemble_uncertainty(ensemble, X_train, y_train, X_ood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison: MC Dropout vs Ensembles\n",
    "\n",
    "Let's compare both methods side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_uncertainty_methods(mc_model, ensemble, X_train, y_train, X_ood):\n",
    "    \"\"\"Compare MC Dropout and Ensemble uncertainty estimates.\"\"\"\n",
    "    \n",
    "    # Create test points\n",
    "    # 1. In-distribution points\n",
    "    in_dist_idx = np.random.choice(len(X_train), 100, replace=False)\n",
    "    X_in = X_train[in_dist_idx]\n",
    "    \n",
    "    # 2. OOD points\n",
    "    X_out = X_ood\n",
    "    \n",
    "    # Get uncertainties\n",
    "    # MC Dropout\n",
    "    X_in_tensor = torch.FloatTensor(X_in).to(device)\n",
    "    X_out_tensor = torch.FloatTensor(X_out).to(device)\n",
    "    \n",
    "    _, mc_std_in, mc_ent_in = mc_model.predict_with_uncertainty(X_in_tensor, n_samples=50)\n",
    "    _, mc_std_out, mc_ent_out = mc_model.predict_with_uncertainty(X_out_tensor, n_samples=50)\n",
    "    \n",
    "    mc_unc_in = mc_std_in[:, 1].cpu().numpy()\n",
    "    mc_unc_out = mc_std_out[:, 1].cpu().numpy()\n",
    "    \n",
    "    # Ensemble\n",
    "    _, ens_std_in, ens_ent_in = ensemble.predict_with_uncertainty(X_in_tensor)\n",
    "    _, ens_std_out, ens_ent_out = ensemble.predict_with_uncertainty(X_out_tensor)\n",
    "    \n",
    "    ens_unc_in = ens_std_in[:, 1].cpu().numpy()\n",
    "    ens_unc_out = ens_std_out[:, 1].cpu().numpy()\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # 1. Distribution comparison\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(mc_unc_in, bins=20, alpha=0.5, label='MC Dropout (In-Dist)', color='blue')\n",
    "    ax.hist(mc_unc_out, bins=20, alpha=0.5, label='MC Dropout (OOD)', color='red')\n",
    "    ax.set_xlabel('Uncertainty (Std Dev)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('MC Dropout Uncertainty Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.hist(ens_unc_in, bins=20, alpha=0.5, label='Ensemble (In-Dist)', color='blue')\n",
    "    ax.hist(ens_unc_out, bins=20, alpha=0.5, label='Ensemble (OOD)', color='red')\n",
    "    ax.set_xlabel('Uncertainty (Std Dev)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Ensemble Uncertainty Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plots\n",
    "    ax = axes[1, 0]\n",
    "    data_mc = [mc_unc_in, mc_unc_out]\n",
    "    bp = ax.boxplot(data_mc, labels=['In-Distribution', 'OOD'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][1].set_facecolor('lightcoral')\n",
    "    ax.set_ylabel('Uncertainty')\n",
    "    ax.set_title('MC Dropout: In-Dist vs OOD')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    data_ens = [ens_unc_in, ens_unc_out]\n",
    "    bp = ax.boxplot(data_ens, labels=['In-Distribution', 'OOD'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][1].set_facecolor('lightcoral')\n",
    "    ax.set_ylabel('Uncertainty')\n",
    "    ax.set_title('Ensemble: In-Dist vs OOD')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"UNCERTAINTY QUANTIFICATION COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Method':<20} {'In-Dist Mean':<15} {'OOD Mean':<15} {'Separation':<15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    mc_sep = mc_unc_out.mean() / (mc_unc_in.mean() + 1e-10)\n",
    "    ens_sep = ens_unc_out.mean() / (ens_unc_in.mean() + 1e-10)\n",
    "    \n",
    "    print(f\"{'MC Dropout':<20} {mc_unc_in.mean():<15.4f} {mc_unc_out.mean():<15.4f} {mc_sep:<15.2f}x\")\n",
    "    print(f\"{'Deep Ensemble':<20} {ens_unc_in.mean():<15.4f} {ens_unc_out.mean():<15.4f} {ens_sep:<15.2f}x\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\nüéØ Key Insights:\")\n",
    "    print(f\"1. Both methods detect OOD: uncertainty is {mc_sep:.1f}x and {ens_sep:.1f}x higher\")\n",
    "    print(\"2. Ensembles typically show better separation (more reliable)\")\n",
    "    print(\"3. MC Dropout is cheaper (single model) but may underestimate uncertainty\")\n",
    "    print(\"4. For safety-critical AVs: Ensembles preferred despite higher cost\")\n",
    "\n",
    "compare_uncertainty_methods(mc_model, ensemble, X_train, y_train, X_ood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computational Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_computational_costs():\n",
    "    \"\"\"Analyze training and inference costs.\"\"\"\n",
    "    \n",
    "    # Create test dataset\n",
    "    X_test = np.random.randn(1000, 2)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 1. Single model (baseline)\n",
    "    single_model = StandardClassifier(input_dim=2, hidden_dim=100, num_classes=2).to(device)\n",
    "    single_model.eval()\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = single_model(X_test_tensor)\n",
    "    single_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'method': 'Single Model',\n",
    "        'training_cost': '1x',\n",
    "        'memory_cost': '1x',\n",
    "        'inference_time': single_time,\n",
    "        'relative_time': 1.0,\n",
    "        'uncertainty': 'No'\n",
    "    })\n",
    "    \n",
    "    # 2. MC Dropout\n",
    "    mc_samples = [10, 30, 50]\n",
    "    for n in mc_samples:\n",
    "        start = time.time()\n",
    "        _ = mc_model.predict_with_uncertainty(X_test_tensor, n_samples=n)\n",
    "        mc_time = time.time() - start\n",
    "        \n",
    "        results.append({\n",
    "            'method': f'MC Dropout (N={n})',\n",
    "            'training_cost': '1x',\n",
    "            'memory_cost': '1x',\n",
    "            'inference_time': mc_time,\n",
    "            'relative_time': mc_time / single_time,\n",
    "            'uncertainty': 'Yes (Epistemic)'\n",
    "        })\n",
    "    \n",
    "    # 3. Ensemble\n",
    "    start = time.time()\n",
    "    _ = ensemble.predict_with_uncertainty(X_test_tensor)\n",
    "    ens_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'method': f'Ensemble (M={ensemble.num_models})',\n",
    "        'training_cost': f'{ensemble.num_models}x',\n",
    "        'memory_cost': f'{ensemble.num_models}x',\n",
    "        'inference_time': ens_time,\n",
    "        'relative_time': ens_time / single_time,\n",
    "        'uncertainty': 'Yes (Both types)'\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Inference time comparison\n",
    "    methods = [r['method'] for r in results]\n",
    "    times = [r['inference_time'] * 1000 for r in results]  # Convert to ms\n",
    "    colors = ['blue'] + ['orange']*3 + ['red']\n",
    "    \n",
    "    bars = ax1.bar(methods, times, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax1.set_ylabel('Inference Time (ms)')\n",
    "    ax1.set_title('Inference Speed Comparison (1000 samples)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{time_val:.1f}ms',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Relative cost visualization\n",
    "    categories = ['Training Cost', 'Memory Cost', 'Inference Cost']\n",
    "    mc_costs = [1, 1, 30]  # MC Dropout with N=30\n",
    "    ens_costs = [5, 5, 5]  # Ensemble with M=5\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax2.bar(x - width/2, mc_costs, width, label='MC Dropout (N=30)', \n",
    "                    color='orange', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    bars2 = ax2.bar(x + width/2, ens_costs, width, label='Ensemble (M=5)',\n",
    "                   color='red', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax2.set_ylabel('Relative Cost (vs Single Model)')\n",
    "    ax2.set_title('Resource Cost Comparison')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.axhline(y=1, color='blue', linestyle='--', linewidth=2, alpha=0.5, label='Single Model')\n",
    "    \n",
    "    # Add values\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.0f}x',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print table\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"COMPUTATIONAL COST ANALYSIS\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"\\n{'Method':<25} {'Train':<10} {'Memory':<10} {'Infer (ms)':<12} {'Speedup':<12} {'Uncertainty'}\")\n",
    "    print(\"-\"*90)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['method']:<25} {r['training_cost']:<10} {r['memory_cost']:<10} \"\n",
    "              f\"{r['inference_time']*1000:<12.2f} {r['relative_time']:<12.1f}x {r['uncertainty']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"\\nüéØ Trade-off Analysis:\")\n",
    "    print(\"\\nMC Dropout:\")\n",
    "    print(\"  ‚úÖ Same training cost as single model\")\n",
    "    print(\"  ‚úÖ Same memory footprint\")\n",
    "    print(\"  ‚ùå Slower inference (N forward passes)\")\n",
    "    print(\"  ‚ö†Ô∏è  May underestimate uncertainty\")\n",
    "    print(\"\\nDeep Ensemble:\")\n",
    "    print(\"  ‚ùå M times training cost\")\n",
    "    print(\"  ‚ùå M times memory (critical for edge deployment!)\")\n",
    "    print(\"  ‚ùå M times slower inference\")\n",
    "    print(\"  ‚úÖ Better uncertainty estimates\")\n",
    "    print(\"  ‚úÖ Can capture both uncertainty types\")\n",
    "    print(\"\\nüí° Recommendation for AVs:\")\n",
    "    print(\"  - Development/Research: Use Ensembles (M=5-10)\")\n",
    "    print(\"  - Production (cloud): Use Ensembles (M=3-5)\")\n",
    "    print(\"  - Production (edge): Use MC Dropout (N=10-30) or optimized ensembles\")\n",
    "\n",
    "analyze_computational_costs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other Uncertainty Quantification Methods (Brief Overview)\n",
    "\n",
    "### 5.1 Bayesian Neural Networks (BNNs)\n",
    "\n",
    "**Idea:** Place distributions over network weights instead of point estimates.\n",
    "\n",
    "```python\n",
    "# Conceptual (full implementation is complex)\n",
    "# Instead of: w = optimize(data)\n",
    "# We get: p(w|data) - full posterior distribution\n",
    "```\n",
    "\n",
    "**Challenges:**\n",
    "- Computationally expensive\n",
    "- Requires careful prior selection\n",
    "- Approximate inference needed (variational inference, MCMC)\n",
    "\n",
    "**Libraries:** PyTorch BNN extensions, TensorFlow Probability, Pyro\n",
    "\n",
    "### 5.2 Evidential Deep Learning\n",
    "\n",
    "**Idea:** Network directly predicts parameters of a prior distribution.\n",
    "\n",
    "**Advantages:**\n",
    "- Single forward pass (fast!)\n",
    "- Separates aleatoric and epistemic uncertainty\n",
    "- Principled approach based on subjective logic\n",
    "\n",
    "**Reference:** Sensoy et al. \"Evidential Deep Learning to Quantify Classification Uncertainty\" (NeurIPS 2018)\n",
    "\n",
    "### 5.3 Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Method': [\n",
    "        'Single Model',\n",
    "        'MC Dropout',\n",
    "        'Deep Ensemble',\n",
    "        'Bayesian NN',\n",
    "        'Evidential DL'\n",
    "    ],\n",
    "    'Training Cost': ['1x', '1x', 'Mx', 'High', '1x'],\n",
    "    'Inference Cost': ['1x', 'Nx', 'Mx', 'High', '1x'],\n",
    "    'Memory': ['1x', '1x', 'Mx', '1-2x', '1x'],\n",
    "    'Aleatoric': ['‚ùå', '‚ùå', '‚úÖ*', '‚úÖ', '‚úÖ'],\n",
    "    'Epistemic': ['‚ùå', '‚úÖ', '‚úÖ', '‚úÖ', '‚úÖ'],\n",
    "    'Implementation': ['Easy', 'Easy', 'Easy', 'Hard', 'Medium'],\n",
    "    'Quality': ['N/A', 'Good', 'Excellent', 'Excellent', 'Good'],\n",
    "    'Production Ready': ['‚úÖ', '‚úÖ', '‚úÖ', '‚ö†Ô∏è', '‚ö†Ô∏è']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"UNCERTAINTY QUANTIFICATION METHODS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n*Deep Ensemble can capture aleatoric if models predict variance\")\n",
    "print(\"N = number of MC samples, M = number of ensemble models\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "methods = df['Method'].tolist()\n",
    "metrics = ['Training Cost', 'Inference Cost', 'Memory']\n",
    "\n",
    "# Convert costs to numeric (rough estimates)\n",
    "cost_map = {'1x': 1, 'Nx': 5, 'Mx': 5, 'High': 10, '1-2x': 1.5}\n",
    "quality_map = {'N/A': 0, 'Good': 3, 'Excellent': 5}\n",
    "impl_map = {'Easy': 1, 'Medium': 2, 'Hard': 3}\n",
    "\n",
    "# Create cost score (lower is better)\n",
    "cost_scores = []\n",
    "for _, row in df.iterrows():\n",
    "    cost = (cost_map[row['Training Cost']] + \n",
    "            cost_map[row['Inference Cost']] + \n",
    "            cost_map[row['Memory']]) / 3\n",
    "    cost_scores.append(cost)\n",
    "\n",
    "# Create quality score (higher is better)\n",
    "quality_scores = [quality_map[q] for q in df['Quality']]\n",
    "\n",
    "# Plot\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, cost_scores, width, label='Computational Cost',\n",
    "              color='red', alpha=0.6, edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax.bar(x + width/2, quality_scores, width, label='Uncertainty Quality',\n",
    "              color='green', alpha=0.6, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Method Comparison: Cost vs Quality\\n(Lower cost is better, Higher quality is better)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.1f}',\n",
    "               ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Bottom Line for AV Applications:\")\n",
    "print(\"\\n1. **For Production AVs:**\")\n",
    "print(\"   - Deep Ensembles (M=3-5) ‚Üí Best uncertainty, proven track record\")\n",
    "print(\"   - MC Dropout (N=10-30) ‚Üí Good uncertainty, lower cost\")\n",
    "print(\"\\n2. **For Research:**\")\n",
    "print(\"   - Deep Ensembles (M=5-10) ‚Üí Benchmark\")\n",
    "print(\"   - Bayesian NNs ‚Üí Theoretical rigor\")\n",
    "print(\"   - Evidential DL ‚Üí Emerging method, watch this space\")\n",
    "print(\"\\n3. **For Edge Deployment:**\")\n",
    "print(\"   - MC Dropout ‚Üí Memory constrained\")\n",
    "print(\"   - Evidential DL ‚Üí Fast inference needed\")\n",
    "print(\"   - Knowledge Distillation from Ensemble ‚Üí Best of both worlds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Monte Carlo Dropout:**\n",
    "   - Enable dropout at test time\n",
    "   - Multiple forward passes = uncertainty estimate\n",
    "   - Cheap (single model) but slower inference\n",
    "   - Good epistemic uncertainty estimation\n",
    "\n",
    "2. **Deep Ensembles:**\n",
    "   - Train multiple independent models\n",
    "   - Model disagreement = uncertainty\n",
    "   - Expensive but excellent uncertainty quality\n",
    "   - State-of-the-art empirical performance\n",
    "\n",
    "3. **Trade-offs:**\n",
    "   - MC Dropout: Low memory, moderate quality\n",
    "   - Ensembles: High memory, high quality\n",
    "   - Choice depends on deployment constraints\n",
    "\n",
    "4. **For AVs:**\n",
    "   - Both methods successfully detect OOD scenarios\n",
    "   - Ensembles preferred for safety-critical applications\n",
    "   - MC Dropout useful for resource-constrained edge deployment\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 17:** Learn how to calibrate model confidence\n",
    "- **Notebook 18:** Learn validation strategies for AV perception\n",
    "\n",
    "---\n",
    "\n",
    "## Interactive Exercise\n",
    "\n",
    "**Try this:**\n",
    "1. Change dropout rate in MC Dropout model (try 0.1, 0.3, 0.5)\n",
    "2. Change number of ensemble models (try M=3, M=10)\n",
    "3. Change number of MC samples (try N=10, N=100)\n",
    "4. How do these affect uncertainty estimates?\n",
    "\n",
    "**Challenge:**\n",
    "- Implement a hybrid approach: Train an ensemble with MC Dropout\n",
    "- Does combining both methods improve uncertainty?\n",
    "\n",
    "**Discussion:**\n",
    "- For a production AV, would you choose MC Dropout or Ensembles? Why?\n",
    "- How would you decide the threshold for triggering fallback behavior?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
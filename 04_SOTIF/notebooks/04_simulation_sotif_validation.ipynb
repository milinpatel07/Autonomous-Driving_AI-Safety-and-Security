{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright (c) 2025 Milin Patel\n",
    "Hochschule Kempten - University of Applied Sciences\n",
    "-->\n",
    "\n",
    "*Copyright (c) 2025 Milin Patel. All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation-Based SOTIF Validation\n",
    "\n",
    "**Module 04: Safety of the Intended Functionality (ISO 21448)**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/04_SOTIF/notebooks/04_simulation_sotif_validation.ipynb)\n",
    "\n",
    "**Author:** Milin Patel  \n",
    "**Institution:** Hochschule Kempten - University of Applied Sciences\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the role of simulation in SOTIF validation\n",
    "- Learn about CARLA simulator for autonomous driving testing\n",
    "- Create SOTIF-related test scenarios programmatically\n",
    "- Evaluate perception system performance under various conditions\n",
    "- Apply metrics for SOTIF acceptance criteria\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "This notebook is based on research:\n",
    "\n",
    "> Patel, M., Jung, R. (2024). \"Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a LiDAR Point Cloud Dataset in a SOTIF-related Use Case.\" VEHITS 2024.\n",
    "\n",
    "Simulation enables testing perception systems across thousands of scenarios that would be impractical or dangerous to test in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulation in SOTIF Validation\n",
    "\n",
    "### Why Simulation?\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Safety** | Test dangerous scenarios without risk |\n",
    "| **Scale** | Run thousands of scenarios automatically |\n",
    "| **Control** | Precise control over environmental conditions |\n",
    "| **Reproducibility** | Exact scenario repetition for debugging |\n",
    "| **Cost** | Lower cost than physical testing |\n",
    "\n",
    "### Simulation Platforms\n",
    "\n",
    "| Platform | Developer | Key Features |\n",
    "|----------|-----------|---------------|\n",
    "| **CARLA** | Intel/CVC | Open-source, sensor simulation, Python API |\n",
    "| **LGSVL** | LG Electronics | Unity-based, ROS integration |\n",
    "| **AirSim** | Microsoft | Unreal Engine, drone + car simulation |\n",
    "| **SUMO** | DLR | Traffic flow simulation |\n",
    "| **PreScan** | Siemens | Commercial, physics-based sensors |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_simulation_role():\n",
    "    \"\"\"Visualize where simulation fits in SOTIF validation.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Validation pyramid\n",
    "    levels = [\n",
    "        ('Simulation\\n(Millions of scenarios)', 0.9, '#3498db'),\n",
    "        ('Test Track\\n(Thousands)', 0.6, '#2ecc71'),\n",
    "        ('Public Road\\n(Hundreds)', 0.35, '#f39c12'),\n",
    "        ('Field Data\\n(Real incidents)', 0.15, '#e74c3c')\n",
    "    ]\n",
    "    \n",
    "    y_base = 0.1\n",
    "    for label, width, color in levels:\n",
    "        rect = plt.Rectangle((0.5-width/2, y_base), width, 0.15,\n",
    "                            facecolor=color, alpha=0.8, edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(0.5, y_base+0.075, label, ha='center', va='center',\n",
    "               fontsize=10, fontweight='bold', color='white')\n",
    "        y_base += 0.17\n",
    "    \n",
    "    # Arrows\n",
    "    ax.annotate('', xy=(0.1, 0.7), xytext=(0.1, 0.15),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    ax.text(0.05, 0.4, 'Coverage', rotation=90, va='center', fontweight='bold')\n",
    "    \n",
    "    ax.annotate('', xy=(0.95, 0.15), xytext=(0.95, 0.7),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    ax.text(0.97, 0.4, 'Fidelity', rotation=90, va='center', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, 1.1)\n",
    "    ax.set_ylim(0, 0.9)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('SOTIF Validation Pyramid - Simulation Role', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_simulation_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SOTIF Test Scenario Design\n",
    "\n",
    "Based on Patel et al. (2024), a SOTIF-related use case includes:\n",
    "\n",
    "- **Ego vehicle** equipped with LiDAR sensor\n",
    "- **Multiple weather conditions** (clear, cloudy, rain)\n",
    "- **Different times of day** (noon, sunset, night)\n",
    "- **Target objects** for detection evaluation\n",
    "\n",
    "### Weather Condition Matrix (21 conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SOTIF test scenario matrix\n",
    "weather_conditions = ['Clear', 'Cloudy', 'Light Rain', 'Heavy Rain', \n",
    "                      'Light Fog', 'Dense Fog', 'Wet Road']\n",
    "times_of_day = ['Morning', 'Noon', 'Sunset', 'Night']\n",
    "\n",
    "# Generate scenario matrix\n",
    "scenarios = []\n",
    "scenario_id = 1\n",
    "for weather in weather_conditions[:3]:  # Subset for demonstration\n",
    "    for time in times_of_day:\n",
    "        scenarios.append({\n",
    "            'ID': f'SC-{scenario_id:03d}',\n",
    "            'Weather': weather,\n",
    "            'Time': time,\n",
    "            'Visibility': np.random.uniform(0.5, 1.0) if weather == 'Clear' else np.random.uniform(0.2, 0.7),\n",
    "            'Precipitation': 0.0 if 'Rain' not in weather else np.random.uniform(0.3, 0.8)\n",
    "        })\n",
    "        scenario_id += 1\n",
    "\n",
    "scenarios_df = pd.DataFrame(scenarios)\n",
    "print(f\"Generated {len(scenarios_df)} test scenarios\")\n",
    "display(scenarios_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulated LiDAR Point Cloud Generation\n",
    "\n",
    "In CARLA, LiDAR sensors generate point clouds representing the 3D environment. For SOTIF validation, we analyze how detection performance varies with conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_point_cloud(n_points=1000, weather='Clear', time='Noon'):\n",
    "    \"\"\"Generate a simulated point cloud with weather effects.\"\"\"\n",
    "    # Base point cloud (vehicle-like shape)\n",
    "    x = np.random.uniform(5, 20, n_points)  # Distance\n",
    "    y = np.random.uniform(-3, 3, n_points)   # Lateral\n",
    "    z = np.random.uniform(-1, 2, n_points)   # Height\n",
    "    \n",
    "    # Add weather-dependent noise\n",
    "    noise_factor = {\n",
    "        'Clear': 0.05,\n",
    "        'Cloudy': 0.08,\n",
    "        'Light Rain': 0.15,\n",
    "        'Heavy Rain': 0.3,\n",
    "        'Light Fog': 0.2,\n",
    "        'Dense Fog': 0.4\n",
    "    }.get(weather, 0.1)\n",
    "    \n",
    "    # Time-dependent intensity\n",
    "    intensity_factor = {\n",
    "        'Morning': 0.9,\n",
    "        'Noon': 1.0,\n",
    "        'Sunset': 0.7,\n",
    "        'Night': 0.5\n",
    "    }.get(time, 0.8)\n",
    "    \n",
    "    # Add noise\n",
    "    x += np.random.normal(0, noise_factor, n_points)\n",
    "    y += np.random.normal(0, noise_factor, n_points)\n",
    "    z += np.random.normal(0, noise_factor * 0.5, n_points)\n",
    "    \n",
    "    # Simulate point dropout (more in bad weather)\n",
    "    dropout_rate = min(0.5, noise_factor * 1.5)\n",
    "    mask = np.random.random(n_points) > dropout_rate\n",
    "    \n",
    "    # Intensity\n",
    "    intensity = np.random.uniform(0.3, 1.0, n_points) * intensity_factor\n",
    "    \n",
    "    return {\n",
    "        'x': x[mask],\n",
    "        'y': y[mask],\n",
    "        'z': z[mask],\n",
    "        'intensity': intensity[mask],\n",
    "        'n_points': sum(mask),\n",
    "        'dropout_rate': dropout_rate\n",
    "    }\n",
    "\n",
    "# Generate point clouds for different conditions\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "conditions = [('Clear', 'Noon'), ('Light Rain', 'Noon'), ('Dense Fog', 'Night')]\n",
    "\n",
    "for i, (weather, time) in enumerate(conditions):\n",
    "    ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
    "    \n",
    "    pc = generate_simulated_point_cloud(n_points=500, weather=weather, time=time)\n",
    "    \n",
    "    scatter = ax.scatter(pc['x'], pc['y'], pc['z'], c=pc['intensity'], \n",
    "                        cmap='viridis', s=5, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('X (forward)')\n",
    "    ax.set_ylabel('Y (lateral)')\n",
    "    ax.set_zlabel('Z (height)')\n",
    "    ax.set_title(f'{weather}, {time}\\n({pc[\"n_points\"]} points)', fontweight='bold')\n",
    "    ax.view_init(elev=20, azim=45)\n",
    "\n",
    "plt.suptitle('Simulated LiDAR Point Clouds Under Different Conditions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Object Detection Performance Evaluation\n",
    "\n",
    "For SOTIF validation, we evaluate detection performance using:\n",
    "\n",
    "- **Average Precision (AP)** - Area under precision-recall curve\n",
    "- **Recall** - Percentage of actual objects detected\n",
    "- **Detection latency** - Processing time\n",
    "\n",
    "Based on Patel et al. (2024), detection methods include:\n",
    "- PointPillars\n",
    "- SECOND\n",
    "- PV-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_detection_performance(weather, time, detector='PointPillars'):\n",
    "    \"\"\"Simulate detection performance based on conditions.\"\"\"\n",
    "    # Base performance by detector\n",
    "    base_ap = {\n",
    "        'PointPillars': 0.75,\n",
    "        'SECOND': 0.78,\n",
    "        'PV-RCNN': 0.82\n",
    "    }.get(detector, 0.75)\n",
    "    \n",
    "    # Weather degradation\n",
    "    weather_factor = {\n",
    "        'Clear': 1.0,\n",
    "        'Cloudy': 0.98,\n",
    "        'Light Rain': 0.85,\n",
    "        'Heavy Rain': 0.65,\n",
    "        'Light Fog': 0.80,\n",
    "        'Dense Fog': 0.55\n",
    "    }.get(weather, 0.9)\n",
    "    \n",
    "    # Time degradation\n",
    "    time_factor = {\n",
    "        'Morning': 0.95,\n",
    "        'Noon': 1.0,\n",
    "        'Sunset': 0.90,\n",
    "        'Night': 0.75\n",
    "    }.get(time, 0.9)\n",
    "    \n",
    "    # Calculate final performance with some randomness\n",
    "    ap = base_ap * weather_factor * time_factor * np.random.uniform(0.95, 1.05)\n",
    "    recall = min(0.99, ap * np.random.uniform(1.0, 1.1))\n",
    "    \n",
    "    return {\n",
    "        'AP': min(1.0, ap),\n",
    "        'Recall': recall,\n",
    "        'Latency_ms': np.random.uniform(30, 80)\n",
    "    }\n",
    "\n",
    "# Evaluate across all scenarios\n",
    "detectors = ['PointPillars', 'SECOND', 'PV-RCNN']\n",
    "results = []\n",
    "\n",
    "for _, scenario in scenarios_df.iterrows():\n",
    "    for detector in detectors:\n",
    "        perf = simulate_detection_performance(scenario['Weather'], scenario['Time'], detector)\n",
    "        results.append({\n",
    "            'Scenario': scenario['ID'],\n",
    "            'Weather': scenario['Weather'],\n",
    "            'Time': scenario['Time'],\n",
    "            'Detector': detector,\n",
    "            **perf\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Detection Performance Summary\")\n",
    "summary = results_df.groupby(['Weather', 'Detector'])['AP'].mean().unstack()\n",
    "display(summary.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection_performance(results_df):\n",
    "    \"\"\"Visualize detection performance across conditions.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # AP by Weather\n",
    "    sns.boxplot(data=results_df, x='Weather', y='AP', hue='Detector', ax=axes[0])\n",
    "    axes[0].set_title('Average Precision by Weather', fontweight='bold')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].axhline(0.7, color='red', linestyle='--', label='SOTIF Threshold')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # AP by Time\n",
    "    sns.boxplot(data=results_df, x='Time', y='AP', hue='Detector', ax=axes[1])\n",
    "    axes[1].set_title('Average Precision by Time of Day', fontweight='bold')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].axhline(0.7, color='red', linestyle='--')\n",
    "    axes[1].get_legend().remove()\n",
    "    \n",
    "    # Recall vs AP scatter\n",
    "    for detector in results_df['Detector'].unique():\n",
    "        subset = results_df[results_df['Detector'] == detector]\n",
    "        axes[2].scatter(subset['AP'], subset['Recall'], label=detector, alpha=0.6, s=50)\n",
    "    axes[2].set_xlabel('Average Precision')\n",
    "    axes[2].set_ylabel('Recall')\n",
    "    axes[2].set_title('Recall vs AP', fontweight='bold')\n",
    "    axes[2].axvline(0.7, color='red', linestyle='--')\n",
    "    axes[2].axhline(0.8, color='red', linestyle='--')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_detection_performance(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SOTIF Acceptance Criteria\n",
    "\n",
    "Based on ISO 21448, we define acceptance criteria for perception performance:\n",
    "\n",
    "| Metric | Threshold | Rationale |\n",
    "|--------|-----------|----------|\n",
    "| AP (Clear) | >= 0.80 | Baseline performance |\n",
    "| AP (Adverse) | >= 0.70 | Degraded but acceptable |\n",
    "| Recall | >= 0.85 | Safety-critical detection |\n",
    "| Latency | <= 100ms | Real-time requirement |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sotif_acceptance(results_df):\n",
    "    \"\"\"Evaluate SOTIF acceptance criteria.\"\"\"\n",
    "    criteria = {\n",
    "        'AP_Clear': 0.80,\n",
    "        'AP_Adverse': 0.70,\n",
    "        'Recall': 0.85,\n",
    "        'Latency_ms': 100\n",
    "    }\n",
    "    \n",
    "    evaluation = []\n",
    "    \n",
    "    for detector in results_df['Detector'].unique():\n",
    "        subset = results_df[results_df['Detector'] == detector]\n",
    "        clear = subset[subset['Weather'] == 'Clear']\n",
    "        adverse = subset[subset['Weather'] != 'Clear']\n",
    "        \n",
    "        ap_clear = clear['AP'].mean()\n",
    "        ap_adverse = adverse['AP'].mean()\n",
    "        recall = subset['Recall'].mean()\n",
    "        latency = subset['Latency_ms'].mean()\n",
    "        \n",
    "        evaluation.append({\n",
    "            'Detector': detector,\n",
    "            'AP (Clear)': ap_clear,\n",
    "            'AP (Adverse)': ap_adverse,\n",
    "            'Recall': recall,\n",
    "            'Latency (ms)': latency,\n",
    "            'Pass Clear': 'PASS' if ap_clear >= criteria['AP_Clear'] else 'FAIL',\n",
    "            'Pass Adverse': 'PASS' if ap_adverse >= criteria['AP_Adverse'] else 'FAIL',\n",
    "            'Pass Recall': 'PASS' if recall >= criteria['Recall'] else 'FAIL',\n",
    "            'Pass Latency': 'PASS' if latency <= criteria['Latency_ms'] else 'FAIL'\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(evaluation)\n",
    "    \n",
    "    # Overall pass/fail\n",
    "    eval_df['SOTIF Status'] = eval_df.apply(\n",
    "        lambda r: 'ACCEPTED' if all([\n",
    "            r['Pass Clear'] == 'PASS',\n",
    "            r['Pass Adverse'] == 'PASS',\n",
    "            r['Pass Recall'] == 'PASS',\n",
    "            r['Pass Latency'] == 'PASS'\n",
    "        ]) else 'REJECTED',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "eval_results = evaluate_sotif_acceptance(results_df)\n",
    "print(\"SOTIF Acceptance Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "display(eval_results[['Detector', 'AP (Clear)', 'AP (Adverse)', 'Recall', 'SOTIF Status']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identifying Triggering Conditions from Simulation\n",
    "\n",
    "Simulation helps identify specific conditions where performance drops below acceptable thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_triggering_conditions(results_df, ap_threshold=0.7):\n",
    "    \"\"\"Identify conditions causing performance below threshold.\"\"\"\n",
    "    failing = results_df[results_df['AP'] < ap_threshold].copy()\n",
    "    \n",
    "    if len(failing) > 0:\n",
    "        print(f\"Identified {len(failing)} scenarios below AP threshold ({ap_threshold})\")\n",
    "        print(\"\\nTriggering Condition Summary:\")\n",
    "        print(failing.groupby(['Weather', 'Time'])['AP'].agg(['count', 'mean']).round(3))\n",
    "        return failing\n",
    "    else:\n",
    "        print(\"No scenarios below threshold\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "triggering = identify_triggering_conditions(results_df, ap_threshold=0.70)\n",
    "\n",
    "if len(triggering) > 0:\n",
    "    print(\"\\nRecommended Mitigations:\")\n",
    "    print(\"- Restrict ODD to exclude identified conditions\")\n",
    "    print(\"- Improve model robustness through data augmentation\")\n",
    "    print(\"- Add redundant sensing for adverse conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise: Design Your SOTIF Test Campaign\n",
    "\n",
    "**Task:** Design a simulation-based test campaign for a camera-based pedestrian detection system.\n",
    "\n",
    "Consider:\n",
    "- What weather conditions to test?\n",
    "- What lighting conditions?\n",
    "- What pedestrian variations (clothing, pose, occlusion)?\n",
    "- What acceptance criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Design your test campaign\n",
    "\n",
    "# TODO: Define weather conditions\n",
    "# camera_weather = [...]\n",
    "\n",
    "# TODO: Define lighting conditions\n",
    "# camera_lighting = [...]\n",
    "\n",
    "# TODO: Define pedestrian variations\n",
    "# pedestrian_vars = [...]\n",
    "\n",
    "# TODO: Define acceptance criteria\n",
    "# criteria = {...}\n",
    "\n",
    "print(\"Design your camera-based pedestrian detection test campaign.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "- **Simulation Role**: Critical for SOTIF validation at scale\n",
    "- **Scenario Design**: Weather, time, and environmental condition matrices\n",
    "- **Performance Metrics**: AP, Recall for detection evaluation\n",
    "- **Acceptance Criteria**: Thresholds for SOTIF compliance\n",
    "- **Triggering Condition Identification**: Finding failure modes through systematic testing\n",
    "\n",
    "### References\n",
    "\n",
    "- Patel, M., Jung, R. (2024). \"Simulation-Based Performance Evaluation of 3D Object Detection Methods.\" VEHITS.\n",
    "- Patel, M. (2024). \"SOTIF PCOD Dataset.\" IEEE DataPort.\n",
    "- ISO 21448:2022 - Safety of the intended functionality\n",
    "- CARLA Simulator Documentation: https://carla.org\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created by Milin Patel | Hochschule Kempten*  \n",
    "*Last updated: 2025-01-22*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

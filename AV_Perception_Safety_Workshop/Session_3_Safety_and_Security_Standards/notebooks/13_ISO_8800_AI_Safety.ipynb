{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright (c) 2025 Milin Patel\n",
    "Hochschule Kempten - University of Applied Sciences\n",
    "\n",
    "Autonomous Driving: AI Safety and Security Workshop\n",
    "This project is licensed under the MIT License.\n",
    "See LICENSE file in the root directory for full license text.\n",
    "-->\n",
    "\n",
    "*Copyright © 2025 Milin Patel. All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISO/IEC 8800 Series: AI Safety for Autonomous Systems\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/AV_Perception_Safety_Workshop/Session_3_Safety_and_Security_Standards/notebooks/13_ISO_8800_AI_Safety.ipynb)\n",
    "\n",
    "**Duration:** 25 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand AI-specific safety principles and challenges\n",
    "- Master trustworthiness characteristics for AI systems\n",
    "- Learn data quality requirements for safety-critical AI\n",
    "- Apply model monitoring and retraining strategies\n",
    "- Integrate AI safety with ISO 26262 and SOTIF\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import required libraries\n",
    "import sys\n",
    "\n",
    "# Install packages if in Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q matplotlib pandas numpy seaborn scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch, Circle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AI Safety Standards Overview\n",
    "\n",
    "### Relevant Standards\n",
    "\n",
    "1. **ISO/IEC TR 5469**: Artificial Intelligence — Functional safety and AI systems\n",
    "2. **ISO/IEC 23894**: Information technology — AI — Guidance on risk management\n",
    "3. **ISO/IEC 24028**: AI — Overview of trustworthiness in AI\n",
    "4. **ISO/IEC 42001**: AI Management System\n",
    "5. **NIST AI Risk Management Framework**\n",
    "\n",
    "### Why AI Requires Special Safety Considerations\n",
    "\n",
    "**Traditional Software:**\n",
    "- Deterministic behavior\n",
    "- Explicit rules\n",
    "- Predictable failures\n",
    "- Verifiable through testing\n",
    "\n",
    "**AI/ML Systems:**\n",
    "- Non-deterministic (statistical)\n",
    "- Learned patterns\n",
    "- Emergent failures\n",
    "- Difficult to fully verify\n",
    "\n",
    "### Key AI Safety Challenges\n",
    "\n",
    "1. **Opacity**: \"Black box\" models are hard to interpret\n",
    "2. **Data dependency**: Performance tied to training data quality\n",
    "3. **Distribution shift**: Performance degrades on out-of-distribution data\n",
    "4. **Adversarial vulnerability**: Susceptible to malicious inputs\n",
    "5. **Emergent behavior**: Unexpected behavior in novel situations\n",
    "6. **Continuous learning**: Model updates can introduce new risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize traditional software vs AI/ML safety paradigm\n",
    "def visualize_safety_paradigm():\n",
    "    \"\"\"\n",
    "    Compare traditional software and AI/ML safety approaches\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Traditional Software (left)\n",
    "    trad_levels = [\n",
    "        {'y': 0.8, 'text': 'Requirements', 'color': '#3498db'},\n",
    "        {'y': 0.6, 'text': 'Design & Code', 'color': '#3498db'},\n",
    "        {'y': 0.4, 'text': 'Testing\\n(deterministic)', 'color': '#3498db'},\n",
    "        {'y': 0.2, 'text': 'Verification\\n(complete)', 'color': '#2ecc71'}\n",
    "    ]\n",
    "    \n",
    "    for level in trad_levels:\n",
    "        rect = FancyBboxPatch((0.15, level['y']-0.08), 0.7, 0.12,\n",
    "                             boxstyle=\"round,pad=0.01\",\n",
    "                             facecolor=level['color'], edgecolor='black',\n",
    "                             linewidth=2, alpha=0.7)\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(0.5, level['y'], level['text'], ha='center', va='center',\n",
    "                fontsize=12, fontweight='bold', color='white')\n",
    "    \n",
    "    ax1.text(0.5, 0.95, 'Traditional Software', ha='center', va='top',\n",
    "            fontsize=14, fontweight='bold')\n",
    "    ax1.text(0.5, 0.05, 'Deterministic\\nComplete Verification Possible',\n",
    "            ha='center', va='bottom', fontsize=10, style='italic',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # AI/ML System (right)\n",
    "    ai_levels = [\n",
    "        {'y': 0.85, 'text': 'Requirements', 'color': '#e74c3c'},\n",
    "        {'y': 0.70, 'text': 'Data Collection\\n& Labeling', 'color': '#e74c3c'},\n",
    "        {'y': 0.55, 'text': 'Model Training\\n& Tuning', 'color': '#e74c3c'},\n",
    "        {'y': 0.40, 'text': 'Testing\\n(statistical)', 'color': '#f39c12'},\n",
    "        {'y': 0.25, 'text': 'Validation\\n(scenario-based)', 'color': '#f39c12'},\n",
    "        {'y': 0.10, 'text': 'Runtime Monitoring\\n(ongoing)', 'color': '#9b59b6'}\n",
    "    ]\n",
    "    \n",
    "    for level in ai_levels:\n",
    "        rect = FancyBboxPatch((0.15, level['y']-0.05), 0.7, 0.09,\n",
    "                             boxstyle=\"round,pad=0.01\",\n",
    "                             facecolor=level['color'], edgecolor='black',\n",
    "                             linewidth=2, alpha=0.7)\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(0.5, level['y'], level['text'], ha='center', va='center',\n",
    "                fontsize=10, fontweight='bold', color='white')\n",
    "    \n",
    "    ax2.text(0.5, 0.97, 'AI/ML System', ha='center', va='top',\n",
    "            fontsize=14, fontweight='bold')\n",
    "    ax2.text(0.5, 0.02, 'Statistical\\nContinuous Validation Required',\n",
    "            ha='center', va='bottom', fontsize=10, style='italic',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    fig.suptitle('Safety Paradigm: Traditional Software vs AI/ML', \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_safety_paradigm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trustworthiness Characteristics\n",
    "\n",
    "ISO/IEC 24028 defines key trustworthiness characteristics:\n",
    "\n",
    "### 1. Robustness\n",
    "- **Definition**: Ability to maintain performance under adverse conditions\n",
    "- **Key aspects**:\n",
    "  - Noise tolerance\n",
    "  - Adversarial robustness\n",
    "  - Distribution shift resilience\n",
    "  - Graceful degradation\n",
    "\n",
    "### 2. Explainability/Interpretability\n",
    "- **Definition**: Ability to explain decisions and behavior\n",
    "- **Key aspects**:\n",
    "  - Feature importance\n",
    "  - Decision rationale\n",
    "  - Model transparency\n",
    "  - Debugging capability\n",
    "\n",
    "### 3. Transparency\n",
    "- **Definition**: Openness about capabilities and limitations\n",
    "- **Key aspects**:\n",
    "  - Model documentation\n",
    "  - Training data disclosure\n",
    "  - Performance metrics\n",
    "  - Known limitations\n",
    "\n",
    "### 4. Fairness/Bias Assessment\n",
    "- **Definition**: Absence of unfair bias\n",
    "- **Key aspects**:\n",
    "  - Dataset representativeness\n",
    "  - Demographic parity\n",
    "  - Equal opportunity\n",
    "  - Bias detection and mitigation\n",
    "\n",
    "### 5. Reliability\n",
    "- **Definition**: Consistent performance over time\n",
    "- **Key aspects**:\n",
    "  - Predictable behavior\n",
    "  - Low variance\n",
    "  - Stability\n",
    "  - Repeatability\n",
    "\n",
    "### 6. Safety\n",
    "- **Definition**: Ability to avoid hazardous behavior\n",
    "- **Key aspects**:\n",
    "  - Hazard analysis\n",
    "  - Safety constraints\n",
    "  - Fail-safe mechanisms\n",
    "  - Risk mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trustworthiness assessment framework\n",
    "def assess_trustworthiness():\n",
    "    \"\"\"\n",
    "    Framework for assessing AI system trustworthiness\n",
    "    \"\"\"\n",
    "    # Define assessment criteria (0-10 scale)\n",
    "    characteristics = [\n",
    "        'Robustness',\n",
    "        'Explainability',\n",
    "        'Transparency',\n",
    "        'Fairness',\n",
    "        'Reliability',\n",
    "        'Safety'\n",
    "    ]\n",
    "    \n",
    "    # Example assessments for three different systems\n",
    "    systems = {\n",
    "        'Traditional CNN\\n(No Safety Measures)': [6, 3, 4, 5, 6, 4],\n",
    "        'Enhanced CNN\\n(Basic Safety)': [7, 5, 6, 6, 7, 6],\n",
    "        'Safety-Critical AI\\n(Full Measures)': [9, 8, 9, 8, 9, 9]\n",
    "    }\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2 * np.pi, len(characteristics), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), \n",
    "                                   subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each system\n",
    "    colors = ['#e74c3c', '#f39c12', '#2ecc71']\n",
    "    for (name, scores), color in zip(systems.items(), colors):\n",
    "        scores_plot = scores + scores[:1]\n",
    "        ax1.plot(angles, scores_plot, 'o-', linewidth=2.5, label=name, color=color)\n",
    "        ax1.fill(angles, scores_plot, alpha=0.15, color=color)\n",
    "    \n",
    "    ax1.set_xticks(angles[:-1])\n",
    "    ax1.set_xticklabels(characteristics, fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylim(0, 10)\n",
    "    ax1.set_yticks([2, 4, 6, 8, 10])\n",
    "    ax1.set_yticklabels(['2', '4', '6', '8', '10'])\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.set_title('AI Trustworthiness Assessment', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    \n",
    "    # Target profile for safety-critical automotive AI\n",
    "    target = [9, 7, 9, 8, 9, 10]  # Higher requirements for safety\n",
    "    minimum = [7, 5, 6, 6, 7, 8]  # Minimum acceptable\n",
    "    \n",
    "    target_plot = target + target[:1]\n",
    "    minimum_plot = minimum + minimum[:1]\n",
    "    \n",
    "    ax2.plot(angles, target_plot, 'o-', linewidth=3, label='Target Profile', color='green')\n",
    "    ax2.fill(angles, target_plot, alpha=0.2, color='green')\n",
    "    ax2.plot(angles, minimum_plot, 'o-', linewidth=3, label='Minimum Acceptable', color='orange')\n",
    "    ax2.fill(angles, minimum_plot, alpha=0.2, color='orange')\n",
    "    \n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(characteristics, fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylim(0, 10)\n",
    "    ax2.set_yticks([2, 4, 6, 8, 10])\n",
    "    ax2.set_yticklabels(['2', '4', '6', '8', '10'])\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax2.set_title('Automotive AI Safety Requirements', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create comparison table\n",
    "    df = pd.DataFrame(systems, index=characteristics).T\n",
    "    df['Average'] = df.mean(axis=1).round(1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Trustworthiness Assessment Scores (0-10 scale)\")\n",
    "    print(\"=\"*80)\n",
    "    display(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_trust = assess_trustworthiness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Requirements\n",
    "\n",
    "### \"Garbage In, Garbage Out\" - Critical for Safety!\n",
    "\n",
    "Data quality directly impacts AI safety. Key dimensions:\n",
    "\n",
    "### 1. Completeness\n",
    "- **ODD Coverage**: Data covers all operational scenarios\n",
    "- **Edge cases**: Rare but safety-critical scenarios included\n",
    "- **Environmental conditions**: Weather, lighting, road types\n",
    "- **Diversity**: Geographic, demographic, temporal variation\n",
    "\n",
    "### 2. Accuracy\n",
    "- **Label quality**: Ground truth correctness\n",
    "- **Annotation consistency**: Inter-annotator agreement\n",
    "- **Sensor calibration**: Accurate sensor data\n",
    "- **Validation**: Independent verification\n",
    "\n",
    "### 3. Representativeness\n",
    "- **Distribution matching**: Training ~ Deployment\n",
    "- **Balanced classes**: Avoid severe imbalance\n",
    "- **Temporal coverage**: Different times/seasons\n",
    "- **No bias**: Fair representation of all groups\n",
    "\n",
    "### 4. Provenance and Traceability\n",
    "- **Source tracking**: Where data came from\n",
    "- **Version control**: Dataset versioning\n",
    "- **Processing history**: Transformations applied\n",
    "- **Audit trail**: Full lineage documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment tool\n",
    "class DataQualityAssessment:\n",
    "    \"\"\"\n",
    "    Assess training data quality for safety-critical AI\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name: str):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def assess_completeness(self, odd_coverage: float, edge_case_coverage: float):\n",
    "        \"\"\"\n",
    "        Assess data completeness\n",
    "        \"\"\"\n",
    "        self.metrics['ODD Coverage (%)'] = odd_coverage\n",
    "        self.metrics['Edge Case Coverage (%)'] = edge_case_coverage\n",
    "        self.metrics['Completeness Score'] = (odd_coverage + edge_case_coverage) / 2\n",
    "    \n",
    "    def assess_accuracy(self, label_accuracy: float, inter_annotator_agreement: float):\n",
    "        \"\"\"\n",
    "        Assess data accuracy\n",
    "        \"\"\"\n",
    "        self.metrics['Label Accuracy (%)'] = label_accuracy\n",
    "        self.metrics['Inter-Annotator Agreement'] = inter_annotator_agreement\n",
    "        self.metrics['Accuracy Score'] = (label_accuracy + inter_annotator_agreement * 100) / 2\n",
    "    \n",
    "    def assess_balance(self, class_distribution: dict):\n",
    "        \"\"\"\n",
    "        Assess class balance\n",
    "        \"\"\"\n",
    "        total = sum(class_distribution.values())\n",
    "        proportions = {k: v/total for k, v in class_distribution.items()}\n",
    "        \n",
    "        # Calculate imbalance ratio (max/min)\n",
    "        max_prop = max(proportions.values())\n",
    "        min_prop = min(proportions.values())\n",
    "        imbalance_ratio = max_prop / min_prop if min_prop > 0 else float('inf')\n",
    "        \n",
    "        # Score: lower imbalance = higher score\n",
    "        balance_score = max(0, 100 - (imbalance_ratio - 1) * 10)\n",
    "        \n",
    "        self.metrics['Imbalance Ratio'] = imbalance_ratio\n",
    "        self.metrics['Balance Score'] = min(100, balance_score)\n",
    "        \n",
    "        return proportions\n",
    "    \n",
    "    def overall_score(self):\n",
    "        \"\"\"\n",
    "        Calculate overall data quality score\n",
    "        \"\"\"\n",
    "        scores = [\n",
    "            self.metrics.get('Completeness Score', 0),\n",
    "            self.metrics.get('Accuracy Score', 0),\n",
    "            self.metrics.get('Balance Score', 0)\n",
    "        ]\n",
    "        return np.mean(scores)\n",
    "\n",
    "# Example assessment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Quality Assessment Example\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scenario 1: Poor quality dataset\n",
    "dqa_poor = DataQualityAssessment(\"Dataset A: Limited Collection\")\n",
    "dqa_poor.assess_completeness(odd_coverage=70, edge_case_coverage=30)\n",
    "dqa_poor.assess_accuracy(label_accuracy=85, inter_annotator_agreement=0.75)\n",
    "props_poor = dqa_poor.assess_balance({\n",
    "    'Pedestrian': 1000,\n",
    "    'Cyclist': 200,\n",
    "    'Vehicle': 5000,\n",
    "    'Other': 50\n",
    "})\n",
    "\n",
    "# Scenario 2: Good quality dataset\n",
    "dqa_good = DataQualityAssessment(\"Dataset B: Comprehensive Collection\")\n",
    "dqa_good.assess_completeness(odd_coverage=95, edge_case_coverage=85)\n",
    "dqa_good.assess_accuracy(label_accuracy=98, inter_annotator_agreement=0.92)\n",
    "props_good = dqa_good.assess_balance({\n",
    "    'Pedestrian': 3000,\n",
    "    'Cyclist': 2500,\n",
    "    'Vehicle': 4000,\n",
    "    'Other': 2000\n",
    "})\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Overall scores comparison\n",
    "datasets = ['Dataset A\\n(Poor)', 'Dataset B\\n(Good)']\n",
    "overall_scores = [dqa_poor.overall_score(), dqa_good.overall_score()]\n",
    "colors_score = ['#e74c3c' if s < 80 else '#2ecc71' for s in overall_scores]\n",
    "\n",
    "bars = ax1.bar(datasets, overall_scores, color=colors_score, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=95, color='green', linestyle='--', linewidth=2, label='Excellent (≥95)')\n",
    "ax1.axhline(y=80, color='orange', linestyle='--', linewidth=2, label='Acceptable (≥80)')\n",
    "ax1.set_ylabel('Overall Quality Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Overall Data Quality Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim(0, 105)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, overall_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f'{score:.1f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Detailed metrics comparison\n",
    "metrics_to_plot = ['Completeness Score', 'Accuracy Score', 'Balance Score']\n",
    "x_pos = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "poor_values = [dqa_poor.metrics[m] for m in metrics_to_plot]\n",
    "good_values = [dqa_good.metrics[m] for m in metrics_to_plot]\n",
    "\n",
    "ax2.bar(x_pos - width/2, poor_values, width, label='Dataset A (Poor)', \n",
    "       color='#e74c3c', edgecolor='black', linewidth=2)\n",
    "ax2.bar(x_pos + width/2, good_values, width, label='Dataset B (Good)',\n",
    "       color='#2ecc71', edgecolor='black', linewidth=2)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(['Completeness', 'Accuracy', 'Balance'], rotation=0)\n",
    "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Detailed Quality Metrics', fontsize=13, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 105)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Class distribution - Poor dataset\n",
    "ax3.pie(props_poor.values(), labels=props_poor.keys(), autopct='%1.1f%%',\n",
    "       startangle=90, colors=sns.color_palette('Reds', len(props_poor)))\n",
    "ax3.set_title(f'Dataset A: Class Distribution\\n(Imbalance Ratio: {dqa_poor.metrics[\"Imbalance Ratio\"]:.1f})',\n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "# Class distribution - Good dataset\n",
    "ax4.pie(props_good.values(), labels=props_good.keys(), autopct='%1.1f%%',\n",
    "       startangle=90, colors=sns.color_palette('Greens', len(props_good)))\n",
    "ax4.set_title(f'Dataset B: Class Distribution\\n(Imbalance Ratio: {dqa_good.metrics[\"Imbalance Ratio\"]:.1f})',\n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nDataset A (Poor Quality):\")\n",
    "for key, value in dqa_poor.metrics.items():\n",
    "    print(f\"  {key}: {value:.2f}\")\n",
    "print(f\"  Overall Score: {dqa_poor.overall_score():.2f}\")\n",
    "\n",
    "print(\"\\nDataset B (Good Quality):\")\n",
    "for key, value in dqa_good.metrics.items():\n",
    "    print(f\"  {key}: {value:.2f}\")\n",
    "print(f\"  Overall Score: {dqa_good.overall_score():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Recommendations:\")\n",
    "print(\"=\"*80)\n",
    "if dqa_poor.overall_score() < 80:\n",
    "    print(\"⚠️  Dataset A does NOT meet safety-critical requirements\")\n",
    "    print(\"   - Improve edge case coverage\")\n",
    "    print(\"   - Balance class distribution\")\n",
    "    print(\"   - Increase label accuracy\")\n",
    "if dqa_good.overall_score() >= 95:\n",
    "    print(\"✓ Dataset B meets safety-critical requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Monitoring and Retraining Strategies\n",
    "\n",
    "### Runtime Monitoring\n",
    "\n",
    "Continuous monitoring is essential for AI safety:\n",
    "\n",
    "1. **Performance Monitoring**\n",
    "   - Detection accuracy\n",
    "   - False positive/negative rates\n",
    "   - Processing latency\n",
    "   - Resource utilization\n",
    "\n",
    "2. **Distribution Monitoring**\n",
    "   - Input distribution drift\n",
    "   - Out-of-distribution detection\n",
    "   - Concept drift\n",
    "   - Covariate shift\n",
    "\n",
    "3. **Confidence Monitoring**\n",
    "   - Prediction uncertainty\n",
    "   - Calibration quality\n",
    "   - Low-confidence warnings\n",
    "\n",
    "4. **Anomaly Detection**\n",
    "   - Unusual input patterns\n",
    "   - Adversarial attacks\n",
    "   - Sensor malfunctions\n",
    "\n",
    "### Retraining Triggers\n",
    "\n",
    "When to retrain the model:\n",
    "\n",
    "1. **Performance degradation** below threshold\n",
    "2. **Significant distribution shift** detected\n",
    "3. **New edge cases** discovered in field\n",
    "4. **Safety incidents** requiring model update\n",
    "5. **Scheduled updates** (e.g., seasonal)\n",
    "\n",
    "### Safe Retraining Process\n",
    "\n",
    "1. Collect new data (including failure cases)\n",
    "2. Augment training dataset\n",
    "3. Retrain model\n",
    "4. **Regression testing**: Ensure no performance degradation on old scenarios\n",
    "5. Validation on new scenarios\n",
    "6. A/B testing or shadow mode\n",
    "7. Gradual rollout\n",
    "8. Monitor post-deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate model monitoring and drift detection\ndef simulate_model_monitoring():\n    \"\"\"\n    Simulate runtime model monitoring with drift detection\n    \"\"\"\n    np.random.seed(42)\n    \n    # Simulate 6 months of operation (180 days)\n    days = np.arange(0, 180)\n    \n    # Performance metrics\n    # Initial performance is good, degrades after day 100 (seasonal change)\n    base_accuracy = 0.96\n    accuracy = base_accuracy + 0.01 * np.random.randn(len(days))\n    \n    # Introduce drift after day 100\n    drift_start = 100\n    for i in range(drift_start, len(days)):\n        drift_factor = (i - drift_start) / 50  # Gradual degradation\n        accuracy[i] -= 0.03 * drift_factor + 0.01 * np.random.randn()\n    \n    accuracy = np.clip(accuracy, 0.80, 0.99)\n    \n    # Confidence scores\n    confidence = 0.92 + 0.02 * np.random.randn(len(days))\n    for i in range(drift_start, len(days)):\n        drift_factor = (i - drift_start) / 50\n        confidence[i] -= 0.04 * drift_factor + 0.01 * np.random.randn()\n    confidence = np.clip(confidence, 0.75, 0.98)\n    \n    # Distribution drift metric (KL divergence approximation)\n    drift_metric = 0.1 + 0.05 * np.random.rand(len(days))\n    for i in range(drift_start, len(days)):\n        drift_factor = (i - drift_start) / 40\n        drift_metric[i] += 0.3 * drift_factor + 0.05 * np.random.rand()\n    drift_metric = np.clip(drift_metric, 0, 1.5)\n    \n    # Visualization\n    fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n    \n    # Accuracy over time\n    axes[0].plot(days, accuracy, linewidth=2, color='#3498db', label='Model Accuracy')\n    axes[0].axhline(y=0.95, color='green', linestyle='--', linewidth=2, label='Target (95%)')\n    axes[0].axhline(y=0.90, color='orange', linestyle='--', linewidth=2, label='Warning (90%)')\n    axes[0].axhline(y=0.85, color='red', linestyle='--', linewidth=2, label='Critical (85%)')\n    axes[0].axvline(x=drift_start, color='purple', linestyle=':', linewidth=2.5, \n                   label='Drift Start (Seasonal Change)')\n    axes[0].fill_between(days, 0.95, 1.0, alpha=0.2, color='green')\n    axes[0].fill_between(days, 0.90, 0.95, alpha=0.2, color='yellow')\n    axes[0].fill_between(days, 0.85, 0.90, alpha=0.2, color='orange')\n    axes[0].fill_between(days, 0, 0.85, alpha=0.2, color='red')\n    axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n    axes[0].set_title('Model Performance Monitoring', fontsize=14, fontweight='bold')\n    axes[0].legend(loc='lower left')\n    axes[0].grid(alpha=0.3)\n    axes[0].set_ylim(0.80, 1.0)\n    \n    # Confidence over time\n    axes[1].plot(days, confidence, linewidth=2, color='#9b59b6', label='Avg Confidence')\n    axes[1].axhline(y=0.90, color='green', linestyle='--', linewidth=2, label='Target (90%)')\n    axes[1].axhline(y=0.80, color='orange', linestyle='--', linewidth=2, label='Warning (80%)')\n    axes[1].axvline(x=drift_start, color='purple', linestyle=':', linewidth=2.5)\n    axes[1].fill_between(days, 0.90, 1.0, alpha=0.2, color='green')\n    axes[1].fill_between(days, 0.80, 0.90, alpha=0.2, color='yellow')\n    axes[1].fill_between(days, 0, 0.80, alpha=0.2, color='red')\n    axes[1].set_ylabel('Confidence Score', fontsize=12, fontweight='bold')\n    axes[1].set_title('Prediction Confidence Monitoring', fontsize=14, fontweight='bold')\n    axes[1].legend(loc='lower left')\n    axes[1].grid(alpha=0.3)\n    axes[1].set_ylim(0.70, 1.0)\n    \n    # Distribution drift\n    axes[2].plot(days, drift_metric, linewidth=2, color='#e74c3c', label='Drift Metric')\n    axes[2].axhline(y=0.3, color='orange', linestyle='--', linewidth=2, label='Warning Threshold')\n    axes[2].axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Critical Threshold')\n    axes[2].axvline(x=drift_start, color='purple', linestyle=':', linewidth=2.5)\n    axes[2].fill_between(days, 0, 0.3, alpha=0.2, color='green')\n    axes[2].fill_between(days, 0.3, 0.5, alpha=0.2, color='yellow')\n    axes[2].fill_between(days, 0.5, 2.0, alpha=0.2, color='red')\n    axes[2].set_xlabel('Days of Operation', fontsize=12, fontweight='bold')\n    axes[2].set_ylabel('Drift Metric', fontsize=12, fontweight='bold')\n    axes[2].set_title('Distribution Drift Detection', fontsize=14, fontweight='bold')\n    axes[2].legend(loc='upper left')\n    axes[2].grid(alpha=0.3)\n    axes[2].set_ylim(0, 1.6)\n    \n    # Highlight retraining trigger\n    trigger_days = np.where((accuracy < 0.90) | (drift_metric > 0.5))[0]\n    triggered = len(trigger_days) > 0\n    if triggered:\n        trigger_day = trigger_days[0]\n        for ax in axes:\n            ax.axvline(x=trigger_day, color='red', linestyle='-', linewidth=3, \n                      label='Retraining Triggered', alpha=0.7)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Analysis\n    print(\"\\n\" + \"=\"*80)\n    print(\"Model Monitoring Analysis\")\n    print(\"=\"*80)\n    print(f\"\\nInitial Performance (Days 0-{drift_start}):\")\n    print(f\"  Average Accuracy: {np.mean(accuracy[:drift_start]):.3f}\")\n    print(f\"  Average Confidence: {np.mean(confidence[:drift_start]):.3f}\")\n    print(f\"  Average Drift: {np.mean(drift_metric[:drift_start]):.3f}\")\n    \n    print(f\"\\nPost-Drift Performance (Days {drift_start}-180):\")\n    print(f\"  Average Accuracy: {np.mean(accuracy[drift_start:]):.3f}\")\n    print(f\"  Average Confidence: {np.mean(confidence[drift_start:]):.3f}\")\n    print(f\"  Average Drift: {np.mean(drift_metric[drift_start:]):.3f}\")\n    \n    if triggered:\n        print(f\"\\n⚠️  RETRAINING TRIGGERED at Day {trigger_day}\")\n        print(f\"   Reason: Accuracy = {accuracy[trigger_day]:.3f} (< 0.90 threshold)\")\n        print(f\"   Drift metric = {drift_metric[trigger_day]:.3f}\")\n        print(\"\\n   Recommended Actions:\")\n        print(\"   1. Collect data from recent failures\")\n        print(\"   2. Analyze drift causes (seasonal, environmental)\")\n        print(\"   3. Augment training dataset\")\n        print(\"   4. Retrain and validate model\")\n        print(\"   5. Perform regression testing\")\n        print(\"   6. Deploy with monitoring\")\n\nsimulate_model_monitoring()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Human-AI Interaction Safety\n",
    "\n",
    "### Safe Handover and Takeover\n",
    "\n",
    "For SAE Level 2-3 systems, human-AI interaction is critical:\n",
    "\n",
    "1. **Driver Monitoring**\n",
    "   - Attention tracking\n",
    "   - Readiness assessment\n",
    "   - Distraction detection\n",
    "\n",
    "2. **Takeover Requests (TOR)**\n",
    "   - Sufficient warning time\n",
    "   - Clear visual/audio cues\n",
    "   - Graceful degradation if no response\n",
    "\n",
    "3. **Mode Confusion Prevention**\n",
    "   - Clear indication of automation level\n",
    "   - Unambiguous system status\n",
    "   - Intuitive interfaces\n",
    "\n",
    "4. **Trust Calibration**\n",
    "   - Avoid over-trust (complacency)\n",
    "   - Avoid under-trust (disuse)\n",
    "   - Transparent limitations\n",
    "\n",
    "### Explainability for Safety\n",
    "\n",
    "- **Runtime explanations**: Why the system made a decision\n",
    "- **Failure explanations**: Why the system cannot perform\n",
    "- **Counterfactual explanations**: What would change the decision\n",
    "- **Confidence communication**: Uncertainty indication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize human-AI interaction safety framework\n",
    "def visualize_human_ai_safety():\n",
    "    \"\"\"\n",
    "    Visualize human-AI interaction safety components\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Central AI system\n",
    "    center_circle = Circle((0.5, 0.5), 0.12, facecolor='#3498db', \n",
    "                          edgecolor='black', linewidth=3)\n",
    "    ax.add_patch(center_circle)\n",
    "    ax.text(0.5, 0.5, 'AI\\nPerception\\nSystem', ha='center', va='center',\n",
    "           fontsize=12, fontweight='bold', color='white')\n",
    "    \n",
    "    # Safety components around the center\n",
    "    components = [\n",
    "        {'angle': 0, 'name': 'Driver\\nMonitoring', 'color': '#e74c3c'},\n",
    "        {'angle': 60, 'name': 'Confidence\\nReporting', 'color': '#9b59b6'},\n",
    "        {'angle': 120, 'name': 'Explainability', 'color': '#f39c12'},\n",
    "        {'angle': 180, 'name': 'Mode\\nIndication', 'color': '#2ecc71'},\n",
    "        {'angle': 240, 'name': 'Takeover\\nRequest', 'color': '#e67e22'},\n",
    "        {'angle': 300, 'name': 'Graceful\\nDegradation', 'color': '#1abc9c'}\n",
    "    ]\n",
    "    \n",
    "    radius = 0.28\n",
    "    for comp in components:\n",
    "        angle_rad = np.deg2rad(comp['angle'])\n",
    "        x = 0.5 + radius * np.cos(angle_rad)\n",
    "        y = 0.5 + radius * np.sin(angle_rad)\n",
    "        \n",
    "        circle = Circle((x, y), 0.08, facecolor=comp['color'],\n",
    "                       edgecolor='black', linewidth=2, alpha=0.8)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, comp['name'], ha='center', va='center',\n",
    "               fontsize=9, fontweight='bold', color='white')\n",
    "        \n",
    "        # Connection to center\n",
    "        ax.plot([0.5, x], [0.5, y], 'k--', linewidth=1.5, alpha=0.5)\n",
    "    \n",
    "    # Outer safety principles\n",
    "    principles = [\n",
    "        {'x': 0.15, 'y': 0.85, 'text': 'Transparency\\n& Trust'},\n",
    "        {'x': 0.85, 'y': 0.85, 'text': 'Situation\\nAwareness'},\n",
    "        {'x': 0.15, 'y': 0.15, 'text': 'Fail-Safe\\nBehavior'},\n",
    "        {'x': 0.85, 'y': 0.15, 'text': 'Clear\\nCommunication'}\n",
    "    ]\n",
    "    \n",
    "    for principle in principles:\n",
    "        ax.text(principle['x'], principle['y'], principle['text'],\n",
    "               ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow',\n",
    "                        edgecolor='orange', linewidth=2, alpha=0.9))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Human-AI Interaction Safety Framework', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_human_ai_safety()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with ISO 26262 and SOTIF\n",
    "\n",
    "### Three-Layer Safety Approach\n",
    "\n",
    "| Layer | Standard | Focus | Key Activities |\n",
    "|-------|----------|-------|----------------|\n",
    "| **Layer 1** | ISO 26262 | Hardware/Software Faults | HARA, ASIL, FIT rates |\n",
    "| **Layer 2** | ISO 21448 (SOTIF) | Performance Limitations | Scenario coverage, ODD |\n",
    "| **Layer 3** | AI Safety (ISO/IEC 8800 series) | AI-Specific Risks | Data quality, monitoring, explainability |\n",
    "\n",
    "### Integrated Safety Case\n",
    "\n",
    "1. **Hazard Analysis** (26262 + SOTIF + AI)\n",
    "   - Random HW faults → ISO 26262\n",
    "   - Known performance limits → SOTIF\n",
    "   - Data/model risks → AI safety\n",
    "\n",
    "2. **Requirements**\n",
    "   - ASIL-based requirements (26262)\n",
    "   - Performance requirements (SOTIF)\n",
    "   - Data quality requirements (AI)\n",
    "\n",
    "3. **Design**\n",
    "   - Fault tolerance (26262)\n",
    "   - ODD restrictions (SOTIF)\n",
    "   - Robust architecture (AI)\n",
    "\n",
    "4. **Verification & Validation**\n",
    "   - Fault injection (26262)\n",
    "   - Scenario testing (SOTIF)\n",
    "   - Data/model validation (AI)\n",
    "\n",
    "5. **Monitoring**\n",
    "   - Diagnostic coverage (26262)\n",
    "   - Field monitoring (SOTIF)\n",
    "   - Runtime monitoring (AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated safety checklist\n",
    "def create_integrated_safety_checklist():\n",
    "    \"\"\"\n",
    "    Create comprehensive safety checklist integrating all three standards\n",
    "    \"\"\"\n",
    "    checklist = {\n",
    "        'ISO 26262 (Functional Safety)': [\n",
    "            'Item definition completed',\n",
    "            'HARA performed for all hazards',\n",
    "            'ASIL determined for safety goals',\n",
    "            'Functional safety concept defined',\n",
    "            'Technical safety requirements specified',\n",
    "            'Hardware diagnostic coverage achieved',\n",
    "            'Software unit testing completed',\n",
    "            'System integration testing passed',\n",
    "            'Fault injection testing performed',\n",
    "            'Safety case documented'\n",
    "        ],\n",
    "        'ISO 21448 (SOTIF)': [\n",
    "            'ODD clearly defined',\n",
    "            'Triggering conditions identified',\n",
    "            'Known limitations documented',\n",
    "            'Scenario database established',\n",
    "            'S1/S2/S3/S4 categorization complete',\n",
    "            'Validation scenarios tested',\n",
    "            'Field monitoring plan in place',\n",
    "            'Unknown unsafe space minimized',\n",
    "            'Mitigation strategies implemented',\n",
    "            'Continuous validation process defined'\n",
    "        ],\n",
    "        'AI Safety (ISO/IEC 8800)': [\n",
    "            'Training data quality assessed',\n",
    "            'Dataset representativeness verified',\n",
    "            'Bias analysis performed',\n",
    "            'Model validation on edge cases',\n",
    "            'Robustness testing completed',\n",
    "            'Explainability mechanisms implemented',\n",
    "            'Confidence calibration verified',\n",
    "            'Runtime monitoring deployed',\n",
    "            'Drift detection implemented',\n",
    "            'Retraining process defined'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Simulate completion status\n",
    "    np.random.seed(42)\n",
    "    completion = {}\n",
    "    for standard, items in checklist.items():\n",
    "        completion[standard] = {}\n",
    "        for item in items:\n",
    "            # Simulate different completion rates\n",
    "            if 'ISO 26262' in standard:\n",
    "                status = np.random.choice(['Complete', 'In Progress', 'Not Started'], \n",
    "                                         p=[0.7, 0.2, 0.1])\n",
    "            elif 'SOTIF' in standard:\n",
    "                status = np.random.choice(['Complete', 'In Progress', 'Not Started'],\n",
    "                                         p=[0.6, 0.3, 0.1])\n",
    "            else:  # AI Safety\n",
    "                status = np.random.choice(['Complete', 'In Progress', 'Not Started'],\n",
    "                                         p=[0.5, 0.4, 0.1])\n",
    "            completion[standard][item] = status\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "    \n",
    "    status_colors = {\n",
    "        'Complete': '#2ecc71',\n",
    "        'In Progress': '#f39c12',\n",
    "        'Not Started': '#e74c3c'\n",
    "    }\n",
    "    \n",
    "    for idx, (standard, items) in enumerate(checklist.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Count status\n",
    "        status_counts = {'Complete': 0, 'In Progress': 0, 'Not Started': 0}\n",
    "        colors = []\n",
    "        for item in items:\n",
    "            status = completion[standard][item]\n",
    "            status_counts[status] += 1\n",
    "            colors.append(status_colors[status])\n",
    "        \n",
    "        # Bar chart\n",
    "        y_pos = np.arange(len(items))\n",
    "        ax.barh(y_pos, [1] * len(items), color=colors, edgecolor='black', linewidth=1)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels([f\"{i+1}. {item[:40]}...\" if len(item) > 40 else f\"{i+1}. {item}\" \n",
    "                           for i, item in enumerate(items)], fontsize=9)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_title(f'{standard} - Completion: {status_counts[\"Complete\"]}/{len(items)} '\n",
    "                    f'({status_counts[\"Complete\"]/len(items)*100:.0f}%)',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor=color, edgecolor='black', label=status)\n",
    "                          for status, color in status_colors.items()]\n",
    "        ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Integrated Safety Assessment Summary\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_complete = 0\n",
    "    total_items = 0\n",
    "    \n",
    "    for standard, items in checklist.items():\n",
    "        complete = sum(1 for item in items if completion[standard][item] == 'Complete')\n",
    "        in_progress = sum(1 for item in items if completion[standard][item] == 'In Progress')\n",
    "        not_started = sum(1 for item in items if completion[standard][item] == 'Not Started')\n",
    "        \n",
    "        total_complete += complete\n",
    "        total_items += len(items)\n",
    "        \n",
    "        print(f\"\\n{standard}:\")\n",
    "        print(f\"  ✓ Complete: {complete}/{len(items)}\")\n",
    "        print(f\"  ◐ In Progress: {in_progress}/{len(items)}\")\n",
    "        print(f\"  ✗ Not Started: {not_started}/{len(items)}\")\n",
    "    \n",
    "    overall_completion = total_complete / total_items * 100\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Overall Safety Readiness: {overall_completion:.1f}%\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if overall_completion >= 90:\n",
    "        print(\"✓ System ready for safety assessment\")\n",
    "    elif overall_completion >= 75:\n",
    "        print(\"⚠️  Good progress, but more work needed\")\n",
    "    else:\n",
    "        print(\"⚠️  Significant work remaining before safety assessment\")\n",
    "\n",
    "create_integrated_safety_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Key Takeaways\n",
    "\n",
    "### AI Safety in a Nutshell\n",
    "\n",
    "✓ **AI requires special attention**: Non-deterministic, data-dependent, opaque  \n",
    "✓ **Trustworthiness is key**: Robustness, explainability, transparency, fairness  \n",
    "✓ **Data quality is critical**: \"Garbage in, garbage out\" applies to safety  \n",
    "✓ **Runtime monitoring essential**: Continuous validation, drift detection  \n",
    "✓ **Integration is necessary**: AI safety + SOTIF + ISO 26262 = Complete coverage  \n",
    "\n",
    "### Critical Success Factors\n",
    "\n",
    "1. **High-quality training data**: Representative, accurate, complete\n",
    "2. **Robust model design**: Tested on edge cases and adversarial inputs\n",
    "3. **Explainability**: Understand what the model is doing\n",
    "4. **Continuous monitoring**: Detect drift and degradation\n",
    "5. **Safe retraining**: Regression testing, gradual rollout\n",
    "6. **Human-AI interaction**: Clear communication, safe handover\n",
    "\n",
    "### AI Safety Checklist\n",
    "\n",
    "- ✓ Training data quality assessed\n",
    "- ✓ Bias analysis performed\n",
    "- ✓ Model validated on diverse scenarios\n",
    "- ✓ Robustness testing completed\n",
    "- ✓ Explainability implemented\n",
    "- ✓ Confidence calibration verified\n",
    "- ✓ Runtime monitoring deployed\n",
    "- ✓ Drift detection active\n",
    "- ✓ Retraining process defined\n",
    "- ✓ Integration with 26262/SOTIF\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 14**: ISO/SAE 21434 - Cybersecurity for automotive systems\n",
    "- **Exercise 5**: Perform complete HARA for your perception system\n",
    "- **Exercise 6**: Conduct TARA for V2X communication\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- ISO/IEC TR 5469 - Artificial Intelligence — Functional safety and AI systems\n",
    "- ISO/IEC 24028 - AI — Overview of trustworthiness in AI\n",
    "- ISO/IEC 23894 - AI — Guidance on risk management\n",
    "- NIST AI Risk Management Framework\n",
    "- \"Trustworthy AI\" - European Commission Guidelines\n",
    "- \"AI Safety for Autonomous Vehicles\" - NHTSA/SAE\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook 13**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
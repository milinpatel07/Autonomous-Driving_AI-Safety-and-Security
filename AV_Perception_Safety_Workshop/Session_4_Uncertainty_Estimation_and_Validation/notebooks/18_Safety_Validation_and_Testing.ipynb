{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 18: Safety Validation and Testing for AV Perception\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/AV_Perception_Safety_Workshop/Session_4_Uncertainty_Estimation_and_Validation/notebooks/18_Safety_Validation_and_Testing.ipynb)\n",
    "\n",
    "**Session 4: Uncertainty Estimation and Validation**  \n",
    "**Duration:** 25 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand validation challenges for ML-based AV perception\n",
    "- Learn scenario-based testing (Pegasus 6-layer model)\n",
    "- Explore simulation-based validation approaches\n",
    "- Understand X-in-the-Loop testing (SIL, HIL, VIL)\n",
    "- Learn statistical validation requirements\n",
    "- Design a complete validation strategy\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**The Validation Challenge:**\n",
    "\n",
    "Traditional software: Verify against requirements, test all code paths.\n",
    "\n",
    "**ML-based perception:**\n",
    "- Infinite possible inputs (weather, lighting, objects, ...)\n",
    "- No explicit rules to verify\n",
    "- Probabilistic outputs\n",
    "- Corner cases are safety-critical\n",
    "\n",
    "**Kalra & Paddock (2016): \"Driving to Safety\"**\n",
    "- To prove 20% better than human drivers (95% confidence)\n",
    "- Need to drive **275 million miles** without failure\n",
    "- At 25 mph, 24/7: Would take **500 years** for one vehicle!\n",
    "\n",
    "**Solution:** Combination of approaches\n",
    "1. Scenario-based testing\n",
    "2. Simulation\n",
    "3. Accelerated testing\n",
    "4. Statistical methods\n",
    "5. Field operational tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q matplotlib seaborn numpy scipy pandas plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scenario-Based Testing\n",
    "\n",
    "### 1.1 The Pegasus 6-Layer Model\n",
    "\n",
    "**Pegasus Project:** German research project for safety validation of AVs\n",
    "\n",
    "**6 Layers of Scenario Abstraction:**\n",
    "\n",
    "1. **Layer 1 - Concrete Scenario:** Fully specified, single instance\n",
    "   - \"On Street X, at 3pm on June 1st, sunny weather, pedestrian crosses from left\"\n",
    "   - Can be replayed exactly\n",
    "   - Used for debugging specific failures\n",
    "\n",
    "2. **Layer 2 - Concrete Scenario with Ranges:** Parameters with ranges\n",
    "   - \"Pedestrian crosses from left, speed 1.0-1.5 m/s, distance 10-15m\"\n",
    "   - Enables parameter sweeps\n",
    "\n",
    "3. **Layer 3 - Functional Scenario:** Described in functional terms\n",
    "   - \"Pedestrian crossing scenario\"\n",
    "   - Road type: 2-lane urban\n",
    "   - Actor: pedestrian\n",
    "   - Maneuver: crossing perpendicular to road\n",
    "\n",
    "4. **Layer 4 - Logical Scenario:** All possible parameter combinations\n",
    "   - All variations of pedestrian crossing\n",
    "   - Different speeds, angles, occlusions, lighting, weather, ...\n",
    "   - Defines the **search space** for testing\n",
    "\n",
    "5. **Layer 5 - Scenario Category:** General categories\n",
    "   - \"Vulnerable road user interaction\"\n",
    "   - Groups related scenarios\n",
    "\n",
    "6. **Layer 6 - Operational Design Domain (ODD):** Full operating envelope\n",
    "   - \"Urban environment, <50 km/h, daylight, dry roads\"\n",
    "   - Defines where system is designed to operate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConcreteScenario:\n",
    "    \"\"\"Layer 1: Concrete scenario with specific parameters.\"\"\"\n",
    "    name: str\n",
    "    road_type: str\n",
    "    weather: str\n",
    "    lighting: str\n",
    "    pedestrian_speed: float  # m/s\n",
    "    pedestrian_distance: float  # m\n",
    "    ego_speed: float  # km/h\n",
    "    occlusion: bool\n",
    "    \n",
    "    def describe(self):\n",
    "        return (f\"Scenario: {self.name}\\n\"\n",
    "               f\"  Road: {self.road_type}\\n\"\n",
    "               f\"  Weather: {self.weather}, Lighting: {self.lighting}\\n\"\n",
    "               f\"  Pedestrian: {self.pedestrian_speed} m/s at {self.pedestrian_distance}m\\n\"\n",
    "               f\"  Ego speed: {self.ego_speed} km/h\\n\"\n",
    "               f\"  Occlusion: {'Yes' if self.occlusion else 'No'}\")\n",
    "\n",
    "@dataclass\n",
    "class FunctionalScenario:\n",
    "    \"\"\"Layer 3: Functional scenario description.\"\"\"\n",
    "    name: str\n",
    "    scenario_type: str\n",
    "    road_type: List[str]\n",
    "    weather_conditions: List[str]\n",
    "    lighting_conditions: List[str]\n",
    "    pedestrian_speed_range: Tuple[float, float]\n",
    "    pedestrian_distance_range: Tuple[float, float]\n",
    "    ego_speed_range: Tuple[float, float]\n",
    "    \n",
    "    def generate_concrete_scenarios(self, n_samples=10):\n",
    "        \"\"\"Generate concrete scenarios from functional description.\"\"\"\n",
    "        scenarios = []\n",
    "        for i in range(n_samples):\n",
    "            scenario = ConcreteScenario(\n",
    "                name=f\"{self.name}_{i+1}\",\n",
    "                road_type=np.random.choice(self.road_type),\n",
    "                weather=np.random.choice(self.weather_conditions),\n",
    "                lighting=np.random.choice(self.lighting_conditions),\n",
    "                pedestrian_speed=np.random.uniform(*self.pedestrian_speed_range),\n",
    "                pedestrian_distance=np.random.uniform(*self.pedestrian_distance_range),\n",
    "                ego_speed=np.random.uniform(*self.ego_speed_range),\n",
    "                occlusion=np.random.choice([True, False])\n",
    "            )\n",
    "            scenarios.append(scenario)\n",
    "        return scenarios\n",
    "\n",
    "# Example functional scenario\n",
    "pedestrian_crossing = FunctionalScenario(\n",
    "    name=\"Pedestrian_Crossing\",\n",
    "    scenario_type=\"VRU_Interaction\",\n",
    "    road_type=[\"urban_2lane\", \"residential\"],\n",
    "    weather_conditions=[\"clear\", \"rain\", \"fog\"],\n",
    "    lighting_conditions=[\"day\", \"night\", \"dusk\"],\n",
    "    pedestrian_speed_range=(0.5, 2.0),  # m/s\n",
    "    pedestrian_distance_range=(5.0, 30.0),  # m\n",
    "    ego_speed_range=(20.0, 50.0)  # km/h\n",
    ")\n",
    "\n",
    "# Generate concrete scenarios\n",
    "concrete_scenarios = pedestrian_crossing.generate_concrete_scenarios(n_samples=5)\n",
    "\n",
    "print(\"Generated Concrete Scenarios:\\n\" + \"=\"*60)\n",
    "for scenario in concrete_scenarios:\n",
    "    print(scenario.describe())\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scenario space\n",
    "def visualize_scenario_space():\n",
    "    \"\"\"Visualize the scenario parameter space.\"\"\"\n",
    "    \n",
    "    # Generate larger sample\n",
    "    scenarios = pedestrian_crossing.generate_concrete_scenarios(n_samples=200)\n",
    "    \n",
    "    # Extract parameters\n",
    "    ped_speeds = [s.pedestrian_speed for s in scenarios]\n",
    "    ped_distances = [s.pedestrian_distance for s in scenarios]\n",
    "    ego_speeds = [s.ego_speed for s in scenarios]\n",
    "    \n",
    "    # Create difficulty score (higher = more challenging)\n",
    "    difficulty = []\n",
    "    for s in scenarios:\n",
    "        # Difficulty increases with:\n",
    "        # - Higher ego speed\n",
    "        # - Shorter pedestrian distance\n",
    "        # - Higher pedestrian speed\n",
    "        # - Occlusion, bad weather/lighting\n",
    "        score = 0\n",
    "        score += (s.ego_speed / 50) * 30  # 0-30 points\n",
    "        score += (1 - s.pedestrian_distance / 30) * 25  # 0-25 points\n",
    "        score += (s.pedestrian_speed / 2) * 15  # 0-15 points\n",
    "        if s.occlusion:\n",
    "            score += 15\n",
    "        if s.weather != 'clear':\n",
    "            score += 10\n",
    "        if s.lighting != 'day':\n",
    "            score += 5\n",
    "        difficulty.append(score)\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # 3D scatter plot\n",
    "    ax1 = fig.add_subplot(221, projection='3d')\n",
    "    scatter = ax1.scatter(ped_speeds, ped_distances, ego_speeds, \n",
    "                         c=difficulty, cmap='RdYlGn_r', s=50, alpha=0.6)\n",
    "    ax1.set_xlabel('Pedestrian Speed (m/s)', fontsize=10)\n",
    "    ax1.set_ylabel('Pedestrian Distance (m)', fontsize=10)\n",
    "    ax1.set_zlabel('Ego Speed (km/h)', fontsize=10)\n",
    "    ax1.set_title('Scenario Parameter Space\\n(Color = Difficulty)', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=ax1, label='Difficulty Score')\n",
    "    \n",
    "    # 2D heatmap: ego speed vs pedestrian distance\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    H, xedges, yedges = np.histogram2d(ego_speeds, ped_distances, bins=20)\n",
    "    im = ax2.imshow(H.T, origin='lower', cmap='Blues', aspect='auto',\n",
    "                   extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n",
    "    ax2.set_xlabel('Ego Speed (km/h)', fontsize=11)\n",
    "    ax2.set_ylabel('Pedestrian Distance (m)', fontsize=11)\n",
    "    ax2.set_title('Coverage: Ego Speed vs Distance', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax2, label='Number of Scenarios')\n",
    "    \n",
    "    # Difficulty distribution\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax3.hist(difficulty, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(np.percentile(difficulty, 90), color='red', linestyle='--', \n",
    "               linewidth=2, label='90th percentile (critical scenarios)')\n",
    "    ax3.set_xlabel('Difficulty Score', fontsize=11)\n",
    "    ax3.set_ylabel('Number of Scenarios', fontsize=11)\n",
    "    ax3.set_title('Scenario Difficulty Distribution', fontsize=12, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Category breakdown\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    weather_counts = pd.Series([s.weather for s in scenarios]).value_counts()\n",
    "    lighting_counts = pd.Series([s.lighting for s in scenarios]).value_counts()\n",
    "    \n",
    "    x = np.arange(len(weather_counts))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Only plot if we have lighting data to match\n",
    "    ax4_2 = ax4.twinx()\n",
    "    \n",
    "    bars1 = ax4.bar(x - width/2, weather_counts.values, width, \n",
    "                   label='Weather', alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax4.set_ylabel('Weather Conditions Count', fontsize=11)\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(weather_counts.index, rotation=45)\n",
    "    ax4.set_title('Scenario Conditions Coverage', fontsize=12, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Lighting on secondary axis\n",
    "    x2 = np.arange(len(lighting_counts))\n",
    "    bars2 = ax4_2.bar(x2 + width/2, lighting_counts.values, width,\n",
    "                     label='Lighting', alpha=0.7, color='orange', edgecolor='black')\n",
    "    ax4_2.set_ylabel('Lighting Conditions Count', fontsize=11)\n",
    "    \n",
    "    # Combined legend\n",
    "    lines1, labels1 = ax4.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax4_2.get_legend_handles_labels()\n",
    "    ax4.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO SPACE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTotal scenarios generated: {len(scenarios)}\")\n",
    "    print(f\"\\nDifficulty Score Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(difficulty):.2f}\")\n",
    "    print(f\"  Std:  {np.std(difficulty):.2f}\")\n",
    "    print(f\"  Min:  {np.min(difficulty):.2f}\")\n",
    "    print(f\"  Max:  {np.max(difficulty):.2f}\")\n",
    "    print(f\"  90th percentile: {np.percentile(difficulty, 90):.2f}\")\n",
    "    print(f\"\\nCritical scenarios (>90th percentile): {sum(d > np.percentile(difficulty, 90) for d in difficulty)}\")\n",
    "    \n",
    "    print(\"\\nüéØ Validation Strategy:\")\n",
    "    print(\"  1. Test ALL critical scenarios (top 10%)\")\n",
    "    print(\"  2. Sample moderate scenarios (middle 80%)\")\n",
    "    print(\"  3. Include some easy scenarios (bottom 10%) for baseline\")\n",
    "    print(\"  4. Ensure coverage across all weather/lighting conditions\")\n",
    "\n",
    "visualize_scenario_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulation-Based Validation\n",
    "\n",
    "### 2.1 Why Simulation?\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Safe: No risk to people or vehicles\n",
    "- ‚úÖ Repeatable: Exact scenario replay\n",
    "- ‚úÖ Scalable: Run 24/7, parallel instances\n",
    "- ‚úÖ Controllable: Precisely set parameters\n",
    "- ‚úÖ Accelerated: Test rare scenarios\n",
    "- ‚úÖ Measurable: Full ground truth\n",
    "\n",
    "**Challenges:**\n",
    "- ‚ùå Sim-to-real gap: Sensors, physics, actor behavior\n",
    "- ‚ùå Validation of simulator: How do we know sim is correct?\n",
    "- ‚ùå Corner case modeling: Hard to simulate unknown scenarios\n",
    "\n",
    "### 2.2 Popular AV Simulators\n",
    "\n",
    "**CARLA (Open-source):**\n",
    "- Based on Unreal Engine\n",
    "- Physics simulation\n",
    "- Sensor suite (cameras, LiDAR, radar)\n",
    "- Python API\n",
    "- Used for: Perception, planning, control testing\n",
    "\n",
    "**LGSVL (Open-source, now Apollo):**\n",
    "- Unity-based\n",
    "- High-fidelity sensor simulation\n",
    "- ROS integration\n",
    "- Cloud deployment\n",
    "\n",
    "**MetaDrive (Research):**\n",
    "- Lightweight, fast\n",
    "- Procedural generation\n",
    "- RL-focused\n",
    "\n",
    "**Commercial:** IPG CarMaker, dSPACE, ANSYS, rFpro, etc.\n",
    "\n",
    "### 2.3 Coverage Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze test coverage for scenario-based testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.coverage_bins = {}\n",
    "        self.total_bins = 0\n",
    "        \n",
    "    def define_coverage_space(self, parameter_bins: Dict[str, int]):\n",
    "        \"\"\"\n",
    "        Define the parameter space for coverage analysis.\n",
    "        \n",
    "        Args:\n",
    "            parameter_bins: Dict of parameter name -> number of bins\n",
    "        \"\"\"\n",
    "        self.parameter_bins = parameter_bins\n",
    "        \n",
    "        # Calculate total number of bins (combinations)\n",
    "        self.total_bins = 1\n",
    "        for n_bins in parameter_bins.values():\n",
    "            self.total_bins *= n_bins\n",
    "        \n",
    "        print(f\"Coverage space defined: {self.total_bins:,} total bins\")\n",
    "        print(f\"Parameters: {list(parameter_bins.keys())}\")\n",
    "        \n",
    "    def compute_coverage(self, tested_scenarios: List[ConcreteScenario]):\n",
    "        \"\"\"\n",
    "        Compute coverage percentage from tested scenarios.\n",
    "        \"\"\"\n",
    "        # Simplified: count unique combinations of discretized parameters\n",
    "        covered_bins = set()\n",
    "        \n",
    "        for scenario in tested_scenarios:\n",
    "            # Create bin identifier (simplified)\n",
    "            bin_id = (\n",
    "                scenario.weather,\n",
    "                scenario.lighting,\n",
    "                int(scenario.pedestrian_speed * 2),  # Bin by 0.5 m/s\n",
    "                int(scenario.pedestrian_distance / 5),  # Bin by 5m\n",
    "                int(scenario.ego_speed / 10),  # Bin by 10 km/h\n",
    "                scenario.occlusion\n",
    "            )\n",
    "            covered_bins.add(bin_id)\n",
    "        \n",
    "        coverage_pct = len(covered_bins) / self.total_bins * 100\n",
    "        \n",
    "        return coverage_pct, len(covered_bins)\n",
    "    \n",
    "    def visualize_coverage_growth(self, max_scenarios=1000, step=50):\n",
    "        \"\"\"\n",
    "        Visualize how coverage grows with number of test scenarios.\n",
    "        \"\"\"\n",
    "        n_scenarios_list = list(range(step, max_scenarios + 1, step))\n",
    "        coverage_list = []\n",
    "        \n",
    "        for n in n_scenarios_list:\n",
    "            scenarios = pedestrian_crossing.generate_concrete_scenarios(n)\n",
    "            coverage, _ = self.compute_coverage(scenarios)\n",
    "            coverage_list.append(coverage)\n",
    "        \n",
    "        # Plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Coverage growth\n",
    "        ax1.plot(n_scenarios_list, coverage_list, 'b-', linewidth=2, marker='o')\n",
    "        ax1.axhline(y=80, color='green', linestyle='--', linewidth=2, label='Target: 80%')\n",
    "        ax1.axhline(y=95, color='orange', linestyle='--', linewidth=2, label='Goal: 95%')\n",
    "        ax1.set_xlabel('Number of Test Scenarios', fontsize=12)\n",
    "        ax1.set_ylabel('Coverage (%)', fontsize=12)\n",
    "        ax1.set_title('Test Coverage Growth', fontsize=14, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Diminishing returns\n",
    "        marginal_coverage = [0] + [coverage_list[i] - coverage_list[i-1] \n",
    "                                   for i in range(1, len(coverage_list))]\n",
    "        ax2.bar(n_scenarios_list, marginal_coverage, width=step*0.8, \n",
    "               color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        ax2.set_xlabel('Number of Test Scenarios', fontsize=12)\n",
    "        ax2.set_ylabel('Marginal Coverage Gain (%)', fontsize=12)\n",
    "        ax2.set_title('Diminishing Returns in Coverage', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find scenarios needed for thresholds\n",
    "        for threshold in [80, 90, 95]:\n",
    "            for i, cov in enumerate(coverage_list):\n",
    "                if cov >= threshold:\n",
    "                    print(f\"To reach {threshold}% coverage: ~{n_scenarios_list[i]} scenarios needed\")\n",
    "                    break\n",
    "\n",
    "# Define coverage space\n",
    "analyzer = CoverageAnalyzer(\"Pedestrian Crossing\")\n",
    "analyzer.define_coverage_space({\n",
    "    'weather': 3,  # clear, rain, fog\n",
    "    'lighting': 3,  # day, night, dusk\n",
    "    'pedestrian_speed': 4,  # binned\n",
    "    'pedestrian_distance': 6,  # binned\n",
    "    'ego_speed': 4,  # binned\n",
    "    'occlusion': 2  # yes/no\n",
    "})\n",
    "\n",
    "print(\"\\n\")\n",
    "analyzer.visualize_coverage_growth(max_scenarios=1000, step=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. X-in-the-Loop Testing\n",
    "\n",
    "**Progressive validation strategy from simulation to real world:**\n",
    "\n",
    "### 3.1 SIL (Software-in-the-Loop)\n",
    "\n",
    "**What:** Software runs entirely in simulation\n",
    "\n",
    "**Setup:**\n",
    "- Simulated sensors ‚Üí Perception ‚Üí Planning ‚Üí Control ‚Üí Simulated vehicle\n",
    "- All components are software\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Fastest iteration\n",
    "- ‚úÖ Cheapest\n",
    "- ‚úÖ Unlimited scenarios\n",
    "\n",
    "**Use for:**\n",
    "- Algorithm development\n",
    "- Functional testing\n",
    "- Coverage analysis\n",
    "\n",
    "### 3.2 HIL (Hardware-in-the-Loop)\n",
    "\n",
    "**What:** Real hardware (ECUs, sensors) connected to simulation\n",
    "\n",
    "**Setup:**\n",
    "- Real sensors (camera, LiDAR) ‚Üí Real compute ‚Üí Simulated vehicle dynamics\n",
    "- Or: Sensor simulation ‚Üí Real compute hardware ‚Üí Simulated actuators\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Tests real hardware interfaces\n",
    "- ‚úÖ Tests real-time performance\n",
    "- ‚úÖ Detects hardware-specific issues\n",
    "\n",
    "**Use for:**\n",
    "- Integration testing\n",
    "- Timing validation\n",
    "- ECU testing\n",
    "\n",
    "### 3.3 VIL (Vehicle-in-the-Loop)\n",
    "\n",
    "**What:** Real vehicle in controlled environment (proving ground)\n",
    "\n",
    "**Setup:**\n",
    "- Real vehicle, real sensors, real actuators\n",
    "- Controlled test track\n",
    "- Soft targets (inflatable pedestrians, etc.)\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Real vehicle dynamics\n",
    "- ‚úÖ Real sensor data\n",
    "- ‚úÖ Safety-critical scenarios possible\n",
    "\n",
    "**Use for:**\n",
    "- Final validation before public roads\n",
    "- Rare/dangerous scenarios\n",
    "- Regulatory testing\n",
    "\n",
    "### 3.4 FOT (Field Operational Tests)\n",
    "\n",
    "**What:** Real-world driving on public roads\n",
    "\n",
    "**Requirements:**\n",
    "- Safety driver\n",
    "- Extensive monitoring\n",
    "- Regulatory approval\n",
    "\n",
    "**Use for:**\n",
    "- Final validation\n",
    "- Long-tail scenario discovery\n",
    "- Performance in real conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_testing_pyramid():\n",
    "    \"\"\"\n",
    "    Visualize the testing strategy pyramid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define pyramid levels\n",
    "    levels = [\n",
    "        {'name': 'FOT\\n(Field Tests)', 'scenarios': 100, 'cost': 10000, 'fidelity': 100},\n",
    "        {'name': 'VIL\\n(Proving Ground)', 'scenarios': 1000, 'cost': 1000, 'fidelity': 90},\n",
    "        {'name': 'HIL\\n(Hardware-in-Loop)', 'scenarios': 10000, 'cost': 100, 'fidelity': 70},\n",
    "        {'name': 'SIL\\n(Simulation)', 'scenarios': 1000000, 'cost': 1, 'fidelity': 50},\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "    \n",
    "    # Pyramid 1: Number of scenarios\n",
    "    ax = axes[0]\n",
    "    y_pos = np.arange(len(levels))\n",
    "    widths = [l['scenarios'] for l in levels]\n",
    "    colors = ['red', 'orange', 'yellow', 'lightgreen']\n",
    "    \n",
    "    for i, (level, width, color) in enumerate(zip(levels, widths, colors)):\n",
    "        ax.barh(i, width, color=color, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        # Add text\n",
    "        ax.text(width/2, i, f\"{level['name']}\\n{width:,} scenarios\", \n",
    "               ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('Number of Test Scenarios (log scale)', fontsize=11)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_title('Testing Pyramid: Volume', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Pyramid 2: Cost per scenario\n",
    "    ax = axes[1]\n",
    "    costs = [l['cost'] for l in levels]\n",
    "    \n",
    "    for i, (level, cost, color) in enumerate(zip(levels, costs, colors)):\n",
    "        ax.barh(i, cost, color=color, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.text(cost/2, i, f\"${cost:,}/scenario\",\n",
    "               ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('Relative Cost per Scenario (log scale)', fontsize=11)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_title('Testing Pyramid: Cost', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Pyramid 3: Fidelity\n",
    "    ax = axes[2]\n",
    "    fidelities = [l['fidelity'] for l in levels]\n",
    "    \n",
    "    for i, (level, fid, color) in enumerate(zip(levels, fidelities, colors)):\n",
    "        ax.barh(i, fid, color=color, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.text(fid/2, i, f\"{fid}% fidelity\",\n",
    "               ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('Fidelity to Real World (%)', fontsize=11)\n",
    "    ax.set_title('Testing Pyramid: Fidelity', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.set_xlim([0, 100])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING STRATEGY PYRAMID\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Level':<25} {'Scenarios':<15} {'Cost/Scenario':<15} {'Fidelity'}\")\n",
    "    print(\"-\"*70)\n",
    "    for level in levels:\n",
    "        print(f\"{level['name'].replace(chr(10), ' '):<25} {level['scenarios']:<15,} \"\n",
    "              f\"${level['cost']:<14,} {level['fidelity']}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\nüéØ Validation Strategy:\")\n",
    "    print(\"\\n1. SIL (Simulation):\")\n",
    "    print(\"   ‚Ä¢ Run MILLIONS of scenarios\")\n",
    "    print(\"   ‚Ä¢ Cover entire parameter space\")\n",
    "    print(\"   ‚Ä¢ Find algorithmic issues\")\n",
    "    print(\"\\n2. HIL (Hardware-in-Loop):\")\n",
    "    print(\"   ‚Ä¢ Run THOUSANDS of scenarios\")\n",
    "    print(\"   ‚Ä¢ Focus on critical/corner cases\")\n",
    "    print(\"   ‚Ä¢ Validate real-time performance\")\n",
    "    print(\"\\n3. VIL (Vehicle-in-Loop):\")\n",
    "    print(\"   ‚Ä¢ Run HUNDREDS of scenarios\")\n",
    "    print(\"   ‚Ä¢ Test dangerous scenarios safely\")\n",
    "    print(\"   ‚Ä¢ Validate complete system\")\n",
    "    print(\"\\n4. FOT (Field Tests):\")\n",
    "    print(\"   ‚Ä¢ Run ~100 critical scenarios\")\n",
    "    print(\"   ‚Ä¢ Plus general driving for long-tail discovery\")\n",
    "    print(\"   ‚Ä¢ Final confidence before deployment\")\n",
    "    print(\"\\nüí° Total testing: 1M+ SIL + 10K HIL + 1K VIL + 100 FOT scenarios\")\n",
    "\n",
    "visualize_testing_pyramid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Validation Requirements\n",
    "\n",
    "### 4.1 The Kalra & Paddock Analysis\n",
    "\n",
    "**Question:** How many miles to prove AV is safer than humans?\n",
    "\n",
    "**Assumptions:**\n",
    "- Human fatality rate: ~1 per 100 million miles\n",
    "- Want to prove AV is 20% better with 95% confidence\n",
    "\n",
    "**Answer:** **275 million failure-free miles**\n",
    "\n",
    "**At 25 mph, 24/7:** Would take ~500 years for one vehicle!\n",
    "\n",
    "### 4.2 Confidence Intervals for Rare Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistical_evidence(target_failure_rate, confidence_level=0.95, \n",
    "                                 improvement_factor=1.2):\n",
    "    \"\"\"\n",
    "    Compute miles needed to demonstrate safety with statistical confidence.\n",
    "    \n",
    "    Uses Poisson distribution for rare events.\n",
    "    \n",
    "    Args:\n",
    "        target_failure_rate: Failures per mile (e.g., 1/100M for fatalities)\n",
    "        confidence_level: Statistical confidence (e.g., 0.95)\n",
    "        improvement_factor: How much better than target (e.g., 1.2 = 20% better)\n",
    "    \"\"\"\n",
    "    # Target rate for AV (better than baseline)\n",
    "    av_target_rate = target_failure_rate / improvement_factor\n",
    "    \n",
    "    # For zero failures observed, upper confidence bound:\n",
    "    # Œª_upper = -ln(1 - confidence) / n_miles\n",
    "    # We want: Œª_upper <= av_target_rate\n",
    "    # Therefore: n_miles >= -ln(1 - confidence) / av_target_rate\n",
    "    \n",
    "    miles_needed = -np.log(1 - confidence_level) / av_target_rate\n",
    "    \n",
    "    return miles_needed\n",
    "\n",
    "def visualize_statistical_requirements():\n",
    "    \"\"\"\n",
    "    Visualize miles needed for different safety metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Different safety metrics\n",
    "    metrics = [\n",
    "        {'name': 'Fatality', 'baseline_rate': 1/100e6, 'critical': True},\n",
    "        {'name': 'Injury accident', 'baseline_rate': 1/1e6, 'critical': True},\n",
    "        {'name': 'Any accident', 'baseline_rate': 1/100e3, 'critical': False},\n",
    "        {'name': 'Critical intervention', 'baseline_rate': 1/10e3, 'critical': False},\n",
    "    ]\n",
    "    \n",
    "    confidence_levels = [0.90, 0.95, 0.99]\n",
    "    improvement_factors = [1.2, 1.5, 2.0]  # 20%, 50%, 100% better\n",
    "    \n",
    "    # Compute for base case: 95% confidence, 20% improvement\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot 1: Miles needed for different metrics\n",
    "    ax = axes[0]\n",
    "    miles_needed = []\n",
    "    colors_list = ['darkred', 'red', 'orange', 'yellow']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        miles = compute_statistical_evidence(metric['baseline_rate'], \n",
    "                                            confidence_level=0.95,\n",
    "                                            improvement_factor=1.2)\n",
    "        miles_needed.append(miles / 1e6)  # Convert to millions\n",
    "    \n",
    "    bars = ax.barh(range(len(metrics)), miles_needed, \n",
    "                   color=colors_list, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax.set_yticks(range(len(metrics)))\n",
    "    ax.set_yticklabels([m['name'] for m in metrics])\n",
    "    ax.set_xlabel('Miles Needed (Millions)', fontsize=11)\n",
    "    ax.set_title('Statistical Evidence Required\\n(95% confidence, 20% improvement)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add values\n",
    "    for i, (bar, miles) in enumerate(zip(bars, miles_needed)):\n",
    "        ax.text(miles, i, f'  {miles:.0f}M miles', \n",
    "               va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Effect of confidence level\n",
    "    ax = axes[1]\n",
    "    metric = metrics[0]  # Use fatality as example\n",
    "    \n",
    "    miles_by_conf = []\n",
    "    for conf in confidence_levels:\n",
    "        miles = compute_statistical_evidence(metric['baseline_rate'],\n",
    "                                            confidence_level=conf,\n",
    "                                            improvement_factor=1.2)\n",
    "        miles_by_conf.append(miles / 1e6)\n",
    "    \n",
    "    bars = ax.bar([f\"{int(c*100)}%\" for c in confidence_levels], miles_by_conf,\n",
    "                  color='steelblue', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax.set_ylabel('Miles Needed (Millions)', fontsize=11)\n",
    "    ax.set_xlabel('Confidence Level', fontsize=11)\n",
    "    ax.set_title('Effect of Confidence Level\\n(Fatality metric, 20% improvement)',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, miles in zip(bars, miles_by_conf):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{miles:.0f}M',\n",
    "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Effect of improvement factor\n",
    "    ax = axes[2]\n",
    "    miles_by_improv = []\n",
    "    for improv in improvement_factors:\n",
    "        miles = compute_statistical_evidence(metric['baseline_rate'],\n",
    "                                            confidence_level=0.95,\n",
    "                                            improvement_factor=improv)\n",
    "        miles_by_improv.append(miles / 1e6)\n",
    "    \n",
    "    bars = ax.bar([f\"{int((f-1)*100)}%\" for f in improvement_factors], \n",
    "                  miles_by_improv,\n",
    "                  color='green', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax.set_ylabel('Miles Needed (Millions)', fontsize=11)\n",
    "    ax.set_xlabel('Improvement over Baseline', fontsize=11)\n",
    "    ax.set_title('Effect of Target Improvement\\n(Fatality metric, 95% confidence)',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, miles in zip(bars, miles_by_improv):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{miles:.0f}M',\n",
    "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Time required\n",
    "    ax = axes[3]\n",
    "    \n",
    "    # Compute time for different fleet sizes\n",
    "    target_miles = miles_needed[0] * 1e6  # Fatality metric\n",
    "    fleet_sizes = [1, 10, 100, 1000]\n",
    "    avg_speed_mph = 25\n",
    "    hours_per_day = 24\n",
    "    \n",
    "    years_needed = []\n",
    "    for fleet_size in fleet_sizes:\n",
    "        miles_per_day = fleet_size * avg_speed_mph * hours_per_day\n",
    "        days_needed = target_miles / miles_per_day\n",
    "        years = days_needed / 365\n",
    "        years_needed.append(years)\n",
    "    \n",
    "    bars = ax.bar([f\"{fs} vehicle{'s' if fs > 1 else ''}\" for fs in fleet_sizes],\n",
    "                  years_needed,\n",
    "                  color='purple', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax.set_ylabel('Years Required', fontsize=11)\n",
    "    ax.set_xlabel('Fleet Size', fontsize=11)\n",
    "    ax.set_title(f'Time to Collect {miles_needed[0]:.0f}M Miles\\n(25 mph average, 24/7 operation)',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, years in zip(bars, years_needed):\n",
    "        height = bar.get_height()\n",
    "        if years >= 1:\n",
    "            label = f'{years:.0f} years'\n",
    "        else:\n",
    "            label = f'{years*12:.0f} months'\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               label,\n",
    "               ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICAL VALIDATION REQUIREMENTS (Kalra & Paddock Analysis)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Metric':<25} {'Baseline Rate':<20} {'Miles Needed (95% conf, 20% better)'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for metric, miles in zip(metrics, miles_needed):\n",
    "        rate_str = f\"1 per {int(1/metric['baseline_rate']):,} miles\"\n",
    "        print(f\"{metric['name']:<25} {rate_str:<20} {miles:.1f} million miles\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nüéØ Key Insights:\")\n",
    "    print(\"\\n1. Pure statistical proof for fatalities is IMPRACTICAL\")\n",
    "    print(f\"   ‚Ä¢ Need {miles_needed[0]:.0f}M miles = {years_needed[0]:.0f} years for 1 vehicle\")\n",
    "    print(f\"   ‚Ä¢ Even with 100 vehicles: {years_needed[2]:.1f} years\")\n",
    "    print(\"\\n2. Must use SURROGATE metrics:\")\n",
    "    print(\"   ‚Ä¢ Critical interventions (easier to measure)\")\n",
    "    print(\"   ‚Ä¢ Near-miss events\")\n",
    "    print(\"   ‚Ä¢ Disengagements\")\n",
    "    print(\"\\n3. Complementary evidence needed:\")\n",
    "    print(\"   ‚Ä¢ Simulation validation (millions of scenarios)\")\n",
    "    print(\"   ‚Ä¢ Proving ground tests\")\n",
    "    print(\"   ‚Ä¢ Safety argument based on design\")\n",
    "    print(\"   ‚Ä¢ Continuous monitoring in operation\")\n",
    "\n",
    "visualize_statistical_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation Metrics for AV Perception\n",
    "\n",
    "### 5.1 Standard ML Metrics\n",
    "\n",
    "**For object detection:**\n",
    "- Precision, Recall, F1-score\n",
    "- mAP (mean Average Precision)\n",
    "- IoU (Intersection over Union)\n",
    "\n",
    "**For classification:**\n",
    "- Accuracy\n",
    "- Confusion matrix\n",
    "- ROC curve, AUC\n",
    "\n",
    "### 5.2 Safety-Specific Metrics\n",
    "\n",
    "**For AV perception, we prioritize:**\n",
    "\n",
    "1. **Recall (Sensitivity):** Don't miss critical objects!\n",
    "   - False negatives = potentially fatal\n",
    "   - Especially for pedestrians, cyclists\n",
    "\n",
    "2. **False Negative Rate at critical distances:**\n",
    "   - Must detect pedestrians at >30m in urban scenarios\n",
    "   - Must detect vehicles at >100m on highway\n",
    "\n",
    "3. **Worst-case performance:**\n",
    "   - 99th percentile latency\n",
    "   - Performance in adverse conditions\n",
    "\n",
    "4. **Uncertainty quality:**\n",
    "   - Calibration (ECE)\n",
    "   - Uncertainty on OOD inputs\n",
    "\n",
    "5. **Temporal consistency:**\n",
    "   - Track stability\n",
    "   - No flickering detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_metric_comparison():\n",
    "    \"\"\"\n",
    "    Compare standard ML metrics vs safety-oriented metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulated detector performance\n",
    "    scenarios = [\n",
    "        {\n",
    "            'name': 'Model A: High Accuracy',\n",
    "            'TP': 950, 'FP': 50, 'FN': 50, 'TN': 950,\n",
    "            'description': 'Balanced'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Model B: High Precision',\n",
    "            'TP': 900, 'FP': 10, 'FN': 100, 'TN': 990,\n",
    "            'description': 'Conservative (many false negatives!)'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Model C: High Recall',\n",
    "            'TP': 980, 'FP': 200, 'FN': 20, 'TN': 800,\n",
    "            'description': 'Aggressive (few false negatives)'\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Compute metrics\n",
    "    for scenario in scenarios:\n",
    "        TP, FP, FN, TN = scenario['TP'], scenario['FP'], scenario['FN'], scenario['TN']\n",
    "        \n",
    "        scenario['accuracy'] = (TP + TN) / (TP + FP + FN + TN)\n",
    "        scenario['precision'] = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        scenario['recall'] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        scenario['f1'] = 2 * (scenario['precision'] * scenario['recall']) / \\\n",
    "                        (scenario['precision'] + scenario['recall']) if \\\n",
    "                        (scenario['precision'] + scenario['recall']) > 0 else 0\n",
    "        scenario['fnr'] = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        scenario['fpr'] = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Standard metrics\n",
    "    ax = axes[0, 0]\n",
    "    models = [s['name'].split(':')[0] for s in scenarios]\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.2\n",
    "    \n",
    "    ax.bar(x - width, [s['accuracy'] for s in scenarios], width, \n",
    "          label='Accuracy', alpha=0.8, edgecolor='black')\n",
    "    ax.bar(x, [s['precision'] for s in scenarios], width,\n",
    "          label='Precision', alpha=0.8, edgecolor='black')\n",
    "    ax.bar(x + width, [s['recall'] for s in scenarios], width,\n",
    "          label='Recall', alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    ax.set_ylabel('Score', fontsize=11)\n",
    "    ax.set_title('Standard ML Metrics', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # Plot 2: Safety-critical metrics\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    fnrs = [s['fnr'] for s in scenarios]\n",
    "    fprs = [s['fpr'] for s in scenarios]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, fnrs, width, label='FNR (‚ö†Ô∏è Safety Critical!)',\n",
    "                  color='red', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    bars2 = ax.bar(x + width/2, fprs, width, label='FPR (Comfort)',\n",
    "                  color='orange', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax.set_ylabel('Error Rate', fontsize=11)\n",
    "    ax.set_title('Safety-Critical Metrics (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add target line\n",
    "    ax.axhline(y=0.05, color='green', linestyle='--', linewidth=2, label='Target: <5%')\n",
    "    \n",
    "    # Plot 3: Confusion matrices\n",
    "    for i, scenario in enumerate(scenarios[:2]):\n",
    "        ax = axes[1, i]\n",
    "        conf_matrix = np.array([[scenario['TN'], scenario['FP']],\n",
    "                               [scenario['FN'], scenario['TP']]])\n",
    "        \n",
    "        im = ax.imshow(conf_matrix, cmap='RdYlGn', alpha=0.7, vmin=0, vmax=1000)\n",
    "        \n",
    "        for r in range(2):\n",
    "            for c in range(2):\n",
    "                text = ax.text(c, r, f'{conf_matrix[r, c]}',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\",\n",
    "                             fontsize=16, fontweight='bold')\n",
    "        \n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels(['Pred Neg', 'Pred Pos'])\n",
    "        ax.set_yticklabels(['True Neg', 'True Pos'])\n",
    "        ax.set_title(f\"{scenario['name']}\\nFNR={scenario['fnr']:.1%}\",\n",
    "                    fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Highlight FN in red\n",
    "        if scenario['FN'] > 50:\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor('red')\n",
    "                spine.set_linewidth(4)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAFETY METRIC ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Model':<25} {'Acc':<6} {'Prec':<6} {'Rec':<6} {'F1':<6} {'FNR':<6} {'Safety'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        name = scenario['name'].split(':')[0]\n",
    "        safety = '‚úÖ GOOD' if scenario['fnr'] < 0.05 else '‚ùå BAD'\n",
    "        print(f\"{name:<25} {scenario['accuracy']:<6.3f} {scenario['precision']:<6.3f} \"\n",
    "              f\"{scenario['recall']:<6.3f} {scenario['f1']:<6.3f} \"\n",
    "              f\"{scenario['fnr']:<6.3f} {safety}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nüéØ Key Insights for AV Perception:\")\n",
    "    print(\"\\n1. High ACCURACY ‚â† Safe for AVs!\")\n",
    "    print(\"   ‚Ä¢ Model B has 95% accuracy but 10% FNR (misses 1 in 10 pedestrians!)\")\n",
    "    print(\"\\n2. RECALL is critical for safety:\")\n",
    "    print(\"   ‚Ä¢ False negatives = missed pedestrians = potential fatalities\")\n",
    "    print(\"   ‚Ä¢ Model C: 98% recall, only 2% FNR ‚úÖ\")\n",
    "    print(\"\\n3. Trade-off: Recall vs Precision\")\n",
    "    print(\"   ‚Ä¢ High recall ‚Üí More false alarms (uncomfortable braking)\")\n",
    "    print(\"   ‚Ä¢ But this is MUCH better than missing pedestrians!\")\n",
    "    print(\"\\n4. Target metrics for pedestrian detection:\")\n",
    "    print(\"   ‚Ä¢ Recall: >98% (FNR <2%)\")\n",
    "    print(\"   ‚Ä¢ Precision: >90% (acceptable false alarm rate)\")\n",
    "    print(\"   ‚Ä¢ F1-score: >0.93\")\n",
    "    print(\"\\nüí° For safety: Optimize for RECALL first, then improve precision\")\n",
    "\n",
    "safety_metric_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Validation Plan Template\n",
    "\n",
    "Let's create a comprehensive validation plan for an AV perception system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_plan():\n",
    "    \"\"\"\n",
    "    Create a comprehensive validation plan template.\n",
    "    \"\"\"\n",
    "    \n",
    "    plan = {\n",
    "        'System': 'AV Perception System (Pedestrian Detection)',\n",
    "        'ODD': 'Urban environment, <50 km/h, daylight and dusk, dry and light rain',\n",
    "        \n",
    "        'Phases': [\n",
    "            {\n",
    "                'phase': '1. SIL (Simulation)',\n",
    "                'scenarios': 1000000,\n",
    "                'duration': '3 months',\n",
    "                'objectives': [\n",
    "                    'Cover full parameter space',\n",
    "                    'Test all weather/lighting combinations',\n",
    "                    'Verify functional requirements',\n",
    "                    'Measure baseline performance'\n",
    "                ],\n",
    "                'acceptance': [\n",
    "                    'Recall >98% across all scenarios',\n",
    "                    'Precision >85%',\n",
    "                    'ECE <0.05',\n",
    "                    'Coverage >95% of parameter space'\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                'phase': '2. HIL (Hardware-in-Loop)',\n",
    "                'scenarios': 10000,\n",
    "                'duration': '2 months',\n",
    "                'objectives': [\n",
    "                    'Validate on real hardware',\n",
    "                    'Test real-time performance',\n",
    "                    'Focus on critical scenarios',\n",
    "                    'Verify sensor fusion'\n",
    "                ],\n",
    "                'acceptance': [\n",
    "                    'Recall >97% on critical scenarios',\n",
    "                    'Latency <100ms (95th percentile)',\n",
    "                    'No regression from SIL',\n",
    "                    'Hardware meets specifications'\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                'phase': '3. VIL (Proving Ground)',\n",
    "                'scenarios': 1000,\n",
    "                'duration': '2 months',\n",
    "                'objectives': [\n",
    "                    'Test complete system integration',\n",
    "                    'Validate dangerous scenarios safely',\n",
    "                    'Test edge cases',\n",
    "                    'Verify fallback behaviors'\n",
    "                ],\n",
    "                'acceptance': [\n",
    "                    'Zero missed detections in critical scenarios',\n",
    "                    'Successful emergency braking tests',\n",
    "                    'Correct uncertainty estimates',\n",
    "                    'All edge cases handled'\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                'phase': '4. FOT (Field Tests)',\n",
    "                'scenarios': '100 critical + 10,000 miles general',\n",
    "                'duration': '6 months',\n",
    "                'objectives': [\n",
    "                    'Validate in real-world conditions',\n",
    "                    'Discover unknown scenarios',\n",
    "                    'Build statistical evidence',\n",
    "                    'Final confidence before deployment'\n",
    "                ],\n",
    "                'acceptance': [\n",
    "                    'Zero safety-critical failures',\n",
    "                    'Interventions <1 per 1000 miles',\n",
    "                    'All unknowns properly handled',\n",
    "                    'Regulatory approval achieved'\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        'Metrics': {\n",
    "            'Primary': [\n",
    "                'Recall (>98%)',\n",
    "                'False Negative Rate (<2%)',\n",
    "                'Detection range (>30m urban)'\n",
    "            ],\n",
    "            'Secondary': [\n",
    "                'Precision (>90%)',\n",
    "                'F1-score (>0.93)',\n",
    "                'Latency (<100ms, p95)'\n",
    "            ],\n",
    "            'Safety': [\n",
    "                'Calibration ECE (<0.05)',\n",
    "                'OOD detection rate (>95%)',\n",
    "                'Worst-case performance'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'Risk_Mitigation': [\n",
    "            'Diverse test scenarios including edge cases',\n",
    "            'Progressive validation (SIL‚ÜíHIL‚ÜíVIL‚ÜíFOT)',\n",
    "            'Uncertainty monitoring in production',\n",
    "            'Continuous data collection and retraining',\n",
    "            'Redundant perception (multiple sensors)',\n",
    "            'Human-in-loop for unknown scenarios'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Print plan\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"VALIDATION PLAN: {plan['System']}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nOperational Design Domain (ODD):\")\n",
    "    print(f\"  {plan['ODD']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"VALIDATION PHASES\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for phase_data in plan['Phases']:\n",
    "        print(f\"\\n{phase_data['phase']}\")\n",
    "        print(f\"  Scenarios: {phase_data['scenarios']:,}\" if isinstance(phase_data['scenarios'], int) \n",
    "              else f\"  Scenarios: {phase_data['scenarios']}\")\n",
    "        print(f\"  Duration: {phase_data['duration']}\")\n",
    "        print(f\"\\n  Objectives:\")\n",
    "        for obj in phase_data['objectives']:\n",
    "            print(f\"    ‚Ä¢ {obj}\")\n",
    "        print(f\"\\n  Acceptance Criteria:\")\n",
    "        for criterion in phase_data['acceptance']:\n",
    "            print(f\"    ‚úì {criterion}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"VALIDATION METRICS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for category, metrics in plan['Metrics'].items():\n",
    "        print(f\"\\n{category} Metrics:\")\n",
    "        for metric in metrics:\n",
    "            print(f\"  ‚Ä¢ {metric}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"RISK MITIGATION STRATEGIES\")\n",
    "    print(\"-\"*80)\n",
    "    for strategy in plan['Risk_Mitigation']:\n",
    "        print(f\"  ‚úì {strategy}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nTotal timeline: ~13 months\")\n",
    "    print(\"Total cost estimate: $2-5M (depending on simulator, proving ground, fleet)\")\n",
    "    print(\"\\nThis plan provides ISO 26262 and ISO 21448 (SOTIF) compliant evidence.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return plan\n",
    "\n",
    "validation_plan = create_validation_plan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Validation Challenges:**\n",
    "   - Cannot test all possible scenarios\n",
    "   - Pure statistical proof requires millions of miles\n",
    "   - Need multi-faceted approach\n",
    "\n",
    "2. **Scenario-Based Testing:**\n",
    "   - Pegasus 6-layer model: concrete ‚Üí functional ‚Üí logical ‚Üí ODD\n",
    "   - Systematically cover parameter space\n",
    "   - Focus on critical/corner cases\n",
    "\n",
    "3. **Simulation + Real-World:**\n",
    "   - SIL: Millions of scenarios, full coverage\n",
    "   - HIL: Thousands of scenarios, real hardware\n",
    "   - VIL: Hundreds of scenarios, complete system\n",
    "   - FOT: Targeted scenarios + general driving\n",
    "\n",
    "4. **Statistical Evidence:**\n",
    "   - Need ~275M miles to prove better than humans (impractical!)\n",
    "   - Use surrogate metrics and simulation\n",
    "   - Continuous monitoring in operation\n",
    "\n",
    "5. **Safety Metrics:**\n",
    "   - Prioritize RECALL over precision\n",
    "   - False negatives are safety-critical\n",
    "   - Measure worst-case performance\n",
    "   - Validate uncertainty estimates\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Progressive validation:** Simulation ‚Üí Proving ground ‚Üí Public roads\n",
    "2. **Diverse scenarios:** Cover full ODD systematically\n",
    "3. **Safety-first metrics:** Optimize for recall, not just accuracy\n",
    "4. **Uncertainty monitoring:** Track model confidence in production\n",
    "5. **Continuous learning:** Collect edge cases, retrain, re-validate\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed **Session 4: Uncertainty Estimation and Validation**!\n",
    "\n",
    "You now understand:\n",
    "- ‚úÖ Uncertainty types and quantification methods\n",
    "- ‚úÖ MC Dropout and Deep Ensembles\n",
    "- ‚úÖ Model calibration techniques\n",
    "- ‚úÖ Comprehensive validation strategies\n",
    "\n",
    "**This completes the AV Perception Safety Workshop!**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete Exercise 7: Uncertainty Quantification\n",
    "- Complete Exercise 8: Design a Validation Strategy\n",
    "- Apply these techniques to your own AV projects\n",
    "- Stay updated: This field is rapidly evolving!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 17: Calibration and Reliability\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/AV_Perception_Safety_Workshop/Session_4_Uncertainty_Estimation_and_Validation/notebooks/17_Calibration_and_Reliability.ipynb)\n",
    "\n",
    "**Session 4: Uncertainty Estimation and Validation**  \n",
    "**Duration:** 20 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand model calibration and why it matters\n",
    "- Learn to measure calibration using reliability diagrams and ECE\n",
    "- Implement calibration methods (Temperature Scaling, Platt Scaling)\n",
    "- Apply calibration to AV perception tasks\n",
    "- Connect calibration to safety decision-making\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**The Calibration Problem:**\n",
    "\n",
    "A model predicts \"pedestrian\" with 90% confidence. What does this mean?\n",
    "- **Calibrated model:** 90% confidence â†’ 90% chance it's actually a pedestrian\n",
    "- **Miscalibrated model:** 90% confidence â†’ Could be 50% or 99% actual accuracy!\n",
    "\n",
    "**Modern neural networks are often overconfident** (Guo et al., 2017)\n",
    "- They achieve high accuracy but poor calibration\n",
    "- This is dangerous for AVs: overconfidence â†’ risky decisions\n",
    "\n",
    "**Why calibration matters for AVs:**\n",
    "- Decision thresholds require calibrated probabilities\n",
    "- Safety requires knowing when the model is uncertain\n",
    "- ISO 26262/21448 require understanding system limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q torch torchvision matplotlib seaborn numpy scipy scikit-learn netcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Calibration\n",
    "\n",
    "### 1.1 Definition\n",
    "\n",
    "**Perfect Calibration:** For all predictions with confidence p, exactly p fraction are correct.\n",
    "\n",
    "Mathematically:\n",
    "```\n",
    "P(Y = Å· | P(Å·) = p) = p\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Model makes 100 predictions with 80% confidence\n",
    "- If calibrated: exactly 80 of them should be correct\n",
    "- If overconfident: fewer than 80 are correct\n",
    "- If underconfident: more than 80 are correct\n",
    "\n",
    "### 1.2 Reliability Diagram\n",
    "\n",
    "The primary tool for visualizing calibration.\n",
    "\n",
    "**How it works:**\n",
    "1. Bin predictions by confidence (e.g., [0-0.1, 0.1-0.2, ..., 0.9-1.0])\n",
    "2. For each bin, compute average confidence and actual accuracy\n",
    "3. Plot: x-axis = confidence, y-axis = accuracy\n",
    "4. Perfect calibration = diagonal line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_diagram(y_true, y_prob, n_bins=10, title=\"Reliability Diagram\"):\n",
    "    \"\"\"\n",
    "    Plot reliability diagram.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (binary)\n",
    "        y_prob: Predicted probabilities for positive class\n",
    "        n_bins: Number of bins\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Bin edges\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Assign predictions to bins\n",
    "    bin_indices = np.digitize(y_prob, bin_edges[1:-1])\n",
    "    \n",
    "    # Compute accuracy and average confidence per bin\n",
    "    bin_accuracies = []\n",
    "    bin_confidences = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if mask.sum() > 0:\n",
    "            bin_accuracy = y_true[mask].mean()\n",
    "            bin_confidence = y_prob[mask].mean()\n",
    "            bin_count = mask.sum()\n",
    "        else:\n",
    "            bin_accuracy = 0\n",
    "            bin_confidence = bin_centers[i]\n",
    "            bin_count = 0\n",
    "        \n",
    "        bin_accuracies.append(bin_accuracy)\n",
    "        bin_confidences.append(bin_confidence)\n",
    "        bin_counts.append(bin_count)\n",
    "    \n",
    "    bin_accuracies = np.array(bin_accuracies)\n",
    "    bin_confidences = np.array(bin_confidences)\n",
    "    bin_counts = np.array(bin_counts)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Reliability diagram\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect calibration')\n",
    "    \n",
    "    # Bar chart with gap showing calibration error\n",
    "    gap = bin_confidences - bin_accuracies\n",
    "    colors = ['red' if g > 0 else 'green' for g in gap]\n",
    "    \n",
    "    # Plot bars\n",
    "    for i, (conf, acc, count) in enumerate(zip(bin_confidences, bin_accuracies, bin_counts)):\n",
    "        if count > 0:\n",
    "            # Bar showing actual accuracy\n",
    "            ax1.bar(conf, acc, width=0.08, alpha=0.6, color='blue', \n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "            # Gap showing miscalibration\n",
    "            if gap[i] > 0:  # Overconfident\n",
    "                ax1.bar(conf, gap[i], bottom=acc, width=0.08, alpha=0.4, \n",
    "                       color='red', edgecolor='black', linewidth=1.5)\n",
    "            else:  # Underconfident\n",
    "                ax1.bar(conf, -gap[i], bottom=conf, width=0.08, alpha=0.4,\n",
    "                       color='green', edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Points showing bin centers\n",
    "    valid_mask = bin_counts > 0\n",
    "    ax1.scatter(bin_confidences[valid_mask], bin_accuracies[valid_mask], \n",
    "               s=bin_counts[valid_mask]*2, c='blue', alpha=0.8, \n",
    "               edgecolors='black', linewidth=2, zorder=5, label='Actual accuracy')\n",
    "    \n",
    "    ax1.set_xlabel('Confidence (Predicted Probability)', fontsize=12)\n",
    "    ax1.set_ylabel('Accuracy (Fraction Correct)', fontsize=12)\n",
    "    ax1.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Add text annotations\n",
    "    ax1.text(0.7, 0.2, 'Overconfident\\n(Red gap)', fontsize=10, \n",
    "            bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "    ax1.text(0.2, 0.7, 'Underconfident\\n(Green gap)', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
    "    \n",
    "    # Histogram of predictions\n",
    "    ax2.bar(bin_centers, bin_counts, width=0.08, alpha=0.7, \n",
    "           color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "    ax2.set_xlabel('Confidence', fontsize=12)\n",
    "    ax2.set_ylabel('Number of Predictions', fontsize=12)\n",
    "    ax2.set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_xlim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return bin_accuracies, bin_confidences, bin_counts\n",
    "\n",
    "print(\"Reliability diagram function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Expected Calibration Error (ECE)\n",
    "\n",
    "**ECE:** Single scalar metric for calibration quality.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "ECE = Î£ (|Bâ‚˜| / N) |acc(Bâ‚˜) - conf(Bâ‚˜)|\n",
    "```\n",
    "\n",
    "Where:\n",
    "- Bâ‚˜ = predictions in bin m\n",
    "- |Bâ‚˜| = number of predictions in bin m\n",
    "- N = total number of predictions\n",
    "- acc(Bâ‚˜) = accuracy in bin m\n",
    "- conf(Bâ‚˜) = average confidence in bin m\n",
    "\n",
    "**Interpretation:**\n",
    "- ECE = 0: Perfect calibration\n",
    "- ECE = 0.1: On average, 10% calibration error\n",
    "- Lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"\n",
    "    Compute Expected Calibration Error (ECE).\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (binary)\n",
    "        y_prob: Predicted probabilities for positive class\n",
    "        n_bins: Number of bins\n",
    "        \n",
    "    Returns:\n",
    "        ece: Expected Calibration Error\n",
    "    \"\"\"\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_prob, bin_edges[1:-1])\n",
    "    \n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if mask.sum() > 0:\n",
    "            bin_accuracy = y_true[mask].mean()\n",
    "            bin_confidence = y_prob[mask].mean()\n",
    "            bin_size = mask.sum()\n",
    "            \n",
    "            ece += (bin_size / len(y_true)) * abs(bin_accuracy - bin_confidence)\n",
    "    \n",
    "    return ece\n",
    "\n",
    "print(\"ECE function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Miscalibrated Model\n",
    "\n",
    "Let's train a model and observe its calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = make_classification(n_samples=2000, n_features=20, n_informative=15,\n",
    "                          n_redundant=5, random_state=42)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network\n",
    "class BinaryClassifier(nn.Module):\n",
    "    \"\"\"Simple binary classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=20, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model = BinaryClassifier(input_dim=20, hidden_dim=100).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Plot training\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "model.eval()\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    probs_uncalibrated = torch.sigmoid(torch.FloatTensor(logits)).numpy()\n",
    "\n",
    "# Compute accuracy\n",
    "predictions = (probs_uncalibrated > 0.5).astype(int)\n",
    "accuracy = (predictions == y_test).mean()\n",
    "\n",
    "# Compute ECE\n",
    "ece_before = expected_calibration_error(y_test, probs_uncalibrated)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ECE (before calibration): {ece_before:.4f}\")\n",
    "\n",
    "# Plot reliability diagram\n",
    "print(\"\\nâš ï¸ BEFORE CALIBRATION:\")\n",
    "_ = reliability_diagram(y_test, probs_uncalibrated, n_bins=10, \n",
    "                       title=\"Reliability Diagram - UNCALIBRATED Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibration Methods\n",
    "\n",
    "### 4.1 Temperature Scaling\n",
    "\n",
    "**Idea:** Scale the logits by a temperature parameter T.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Original:  p = softmax(z)\n",
    "Calibrated: p = softmax(z / T)\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- T > 1: Softens probabilities (less confident)\n",
    "- T < 1: Sharpens probabilities (more confident)\n",
    "- T = 1: No change\n",
    "- Optimized on validation set to minimize NLL or ECE\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… Single parameter (simple!)\n",
    "- âœ… Preserves accuracy (predictions don't change)\n",
    "- âœ… Fast and effective\n",
    "- âœ… State-of-the-art for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureScaling:\n",
    "    \"\"\"\n",
    "    Temperature scaling for calibration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.temperature = 1.0\n",
    "    \n",
    "    def fit(self, logits, labels, max_iter=50):\n",
    "        \"\"\"\n",
    "        Learn optimal temperature on validation set.\n",
    "        \n",
    "        Args:\n",
    "            logits: Model logits (before sigmoid)\n",
    "            labels: True labels\n",
    "        \"\"\"\n",
    "        logits = torch.FloatTensor(logits).unsqueeze(1)\n",
    "        labels = torch.FloatTensor(labels).unsqueeze(1)\n",
    "        \n",
    "        # Optimize temperature to minimize negative log likelihood\n",
    "        def nll_criterion(T):\n",
    "            scaled_logits = logits / T\n",
    "            loss = F.binary_cross_entropy_with_logits(scaled_logits, labels)\n",
    "            return loss.item()\n",
    "        \n",
    "        # Find optimal temperature\n",
    "        result = minimize(nll_criterion, x0=1.0, method='L-BFGS-B', \n",
    "                         bounds=[(0.1, 10.0)], options={'maxiter': max_iter})\n",
    "        \n",
    "        self.temperature = result.x[0]\n",
    "        print(f\"Optimal temperature: {self.temperature:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, logits):\n",
    "        \"\"\"\n",
    "        Get calibrated probabilities.\n",
    "        \n",
    "        Args:\n",
    "            logits: Model logits\n",
    "            \n",
    "        Returns:\n",
    "            Calibrated probabilities\n",
    "        \"\"\"\n",
    "        logits = torch.FloatTensor(logits)\n",
    "        scaled_logits = logits / self.temperature\n",
    "        probs = torch.sigmoid(scaled_logits).numpy()\n",
    "        return probs\n",
    "\n",
    "print(\"Temperature Scaling class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply temperature scaling\n",
    "# Get validation logits\n",
    "with torch.no_grad():\n",
    "    val_logits = model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Fit temperature scaling on validation set\n",
    "temp_scaler = TemperatureScaling()\n",
    "temp_scaler.fit(val_logits, y_val)\n",
    "\n",
    "# Get calibrated probabilities on test set\n",
    "probs_temp_scaled = temp_scaler.predict_proba(logits)\n",
    "\n",
    "# Compute ECE\n",
    "ece_temp = expected_calibration_error(y_test, probs_temp_scaled)\n",
    "\n",
    "print(f\"\\nECE after temperature scaling: {ece_temp:.4f}\")\n",
    "print(f\"Improvement: {(ece_before - ece_temp) / ece_before * 100:.1f}%\")\n",
    "\n",
    "# Plot reliability diagram\n",
    "print(\"\\nâœ… AFTER TEMPERATURE SCALING:\")\n",
    "_ = reliability_diagram(y_test, probs_temp_scaled, n_bins=10,\n",
    "                       title=\"Reliability Diagram - TEMPERATURE SCALED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Platt Scaling\n",
    "\n",
    "**Idea:** Fit a logistic regression on top of model outputs.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "p_calibrated = sigmoid(A * z + B)\n",
    "```\n",
    "\n",
    "Where A and B are learned from validation data.\n",
    "\n",
    "**Properties:**\n",
    "- More flexible than temperature scaling (2 parameters)\n",
    "- Can handle both over- and under-confidence\n",
    "- Originally designed for SVMs, works for neural networks too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlattScaling:\n",
    "    \"\"\"\n",
    "    Platt scaling for calibration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lr = LogisticRegression()\n",
    "    \n",
    "    def fit(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Fit Platt scaling on validation set.\n",
    "        \n",
    "        Args:\n",
    "            logits: Model logits\n",
    "            labels: True labels\n",
    "        \"\"\"\n",
    "        self.lr.fit(logits.reshape(-1, 1), labels)\n",
    "        print(f\"Platt scaling parameters: A={self.lr.coef_[0][0]:.4f}, B={self.lr.intercept_[0]:.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, logits):\n",
    "        \"\"\"\n",
    "        Get calibrated probabilities.\n",
    "        \"\"\"\n",
    "        probs = self.lr.predict_proba(logits.reshape(-1, 1))[:, 1]\n",
    "        return probs\n",
    "\n",
    "# Apply Platt scaling\n",
    "platt_scaler = PlattScaling()\n",
    "platt_scaler.fit(val_logits, y_val)\n",
    "\n",
    "# Get calibrated probabilities\n",
    "probs_platt = platt_scaler.predict_proba(logits)\n",
    "\n",
    "# Compute ECE\n",
    "ece_platt = expected_calibration_error(y_test, probs_platt)\n",
    "\n",
    "print(f\"\\nECE after Platt scaling: {ece_platt:.4f}\")\n",
    "print(f\"Improvement: {(ece_before - ece_platt) / ece_before * 100:.1f}%\")\n",
    "\n",
    "# Plot\n",
    "print(\"\\nâœ… AFTER PLATT SCALING:\")\n",
    "_ = reliability_diagram(y_test, probs_platt, n_bins=10,\n",
    "                       title=\"Reliability Diagram - PLATT SCALED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Isotonic Regression\n",
    "\n",
    "**Idea:** Learn a non-parametric monotonic mapping.\n",
    "\n",
    "**Properties:**\n",
    "- Most flexible (non-parametric)\n",
    "- Can fit complex calibration curves\n",
    "- Risk of overfitting with small validation sets\n",
    "- Requires more validation data than temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsotonicCalibration:\n",
    "    \"\"\"\n",
    "    Isotonic regression for calibration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "    \n",
    "    def fit(self, probs, labels):\n",
    "        \"\"\"\n",
    "        Fit isotonic regression.\n",
    "        \n",
    "        Args:\n",
    "            probs: Predicted probabilities (after sigmoid)\n",
    "            labels: True labels\n",
    "        \"\"\"\n",
    "        self.iso_reg.fit(probs, labels)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, probs):\n",
    "        \"\"\"\n",
    "        Get calibrated probabilities.\n",
    "        \"\"\"\n",
    "        return self.iso_reg.predict(probs)\n",
    "\n",
    "# Apply isotonic regression\n",
    "# Need probabilities (not logits) for isotonic regression\n",
    "val_probs_uncalib = torch.sigmoid(torch.FloatTensor(val_logits)).numpy()\n",
    "\n",
    "iso_scaler = IsotonicCalibration()\n",
    "iso_scaler.fit(val_probs_uncalib, y_val)\n",
    "\n",
    "# Get calibrated probabilities\n",
    "probs_isotonic = iso_scaler.predict_proba(probs_uncalibrated)\n",
    "\n",
    "# Compute ECE\n",
    "ece_iso = expected_calibration_error(y_test, probs_isotonic)\n",
    "\n",
    "print(f\"\\nECE after isotonic regression: {ece_iso:.4f}\")\n",
    "print(f\"Improvement: {(ece_before - ece_iso) / ece_before * 100:.1f}%\")\n",
    "\n",
    "# Plot\n",
    "print(\"\\nâœ… AFTER ISOTONIC REGRESSION:\")\n",
    "_ = reliability_diagram(y_test, probs_isotonic, n_bins=10,\n",
    "                       title=\"Reliability Diagram - ISOTONIC REGRESSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of Calibration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_calibration_methods():\n",
    "    \"\"\"Compare all calibration methods.\"\"\"\n",
    "    \n",
    "    methods = {\n",
    "        'Uncalibrated': probs_uncalibrated,\n",
    "        'Temperature Scaling': probs_temp_scaled,\n",
    "        'Platt Scaling': probs_platt,\n",
    "        'Isotonic Regression': probs_isotonic\n",
    "    }\n",
    "    \n",
    "    # Compute ECE for all\n",
    "    eces = {name: expected_calibration_error(y_test, probs) \n",
    "            for name, probs in methods.items()}\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, (name, probs) in zip(axes, methods.items()):\n",
    "        # Reliability curve\n",
    "        bin_edges = np.linspace(0, 1, 11)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        bin_indices = np.digitize(probs, bin_edges[1:-1])\n",
    "        \n",
    "        bin_accs = []\n",
    "        bin_confs = []\n",
    "        bin_counts = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            mask = bin_indices == i\n",
    "            if mask.sum() > 0:\n",
    "                bin_accs.append(y_test[mask].mean())\n",
    "                bin_confs.append(probs[mask].mean())\n",
    "                bin_counts.append(mask.sum())\n",
    "        \n",
    "        # Plot perfect calibration\n",
    "        ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect')\n",
    "        \n",
    "        # Plot actual calibration\n",
    "        if len(bin_confs) > 0:\n",
    "            ax.plot(bin_confs, bin_accs, 'o-', linewidth=3, markersize=10,\n",
    "                   label=f'Actual (ECE={eces[name]:.3f})')\n",
    "        \n",
    "        # Color based on ECE\n",
    "        if eces[name] < 0.05:\n",
    "            color = 'lightgreen'\n",
    "        elif eces[name] < 0.10:\n",
    "            color = 'yellow'\n",
    "        else:\n",
    "            color = 'lightcoral'\n",
    "        ax.set_facecolor(color)\n",
    "        \n",
    "        ax.set_xlabel('Confidence', fontsize=11)\n",
    "        ax.set_ylabel('Accuracy', fontsize=11)\n",
    "        ax.set_title(f'{name}\\nECE = {eces[name]:.4f}', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ECE comparison\n",
    "    names = list(eces.keys())\n",
    "    values = list(eces.values())\n",
    "    colors_bar = ['red', 'lightgreen', 'lightgreen', 'green']\n",
    "    \n",
    "    bars = ax1.bar(names, values, color=colors_bar, alpha=0.7, \n",
    "                   edgecolor='black', linewidth=2)\n",
    "    ax1.set_ylabel('Expected Calibration Error', fontsize=12)\n",
    "    ax1.set_title('Calibration Method Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Improvement over baseline\n",
    "    baseline = eces['Uncalibrated']\n",
    "    improvements = [(baseline - ece) / baseline * 100 \n",
    "                   for ece in list(eces.values())[1:]]\n",
    "    \n",
    "    bars2 = ax2.bar(names[1:], improvements, color=colors_bar[1:], \n",
    "                   alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax2.set_ylabel('Improvement over Uncalibrated (%)', fontsize=12)\n",
    "    ax2.set_title('ECE Reduction', fontsize=14, fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, val in zip(bars2, improvements):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CALIBRATION METHODS COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Method':<25} {'ECE':<10} {'Improvement':<15} {'Parameters'}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    params = ['0', '1', '2', 'Non-parametric']\n",
    "    for (name, ece), param in zip(eces.items(), params):\n",
    "        if name == 'Uncalibrated':\n",
    "            improvement = '-'\n",
    "        else:\n",
    "            improvement = f\"{(baseline - ece) / baseline * 100:.1f}%\"\n",
    "        print(f\"{name:<25} {ece:<10.4f} {improvement:<15} {param}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\nðŸŽ¯ Recommendations:\")\n",
    "    print(\"\\n1. Temperature Scaling:\")\n",
    "    print(\"   âœ… Best choice for most applications\")\n",
    "    print(\"   âœ… Simple, effective, preserves accuracy\")\n",
    "    print(\"   âœ… Works well with small validation sets\")\n",
    "    print(\"\\n2. Platt Scaling:\")\n",
    "    print(\"   âœ… Good alternative, slightly more flexible\")\n",
    "    print(\"   âš ï¸  Two parameters (slightly more risk of overfitting)\")\n",
    "    print(\"\\n3. Isotonic Regression:\")\n",
    "    print(\"   âœ… Most flexible, can handle complex miscalibration\")\n",
    "    print(\"   âš ï¸  Needs larger validation set\")\n",
    "    print(\"   âš ï¸  Risk of overfitting\")\n",
    "    print(\"\\nðŸ’¡ For AV production: Use Temperature Scaling\")\n",
    "    print(\"   Simple, robust, well-understood, industry standard\")\n",
    "\n",
    "compare_calibration_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Why Calibration Matters for AV Safety\n",
    "\n",
    "### 6.1 Decision Thresholds\n",
    "\n",
    "**Problem:** AVs need to make binary decisions based on probabilities.\n",
    "\n",
    "Example: Emergency braking for pedestrian detection\n",
    "- If P(pedestrian) > threshold â†’ BRAKE\n",
    "- If P(pedestrian) â‰¤ threshold â†’ Continue\n",
    "\n",
    "**With miscalibrated model:**\n",
    "- Overconfident model: Threshold seems safe but actually risky\n",
    "- False sense of security\n",
    "\n",
    "**With calibrated model:**\n",
    "- Threshold has clear probabilistic meaning\n",
    "- Can reason about false positive/negative rates\n",
    "\n",
    "### 6.2 Safety Standards\n",
    "\n",
    "**ISO 26262:**\n",
    "- Requires understanding system behavior in all scenarios\n",
    "- Calibrated probabilities help define confidence levels\n",
    "\n",
    "**ISO 21448 (SOTIF):**\n",
    "- Requires knowing when system is uncertain\n",
    "- Calibration ensures uncertainty estimates are meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_decision_threshold_impact():\n",
    "    \"\"\"\n",
    "    Demonstrate how calibration affects decision-making.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decision threshold\n",
    "    threshold = 0.7\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    methods = [\n",
    "        ('Uncalibrated', probs_uncalibrated),\n",
    "        ('Temperature Scaled', probs_temp_scaled),\n",
    "        ('Platt Scaled', probs_platt),\n",
    "        ('Isotonic', probs_isotonic)\n",
    "    ]\n",
    "    \n",
    "    for ax, (name, probs) in zip(axes.flatten(), methods):\n",
    "        # Classify based on threshold\n",
    "        decisions = (probs > threshold).astype(int)\n",
    "        \n",
    "        # Compute metrics\n",
    "        tp = ((decisions == 1) & (y_test == 1)).sum()\n",
    "        fp = ((decisions == 1) & (y_test == 0)).sum()\n",
    "        tn = ((decisions == 0) & (y_test == 0)).sum()\n",
    "        fn = ((decisions == 0) & (y_test == 1)).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        # For safety: False Negative Rate is critical!\n",
    "        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        # Plot confusion matrix style\n",
    "        conf_matrix = np.array([[tn, fp], [fn, tp]])\n",
    "        \n",
    "        im = ax.imshow(conf_matrix, cmap='RdYlGn', alpha=0.7)\n",
    "        \n",
    "        # Add text\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                text = ax.text(j, i, f'{conf_matrix[i, j]}',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\",\n",
    "                             fontsize=20, fontweight='bold')\n",
    "        \n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels(['Pred Neg', 'Pred Pos'])\n",
    "        ax.set_yticklabels(['True Neg', 'True Pos'])\n",
    "        \n",
    "        # Safety color coding\n",
    "        ece = expected_calibration_error(y_test, probs)\n",
    "        if ece < 0.05:\n",
    "            border_color = 'green'\n",
    "        elif ece < 0.10:\n",
    "            border_color = 'orange'\n",
    "        else:\n",
    "            border_color = 'red'\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(border_color)\n",
    "            spine.set_linewidth(4)\n",
    "        \n",
    "        ax.set_title(f\"{name}\\n\"\n",
    "                    f\"ECE={ece:.3f} | FNR={fnr:.3f} (âš ï¸ Safety!)\\n\"\n",
    "                    f\"Precision={precision:.3f} | Recall={recall:.3f}\",\n",
    "                    fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Decision Threshold Analysis (threshold = {threshold})\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Method':<20} {'ECE':<8} {'FNR':<8} {'FPR':<8} {'Precision':<10} {'Recall':<8}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for name, probs in methods:\n",
    "        decisions = (probs > threshold).astype(int)\n",
    "        tp = ((decisions == 1) & (y_test == 1)).sum()\n",
    "        fp = ((decisions == 1) & (y_test == 0)).sum()\n",
    "        tn = ((decisions == 0) & (y_test == 0)).sum()\n",
    "        fn = ((decisions == 0) & (y_test == 1)).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        ece = expected_calibration_error(y_test, probs)\n",
    "        \n",
    "        print(f\"{name:<20} {ece:<8.3f} {fnr:<8.3f} {fpr:<8.3f} {precision:<10.3f} {recall:<8.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nâš ï¸  Key Insight for AV Safety:\")\n",
    "    print(\"   - FNR (False Negative Rate) is CRITICAL for pedestrian detection\")\n",
    "    print(\"   - Missing a pedestrian = potential fatality\")\n",
    "    print(\"   - Calibration helps us set thresholds with known FNR\")\n",
    "    print(\"   - Better calibration â†’ better safety-performance trade-off\")\n",
    "\n",
    "demonstrate_decision_threshold_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Calibration Definition:**\n",
    "   - Confidence should match actual accuracy\n",
    "   - Modern neural networks are often overconfident\n",
    "   - Measured using reliability diagrams and ECE\n",
    "\n",
    "2. **Calibration Methods:**\n",
    "   - **Temperature Scaling:** Simple, effective, industry standard (1 parameter)\n",
    "   - **Platt Scaling:** More flexible (2 parameters)\n",
    "   - **Isotonic Regression:** Most flexible (non-parametric)\n",
    "\n",
    "3. **Why It Matters for AVs:**\n",
    "   - Decision thresholds require calibrated probabilities\n",
    "   - Safety standards (ISO 26262, SOTIF) require understanding uncertainty\n",
    "   - False negatives in pedestrian detection = safety-critical\n",
    "\n",
    "4. **Best Practices:**\n",
    "   - Always check calibration on validation set\n",
    "   - Temperature scaling is recommended for most cases\n",
    "   - Calibration is a post-processing step (doesn't change accuracy)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 18:** Learn validation and testing strategies for AV perception\n",
    "\n",
    "---\n",
    "\n",
    "## Interactive Exercise\n",
    "\n",
    "**Try this:**\n",
    "1. Change the number of bins in reliability diagram (try 5, 15, 20)\n",
    "2. Try different decision thresholds (0.5, 0.8, 0.9)\n",
    "3. Train the model longer - does it become more miscalibrated?\n",
    "\n",
    "**Challenge:**\n",
    "- Implement calibration for a multi-class problem (3+ classes)\n",
    "- How does temperature scaling work with softmax?\n",
    "\n",
    "**Discussion:**\n",
    "- For pedestrian detection, would you prefer:\n",
    "  - High precision (few false alarms)?\n",
    "  - High recall (few missed pedestrians)?\n",
    "- How does calibration help you make this trade-off?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

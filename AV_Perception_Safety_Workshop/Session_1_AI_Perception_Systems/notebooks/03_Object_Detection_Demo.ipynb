{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Object Detection Demo with YOLOv8\n",
    "\n",
    "**Session 1: AI-based Perception Systems in Autonomous Vehicles**\n",
    "\n",
    "**Author:** Milin Patel  \n",
    "**Duration:** ~20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand how deep learning object detection works\n",
    "- ‚úÖ Run YOLOv8 on driving scenes\n",
    "- ‚úÖ Interpret confidence scores and bounding boxes\n",
    "- ‚úÖ Compare different detection models\n",
    "- ‚úÖ Analyze detection performance and failure cases\n",
    "- ‚úÖ Perform real-time inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Understanding Object Detection\n",
    "\n",
    "### What is Object Detection?\n",
    "\n",
    "**Input:** An image (e.g., 1920√ó1080 pixels, 3 color channels)  \n",
    "**Output:** List of detected objects with:\n",
    "- **Bounding box:** [x, y, width, height]\n",
    "- **Class label:** \"car\", \"person\", \"bicycle\", etc.\n",
    "- **Confidence score:** 0.0 to 1.0 (0% to 100%)\n",
    "\n",
    "### Example Output:\n",
    "```python\n",
    "[\n",
    "    {\"class\": \"car\", \"bbox\": [100, 200, 150, 80], \"confidence\": 0.95},\n",
    "    {\"class\": \"person\", \"bbox\": [500, 300, 50, 120], \"confidence\": 0.87},\n",
    "    {\"class\": \"bicycle\", \"bbox\": [800, 250, 80, 100], \"confidence\": 0.72}\n",
    "]\n",
    "```\n",
    "\n",
    "### Popular Architectures:\n",
    "- **YOLO (You Only Look Once):** Fast, real-time (50-150 FPS)\n",
    "- **Faster R-CNN:** High accuracy, slower (5-10 FPS)\n",
    "- **SSD (Single Shot Detector):** Balance of speed and accuracy\n",
    "- **Modern:** EfficientDet, DETR (Transformer-based)\n",
    "\n",
    "**Today's focus:** YOLOv8 - state-of-the-art speed and accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Load YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model\n",
    "print(\"Loading YOLOv8n (nano - fastest) model...\")\n",
    "model = YOLO('yolov8n.pt')  # Automatically downloads if not present\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"  - Input size: 640x640\")\n",
    "print(f\"  - Classes: {len(model.names)} (COCO dataset)\")\n",
    "print(f\"  - Parameters: ~3.2M\")\n",
    "print(f\"\\nSample classes: {list(model.names.values())[:15]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Dataset Classes\n",
    "\n",
    "YOLOv8 is trained on the **COCO (Common Objects in Context)** dataset with **80 object classes**.\n",
    "\n",
    "Relevant for autonomous driving:\n",
    "- **Vehicles:** car, truck, bus, motorcycle, bicycle\n",
    "- **People:** person\n",
    "- **Traffic:** traffic light, stop sign, parking meter\n",
    "- **Animals:** dog, cat, horse, etc. (road hazards!)\n",
    "\n",
    "**Note:** COCO is not driving-specific. For production AVs, models are trained on specialized datasets (nuScenes, Waymo, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Run Detection on Sample Images\n",
    "\n",
    "Let's test on various driving scenarios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to download and display image\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Load image from URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return np.array(img)\n",
    "\n",
    "# Sample driving scene URLs (free, no copyright issues)\n",
    "test_images = {\n",
    "    \"Urban Scene\": \"https://images.unsplash.com/photo-1449824913935-59a10b8d2000?w=800\",\n",
    "    \"Highway Traffic\": \"https://images.unsplash.com/photo-1501594907352-04cda38ebc29?w=800\",\n",
    "    \"Pedestrian Crossing\": \"https://images.unsplash.com/photo-1514565131-fce0801e5785?w=800\",\n",
    "}\n",
    "\n",
    "print(\"üì• Test images ready!\")\n",
    "print(\"   Note: If images fail to load, check internet connection or use local images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run detection and visualize\n",
    "def detect_and_visualize(image, conf_threshold=0.25, title=\"Detection Results\"):\n",
    "    \"\"\"\n",
    "    Run YOLOv8 detection and visualize results\n",
    "    \n",
    "    Args:\n",
    "        image: numpy array (RGB)\n",
    "        conf_threshold: minimum confidence score\n",
    "        title: plot title\n",
    "    \"\"\"\n",
    "    # Run inference\n",
    "    start_time = time.time()\n",
    "    results = model.predict(image, conf=conf_threshold, verbose=False)[0]\n",
    "    inference_time = (time.time() - start_time) * 1000  # ms\n",
    "    \n",
    "    # Get detections\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\n",
    "    confidences = results.boxes.conf.cpu().numpy()\n",
    "    class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    img_with_boxes = image.copy()\n",
    "    for box, conf, cls_id in zip(boxes, confidences, class_ids):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        label = model.names[cls_id]\n",
    "        \n",
    "        # Color based on class\n",
    "        if label in ['person']:\n",
    "            color = (255, 0, 0)  # Red for pedestrians\n",
    "        elif label in ['car', 'truck', 'bus']:\n",
    "            color = (0, 255, 0)  # Green for vehicles\n",
    "        elif label in ['bicycle', 'motorcycle']:\n",
    "            color = (255, 165, 0)  # Orange for bikes\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Blue for others\n",
    "        \n",
    "        # Draw box and label\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), color, 3)\n",
    "        text = f\"{label}: {conf:.2f}\"\n",
    "        cv2.putText(img_with_boxes, text, (x1, y1-10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img_with_boxes)\n",
    "    axes[1].set_title(f\"{title}\\nDetections: {len(boxes)}, Inference: {inference_time:.1f}ms\", \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    print(f\"\\nüìä Detection Summary:\")\n",
    "    print(f\"   - Total objects detected: {len(boxes)}\")\n",
    "    print(f\"   - Inference time: {inference_time:.1f} ms\")\n",
    "    print(f\"   - FPS: {1000/inference_time:.1f}\")\n",
    "    print(f\"\\n   Detected objects:\")\n",
    "    for label, conf in zip([model.names[c] for c in class_ids], confidences):\n",
    "        print(f\"      - {label}: confidence {conf:.3f} ({conf*100:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Detection function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Urban Driving Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and detect urban scene\n",
    "print(\"Testing on: Urban Scene\")\n",
    "try:\n",
    "    img = load_image_from_url(test_images[\"Urban Scene\"])\n",
    "    results = detect_and_visualize(img, conf_threshold=0.4, title=\"Urban Scene Detection\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading image: {e}\")\n",
    "    print(\"   üí° Tip: Use a local image instead or check internet connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Confidence Threshold Analysis\n",
    "\n",
    "The **confidence threshold** controls the trade-off between:\n",
    "- **Precision:** How many detections are correct?\n",
    "- **Recall:** How many actual objects are detected?\n",
    "\n",
    "Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different confidence thresholds\n",
    "def compare_confidence_thresholds(image, thresholds=[0.25, 0.5, 0.75]):\n",
    "    \"\"\"\n",
    "    Compare detection results with different confidence thresholds\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(thresholds), figsize=(18, 5))\n",
    "    \n",
    "    for idx, threshold in enumerate(thresholds):\n",
    "        # Run detection\n",
    "        results = model.predict(image, conf=threshold, verbose=False)[0]\n",
    "        \n",
    "        # Visualize\n",
    "        img_with_boxes = image.copy()\n",
    "        boxes = results.boxes.xyxy.cpu().numpy()\n",
    "        confidences = results.boxes.conf.cpu().numpy()\n",
    "        class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
    "        \n",
    "        for box, conf, cls_id in zip(boxes, confidences, class_ids):\n",
    "            x1, y1, x2, y2 = box.astype(int)\n",
    "            label = model.names[cls_id]\n",
    "            color = (0, 255, 0) if conf > 0.7 else (255, 165, 0) if conf > 0.5 else (255, 0, 0)\n",
    "            cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(img_with_boxes, f\"{label}:{conf:.2f}\", (x1, y1-5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        \n",
    "        axes[idx].imshow(img_with_boxes)\n",
    "        axes[idx].set_title(f\"Threshold: {threshold}\\nDetections: {len(boxes)}\", \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Observations:\")\n",
    "    print(\"   - Lower threshold ‚Üí more detections (higher recall, lower precision)\")\n",
    "    print(\"   - Higher threshold ‚Üí fewer detections (higher precision, lower recall)\")\n",
    "    print(\"   - For safety-critical systems: Need to balance both!\")\n",
    "\n",
    "# Run comparison\n",
    "try:\n",
    "    compare_confidence_thresholds(img)\n",
    "except:\n",
    "    print(\"Use your own image for this comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Real-Time Inference Benchmarking\n",
    "\n",
    "For autonomous driving, **real-time performance** is crucial!\n",
    "\n",
    "Target: **30-60 FPS** (33 ms - 16 ms per frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark inference speed\n",
    "def benchmark_inference(model_name='yolov8n.pt', num_runs=50, img_size=640):\n",
    "    \"\"\"\n",
    "    Benchmark YOLOv8 inference speed\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚è±Ô∏è Benchmarking {model_name} on {device}...\")\n",
    "    \n",
    "    # Load model\n",
    "    test_model = YOLO(model_name)\n",
    "    test_model.to(device)\n",
    "    \n",
    "    # Create dummy image\n",
    "    dummy_img = np.random.randint(0, 255, (img_size, img_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Warm-up runs\n",
    "    for _ in range(10):\n",
    "        _ = test_model.predict(dummy_img, verbose=False)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.time()\n",
    "        _ = test_model.predict(dummy_img, verbose=False)\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()  # Wait for GPU\n",
    "        times.append((time.time() - start) * 1000)  # ms\n",
    "    \n",
    "    # Statistics\n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    fps = 1000 / mean_time\n",
    "    \n",
    "    print(f\"\\nüìä Results ({num_runs} runs):\")\n",
    "    print(f\"   - Mean inference time: {mean_time:.2f} ¬± {std_time:.2f} ms\")\n",
    "    print(f\"   - Throughput: {fps:.1f} FPS\")\n",
    "    print(f\"   - Min time: {min(times):.2f} ms\")\n",
    "    print(f\"   - Max time: {max(times):.2f} ms\")\n",
    "    \n",
    "    if fps >= 30:\n",
    "        print(f\"   ‚úÖ Real-time capable! (>30 FPS)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Below real-time threshold (<30 FPS)\")\n",
    "    \n",
    "    return mean_time, fps\n",
    "\n",
    "# Run benchmark\n",
    "mean_time, fps = benchmark_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Exercise: Analyze Failure Cases\n",
    "\n",
    "**Task:** Upload or use your own driving scene images and identify:\n",
    "1. **False positives:** Detected objects that don't exist\n",
    "2. **False negatives:** Objects that exist but weren't detected\n",
    "3. **Misclassifications:** Wrong label assigned\n",
    "\n",
    "**Think:** Why did these failures occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test your own images\n",
    "# Option 1: Load local image\n",
    "# your_image = cv2.cvtColor(cv2.imread('path/to/your/image.jpg'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Option 2: Load from URL\n",
    "# your_image = load_image_from_url('your_image_url')\n",
    "\n",
    "# Run detection\n",
    "# detect_and_visualize(your_image, conf_threshold=0.3, title=\"Your Test Image\")\n",
    "\n",
    "print(\"üí° Upload challenging images:\")\n",
    "print(\"   - Night scenes\")\n",
    "print(\"   - Rain/fog\")\n",
    "print(\"   - Occlusions\")\n",
    "print(\"   - Unusual objects\")\n",
    "print(\"\\n   Observe where YOLO fails!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### Object Detection for AVs\n",
    "- **Task:** Detect and classify objects in camera images\n",
    "- **Output:** Bounding boxes + class labels + confidence scores\n",
    "- **State-of-the-art:** YOLOv8 achieves real-time performance\n",
    "\n",
    "### Performance Considerations\n",
    "- **Speed:** YOLOv8n runs at 50-150+ FPS (real-time capable!)\n",
    "- **Accuracy:** Trade-off with model size (nano vs small vs medium)\n",
    "- **Confidence:** Threshold controls precision-recall trade-off\n",
    "\n",
    "### Limitations Observed\n",
    "- **Weather:** Performance degrades in rain, fog, snow\n",
    "- **Lighting:** Night scenes are challenging\n",
    "- **Occlusions:** Partially hidden objects missed\n",
    "- **Unusual objects:** Not in training data ‚Üí not detected\n",
    "\n",
    "### Safety Implications\n",
    "- **False negatives:** Missing a pedestrian ‚Üí collision!\n",
    "- **False positives:** Emergency brake for phantom object ‚Üí rear-end collision\n",
    "- **Confidence != certainty:** High confidence can still be wrong\n",
    "\n",
    "**Next session:** We'll analyze real accident cases and failure modes in depth!\n",
    "\n",
    "---\n",
    "\n",
    "## üîú Next Steps\n",
    "\n",
    "1. **Notebook 4:** Explore autonomous driving datasets (KITTI, nuScenes)\n",
    "2. **Notebook 5:** Learn sensor fusion (camera + LiDAR + radar)\n",
    "3. **Notebook 6:** Pedestrian detection case study\n",
    "\n",
    "**Then in Session 2:** Analyze why these systems fail!\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created by Milin Patel | Hochschule Kempten*  \n",
    "*Last updated: 2025-01-17*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

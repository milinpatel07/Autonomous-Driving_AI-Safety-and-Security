{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 4: Autonomous Driving Dataset Exploration\n",
    "\n",
    "**Session 1: AI-based Perception Systems in Autonomous Vehicles**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/AV_Perception_Safety_Workshop/Session_1_AI_Perception_Systems/notebooks/04_Dataset_Exploration.ipynb)\n",
    "\n",
    "**Author:** Milin Patel  \n",
    "**Duration:** ~20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- âœ… Understand major autonomous driving datasets (KITTI, nuScenes, Waymo)\n",
    "- âœ… Explore dataset structure and annotations\n",
    "- âœ… Visualize sample driving scenes with labels\n",
    "- âœ… Compare dataset characteristics and use cases\n",
    "- âœ… Analyze class distributions and data diversity\n",
    "- âœ… Understand the importance of quality datasets for AV safety\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Setup and Imports\n",
    "\n",
    "This cell checks if you're running on Google Colab and installs necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if running on Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ðŸ”§ Running on Google Colab - Installing dependencies...\\n\")\n",
    "    !pip install -q opencv-python matplotlib numpy pillow requests pandas seaborn\n",
    "    print(\"âœ… Setup complete!\\n\")\n",
    "else:\n",
    "    print(\"ðŸ’» Running locally\\n\")\n",
    "\n",
    "print(\"âœ… Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Overview of Major AV Datasets\n",
    "\n",
    "Training robust perception systems requires **large-scale, diverse, and accurately labeled datasets**.\n",
    "\n",
    "### Major Public Datasets:\n",
    "\n",
    "| Dataset | Year | Scenes | Frames | Sensors | 3D Annotations | Key Features |\n",
    "|---------|------|--------|--------|---------|----------------|---------------|\n",
    "| **KITTI** | 2012 | 200+ | 200K+ | Camera, LiDAR, GPS | âœ… | First large-scale 3D dataset |\n",
    "| **nuScenes** | 2019 | 1,000 | 1.4M | 6 cameras, 5 radars, LiDAR | âœ… | Full 360Â° coverage, diverse cities |\n",
    "| **Waymo Open** | 2019 | 2,000+ | 200K+ | 5 cameras, 5 LiDARs | âœ… | Largest, highest quality |\n",
    "| **BDD100K** | 2020 | 100K | 100M | Camera | âŒ (2D only) | Diverse weather, time, location |\n",
    "| **Argoverse** | 2019 | 300+ | 300K+ | Cameras, LiDAR | âœ… | HD maps, trajectory prediction |\n",
    "\n",
    "**Today's focus:** KITTI and nuScenes (most widely used in research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "dataset_comparison = pd.DataFrame({\n",
    "    'Dataset': ['KITTI', 'nuScenes', 'Waymo Open', 'BDD100K', 'Argoverse'],\n",
    "    'Release Year': [2012, 2019, 2019, 2020, 2019],\n",
    "    'Total Frames': ['200K+', '1.4M', '200K+', '100M', '300K+'],\n",
    "    'Cameras': [4, 6, 5, 1, 7],\n",
    "    'LiDAR': ['1 (64-beam)', '1 (32-beam)', '5 (64-beam)', 'None', '2'],\n",
    "    'Radar': ['None', '5', 'None', 'None', 'None'],\n",
    "    '3D Boxes': [True, True, True, False, True],\n",
    "    'Weather Diversity': ['Low', 'High', 'Medium', 'High', 'Medium'],\n",
    "    'Geographic Coverage': ['Germany', 'USA, Singapore', 'USA', 'USA', 'USA'],\n",
    "    'Size (GB)': [150, 500, 1000, 100, 300]\n",
    "})\n",
    "\n",
    "display(dataset_comparison)\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Observations:\")\n",
    "print(\"   - Waymo Open: Largest and highest quality (but 1TB!)\")\n",
    "print(\"   - nuScenes: Best sensor diversity (cameras + radar + LiDAR)\")\n",
    "print(\"   - KITTI: Oldest but still widely used for benchmarking\")\n",
    "print(\"   - BDD100K: Largest image count, but no 3D data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Visualize Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Total frames comparison\n",
    "datasets = ['KITTI', 'nuScenes', 'Waymo', 'BDD100K', 'Argoverse']\n",
    "frames = [200000, 1400000, 200000, 100000000, 300000]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "\n",
    "axes[0, 0].barh(datasets, frames, color=colors)\n",
    "axes[0, 0].set_xlabel('Number of Frames', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Dataset Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xscale('log')\n",
    "for i, v in enumerate(frames):\n",
    "    axes[0, 0].text(v * 1.1, i, f'{v/1e6:.1f}M' if v >= 1e6 else f'{v/1e3:.0f}K', \n",
    "                    va='center', fontweight='bold')\n",
    "\n",
    "# 2. Sensor coverage\n",
    "sensor_data = {\n",
    "    'KITTI': {'Camera': 4, 'LiDAR': 1, 'Radar': 0},\n",
    "    'nuScenes': {'Camera': 6, 'LiDAR': 1, 'Radar': 5},\n",
    "    'Waymo': {'Camera': 5, 'LiDAR': 5, 'Radar': 0},\n",
    "    'BDD100K': {'Camera': 1, 'LiDAR': 0, 'Radar': 0},\n",
    "    'Argoverse': {'Camera': 7, 'LiDAR': 2, 'Radar': 0}\n",
    "}\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.25\n",
    "cameras = [sensor_data[d]['Camera'] for d in datasets]\n",
    "lidars = [sensor_data[d]['LiDAR'] for d in datasets]\n",
    "radars = [sensor_data[d]['Radar'] for d in datasets]\n",
    "\n",
    "axes[0, 1].bar(x - width, cameras, width, label='Cameras', color='#FF6B6B')\n",
    "axes[0, 1].bar(x, lidars, width, label='LiDARs', color='#4ECDC4')\n",
    "axes[0, 1].bar(x + width, radars, width, label='Radars', color='#45B7D1')\n",
    "axes[0, 1].set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Number of Sensors', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Sensor Coverage by Dataset', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(datasets, rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Geographic coverage\n",
    "locations = ['Germany', 'USA', 'Singapore']\n",
    "coverage = {\n",
    "    'KITTI': [1, 0, 0],\n",
    "    'nuScenes': [0, 1, 1],\n",
    "    'Waymo': [0, 1, 0],\n",
    "    'BDD100K': [0, 1, 0],\n",
    "    'Argoverse': [0, 1, 0]\n",
    "}\n",
    "\n",
    "coverage_matrix = np.array([coverage[d] for d in datasets])\n",
    "im = axes[1, 0].imshow(coverage_matrix, cmap='YlGn', aspect='auto')\n",
    "axes[1, 0].set_xticks(range(len(locations)))\n",
    "axes[1, 0].set_xticklabels(locations)\n",
    "axes[1, 0].set_yticks(range(len(datasets)))\n",
    "axes[1, 0].set_yticklabels(datasets)\n",
    "axes[1, 0].set_title('Geographic Coverage', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(len(locations)):\n",
    "        text = axes[1, 0].text(j, i, 'âœ“' if coverage_matrix[i, j] else '',\n",
    "                              ha=\"center\", va=\"center\", color=\"darkgreen\", \n",
    "                              fontsize=20, fontweight='bold')\n",
    "\n",
    "# 4. Dataset size (storage)\n",
    "sizes_gb = [150, 500, 1000, 100, 300]\n",
    "axes[1, 1].pie(sizes_gb, labels=datasets, autopct='%1.1f%%', \n",
    "               colors=colors, startangle=90)\n",
    "axes[1, 1].set_title('Storage Requirements (Total: 2.05TB)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset characteristics vary significantly!\")\n",
    "print(\"   Choice depends on: research goal, computational resources, geographic coverage needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ KITTI Dataset Deep Dive\n",
    "\n",
    "**KITTI Vision Benchmark Suite** (2012) - Karlsruhe Institute of Technology\n",
    "\n",
    "### Key Features:\n",
    "- **Location:** Karlsruhe, Germany (mid-size city)\n",
    "- **Duration:** Hours of driving\n",
    "- **Sensors:**\n",
    "  - 2 color cameras (stereo)\n",
    "  - 2 grayscale cameras (stereo)\n",
    "  - Velodyne HDL-64E LiDAR (64 beams)\n",
    "  - GPS/IMU\n",
    "- **Tasks:** 3D object detection, tracking, depth estimation, odometry, segmentation\n",
    "\n",
    "### Class Distribution (3D Object Detection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate KITTI training set class distribution\n",
    "kitti_classes = {\n",
    "    'Car': 28742,\n",
    "    'Pedestrian': 4487,\n",
    "    'Cyclist': 1627,\n",
    "    'Van': 2914,\n",
    "    'Truck': 1094,\n",
    "    'Tram': 511,\n",
    "    'Person_sitting': 222,\n",
    "    'Misc': 973\n",
    "}\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "classes = list(kitti_classes.keys())\n",
    "counts = list(kitti_classes.values())\n",
    "colors_bar = plt.cm.Set3(range(len(classes)))\n",
    "\n",
    "bars = axes[0].bar(classes, counts, color=colors_bar)\n",
    "axes[0].set_xlabel('Object Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Instances', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('KITTI Class Distribution (Training Set)', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Pie chart for proportions\n",
    "axes[1].pie(counts, labels=classes, autopct='%1.1f%%', \n",
    "            colors=colors_bar, startangle=90)\n",
    "axes[1].set_title('Class Proportions', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ Class Imbalance Observation:\")\n",
    "print(f\"   - Most common: Car ({kitti_classes['Car']:,} instances)\")\n",
    "print(f\"   - Least common: Person_sitting ({kitti_classes['Person_sitting']} instances)\")\n",
    "print(f\"   - Imbalance ratio: {kitti_classes['Car']/kitti_classes['Person_sitting']:.1f}:1\")\n",
    "print(\"\\nðŸ’¡ This imbalance affects model performance!\")\n",
    "print(\"   Models tend to perform worse on rare classes (e.g., Person_sitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Visualize Sample KITTI Scene\n",
    "\n",
    "Since we can't download the full KITTI dataset here, let's create a representative visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic KITTI-style visualization\n",
    "def create_kitti_style_scene():\n",
    "    \"\"\"Create a synthetic KITTI-style driving scene with annotations\"\"\"\n",
    "    # Create base image\n",
    "    img_width, img_height = 1242, 375\n",
    "    image = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Sky\n",
    "    image[:img_height//3] = [135, 206, 235]\n",
    "    \n",
    "    # Road\n",
    "    image[img_height//3:] = [105, 105, 105]\n",
    "    \n",
    "    # Add lane markings\n",
    "    for i in range(0, img_width, 80):\n",
    "        cv2.rectangle(image, (i, img_height-50), (i+40, img_height-45), (255, 255, 255), -1)\n",
    "    \n",
    "    # Add buildings\n",
    "    cv2.rectangle(image, (50, 50), (200, img_height//3), (169, 169, 169), -1)\n",
    "    cv2.rectangle(image, (900, 70), (1100, img_height//3), (139, 139, 139), -1)\n",
    "    \n",
    "    # Simulate objects with bounding boxes\n",
    "    objects = [\n",
    "        {'class': 'Car', 'bbox': [400, 180, 200, 120], 'color': (0, 255, 0)},\n",
    "        {'class': 'Car', 'bbox': [700, 200, 180, 100], 'color': (0, 255, 0)},\n",
    "        {'class': 'Pedestrian', 'bbox': [250, 200, 50, 120], 'color': (255, 0, 0)},\n",
    "        {'class': 'Cyclist', 'bbox': [550, 210, 60, 100], 'color': (255, 165, 0)},\n",
    "    ]\n",
    "    \n",
    "    # Draw objects\n",
    "    for obj in objects:\n",
    "        x, y, w, h = obj['bbox']\n",
    "        # Draw filled rectangle for object\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (80, 80, 80), -1)\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), obj['color'], 3)\n",
    "        # Add label\n",
    "        cv2.putText(image, obj['class'], (x, y-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, obj['color'], 2)\n",
    "    \n",
    "    return image, objects\n",
    "\n",
    "# Generate and visualize\n",
    "kitti_image, annotations = create_kitti_style_scene()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.imshow(kitti_image)\n",
    "plt.title('KITTI-Style Scene with 3D Object Annotations', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', label='Car'),\n",
    "    Patch(facecolor='red', label='Pedestrian'),\n",
    "    Patch(facecolor='orange', label='Cyclist')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='upper right', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Detected Objects:\")\n",
    "for i, obj in enumerate(annotations, 1):\n",
    "    print(f\"   {i}. {obj['class']}: Bbox {obj['bbox']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ nuScenes Dataset Deep Dive\n",
    "\n",
    "**nuScenes** (2019) - Motional (formerly nuTonomy)\n",
    "\n",
    "### Key Improvements over KITTI:\n",
    "- âœ… **Full 360Â° coverage** (6 cameras)\n",
    "- âœ… **Radar data** (5 radars for all-weather perception)\n",
    "- âœ… **Diverse locations** (Boston, Singapore)\n",
    "- âœ… **More weather conditions** (rain, night)\n",
    "- âœ… **Longer sequences** (20 seconds each)\n",
    "- âœ… **23 object classes** (vs. 8 in KITTI)\n",
    "\n",
    "### Sensor Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuScenes sensor configuration\n",
    "print(\"ðŸš— nuScenes Sensor Suite:\\n\")\n",
    "print(\"ðŸ“· Cameras (6):\")\n",
    "cameras = [\n",
    "    'CAM_FRONT',\n",
    "    'CAM_FRONT_LEFT',\n",
    "    'CAM_FRONT_RIGHT',\n",
    "    'CAM_BACK',\n",
    "    'CAM_BACK_LEFT',\n",
    "    'CAM_BACK_RIGHT'\n",
    "]\n",
    "for cam in cameras:\n",
    "    print(f\"   - {cam}\")\n",
    "\n",
    "print(\"\\nðŸ“¡ Radars (5):\")\n",
    "radars = [\n",
    "    'RADAR_FRONT',\n",
    "    'RADAR_FRONT_LEFT',\n",
    "    'RADAR_FRONT_RIGHT',\n",
    "    'RADAR_BACK_LEFT',\n",
    "    'RADAR_BACK_RIGHT'\n",
    "]\n",
    "for radar in radars:\n",
    "    print(f\"   - {radar}\")\n",
    "\n",
    "print(\"\\nðŸŒ LiDAR (1):\")\n",
    "print(\"   - LIDAR_TOP (32-beam, 360Â°)\")\n",
    "\n",
    "# Visualize sensor placement\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Draw vehicle (top-down view)\n",
    "vehicle = plt.Rectangle((-1, -2), 2, 4, fill=True, color='lightgray', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(vehicle)\n",
    "\n",
    "# Camera positions and FOV\n",
    "camera_positions = {\n",
    "    'FRONT': (0, 2.5, 70),\n",
    "    'FRONT_LEFT': (-1.5, 1.5, 70),\n",
    "    'FRONT_RIGHT': (1.5, 1.5, 70),\n",
    "    'BACK': (0, -2.5, 70),\n",
    "    'BACK_LEFT': (-1.5, -1.5, 70),\n",
    "    'BACK_RIGHT': (1.5, -1.5, 70)\n",
    "}\n",
    "\n",
    "for name, (x, y, fov) in camera_positions.items():\n",
    "    ax.plot(x, y, 'ro', markersize=10)\n",
    "    ax.text(x, y+0.5, name.replace('_', '\\n'), ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# LiDAR at center\n",
    "ax.plot(0, 0, 'g^', markersize=20, label='LiDAR (360Â°)')\n",
    "\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlabel('Lateral Distance (m)', fontsize=12)\n",
    "ax.set_ylabel('Longitudinal Distance (m)', fontsize=12)\n",
    "ax.set_title('nuScenes Sensor Placement (Top-Down View)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Full 360Â° coverage eliminates blind spots!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### nuScenes Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuScenes class categories (23 classes grouped)\n",
    "nuscenes_classes = {\n",
    "    'Vehicles': {\n",
    "        'car': 385000,\n",
    "        'truck': 55000,\n",
    "        'bus': 18000,\n",
    "        'trailer': 23000,\n",
    "        'construction_vehicle': 12000,\n",
    "        'motorcycle': 8500,\n",
    "        'bicycle': 9200\n",
    "    },\n",
    "    'Humans': {\n",
    "        'pedestrian': 96000\n",
    "    },\n",
    "    'Objects': {\n",
    "        'traffic_cone': 45000,\n",
    "        'barrier': 50000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flatten for visualization\n",
    "all_classes = {}\n",
    "for category, classes in nuscenes_classes.items():\n",
    "    all_classes.update(classes)\n",
    "\n",
    "# Sort by count\n",
    "sorted_classes = dict(sorted(all_classes.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "classes_list = list(sorted_classes.keys())\n",
    "counts_list = list(sorted_classes.values())\n",
    "\n",
    "axes[0].barh(classes_list, counts_list, color=plt.cm.viridis(np.linspace(0, 1, len(classes_list))))\n",
    "axes[0].set_xlabel('Number of Instances', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('nuScenes Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add counts\n",
    "for i, (cls, count) in enumerate(sorted_classes.items()):\n",
    "    axes[0].text(count + 5000, i, f'{count/1000:.0f}K', va='center', fontsize=9)\n",
    "\n",
    "# Category breakdown\n",
    "category_totals = {cat: sum(classes.values()) for cat, classes in nuscenes_classes.items()}\n",
    "axes[1].pie(category_totals.values(), labels=category_totals.keys(), \n",
    "            autopct='%1.1f%%', startangle=90, colors=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1].set_title('Class Categories', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š nuScenes has more balanced class distribution than KITTI\")\n",
    "print(\"   But still biased toward common objects (cars, pedestrians)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Dataset Quality and Diversity Analysis\n",
    "\n",
    "**Why dataset quality matters for safety:**\n",
    "- Models only learn what they see in training data\n",
    "- Rare scenarios (edge cases) often missing\n",
    "- Geographic/weather bias affects generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dataset diversity dimensions\n",
    "diversity_metrics = pd.DataFrame({\n",
    "    'Dimension': ['Weather Conditions', 'Time of Day', 'Geographic Diversity', \n",
    "                  'Traffic Density', 'Road Types', 'Object Classes'],\n",
    "    'KITTI': ['Low (mostly clear)', 'Day only', 'Single city', 'Medium', 'Urban/Highway', '8 classes'],\n",
    "    'nuScenes': ['High (rain, night)', 'Day & Night', '2 cities, 2 countries', 'High', 'Urban/Highway', '23 classes'],\n",
    "    'Waymo Open': ['Medium', 'Day & Night', 'Multiple US cities', 'High', 'Diverse', '4 main classes'],\n",
    "    'BDD100K': ['High', 'Day & Night', 'USA-wide', 'Very High', 'Very Diverse', '10 classes']\n",
    "})\n",
    "\n",
    "display(diversity_metrics)\n",
    "\n",
    "print(\"\\nâš ï¸ Diversity Gaps Lead to Safety Issues:\")\n",
    "print(\"   1. Tesla Autopilot struggles in heavy rain (trained mostly on clear weather)\")\n",
    "print(\"   2. Systems fail in new cities with different road layouts\")\n",
    "print(\"   3. Rare objects (animals, debris) often missed\")\n",
    "print(\"\\nðŸ’¡ Solution: Collect diverse data + data augmentation + synthetic data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Visualize Diversity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign diversity scores (0-10 scale)\n",
    "datasets_diversity = ['KITTI', 'nuScenes', 'Waymo', 'BDD100K']\n",
    "diversity_scores = {\n",
    "    'Weather': [3, 8, 6, 9],\n",
    "    'Time': [3, 8, 7, 8],\n",
    "    'Geography': [2, 6, 7, 10],\n",
    "    'Traffic': [6, 8, 8, 9],\n",
    "    'Road Types': [6, 7, 8, 10],\n",
    "    'Objects': [5, 9, 6, 7]\n",
    "}\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(diversity_scores), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "for i, dataset in enumerate(datasets_diversity):\n",
    "    values = [diversity_scores[dim][i] for dim in diversity_scores.keys()]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=dataset)\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(diversity_scores.keys(), fontsize=11)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_yticks([2, 4, 6, 8, 10])\n",
    "ax.set_yticklabels(['2', '4', '6', '8', '10'], fontsize=9)\n",
    "ax.set_title('Dataset Diversity Comparison (Radar Chart)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Ideal dataset: Large area covering all dimensions (approaching 10 on all axes)\")\n",
    "print(\"   No single dataset is perfect - consider combining multiple sources!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Edge Cases and Long-Tail Distribution\n",
    "\n",
    "**The Long-Tail Problem:**\n",
    "- Most scenarios are common (highway driving, clear weather)\n",
    "- Rare scenarios (animals, debris, unusual weather) are underrepresented\n",
    "- But rare scenarios are often safety-critical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate long-tail distribution of driving scenarios\n",
    "scenarios = [\n",
    "    'Highway cruising',\n",
    "    'Urban driving',\n",
    "    'Intersections',\n",
    "    'Parking lots',\n",
    "    'Night driving',\n",
    "    'Light rain',\n",
    "    'Heavy rain',\n",
    "    'Construction zones',\n",
    "    'Emergency vehicles',\n",
    "    'Animals crossing',\n",
    "    'Debris on road',\n",
    "    'Snow/ice',\n",
    "    'Fog',\n",
    "    'Unusual objects'\n",
    "]\n",
    "\n",
    "# Frequency (log scale)\n",
    "frequencies = [100000, 80000, 60000, 40000, 20000, 10000, 3000, 2000, 1000, 500, 300, 200, 150, 50]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Linear scale\n",
    "colors_scenarios = ['green' if f > 10000 else 'orange' if f > 1000 else 'red' for f in frequencies]\n",
    "axes[0].bar(range(len(scenarios)), frequencies, color=colors_scenarios)\n",
    "axes[0].set_xticks(range(len(scenarios)))\n",
    "axes[0].set_xticklabels(scenarios, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Frequency in Dataset', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Scenario Distribution in Training Data (Linear Scale)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Log scale\n",
    "axes[1].bar(range(len(scenarios)), frequencies, color=colors_scenarios)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xticks(range(len(scenarios)))\n",
    "axes[1].set_xticklabels(scenarios, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Frequency (log scale)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Scenario Distribution (Log Scale) - Reveals Long Tail', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', label='Common (>10K)'),\n",
    "    Patch(facecolor='orange', label='Uncommon (1K-10K)'),\n",
    "    Patch(facecolor='red', label='Rare (<1K)')\n",
    "]\n",
    "axes[1].legend(handles=legend_elements, loc='upper right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ The Long-Tail Problem:\")\n",
    "print(f\"   - Common scenarios: {sum(frequencies[:4])/sum(frequencies)*100:.1f}% of data\")\n",
    "print(f\"   - Rare scenarios: {sum(frequencies[9:])/sum(frequencies)*100:.1f}% of data\")\n",
    "print(\"\\nðŸ’¡ But rare scenarios cause accidents!\")\n",
    "print(\"   - Need targeted data collection for edge cases\")\n",
    "print(\"   - Simulation helps fill gaps (CARLA, LGSVL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœï¸ Exercise: Dataset Selection\n",
    "\n",
    "**Scenario:** You're developing a perception system for an autonomous shuttle that will operate:\n",
    "- **Location:** University campus (urban, low speed)\n",
    "- **Speed:** Max 25 km/h\n",
    "- **Weather:** All seasons (including snow)\n",
    "- **Time:** Day and night\n",
    "- **Key risks:** Pedestrians, cyclists, unexpected obstacles\n",
    "\n",
    "**Question:** Which dataset(s) would you choose and why? What are the gaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in your analysis\n",
    "your_choice = {\n",
    "    'primary_dataset': '',  # e.g., 'nuScenes'\n",
    "    'secondary_datasets': [],  # e.g., ['KITTI', 'BDD100K']\n",
    "    'reasoning': '',  # Explain your choice\n",
    "    'identified_gaps': [],  # What's missing?\n",
    "    'mitigation_strategy': ''  # How to address gaps?\n",
    "}\n",
    "\n",
    "print(\"ðŸ’¡ Consider:\")\n",
    "print(\"   - Sensor coverage needed\")\n",
    "print(\"   - Speed range (campus is low-speed)\")\n",
    "print(\"   - Weather diversity\")\n",
    "print(\"   - Pedestrian/cyclist density\")\n",
    "print(\"   - Dataset size (computational constraints)\")\n",
    "\n",
    "# Uncomment to see sample answer\n",
    "# sample_answer = {\n",
    "#     'primary_dataset': 'nuScenes',\n",
    "#     'secondary_datasets': ['BDD100K'],\n",
    "#     'reasoning': 'nuScenes has diverse weather, night data, and good pedestrian coverage. BDD100K adds more diversity.',\n",
    "#     'identified_gaps': ['Low-speed campus scenarios', 'Dense pedestrian crowds', 'Campus-specific objects (scooters, etc.)'],\n",
    "#     'mitigation_strategy': 'Collect custom campus data, use simulation (CARLA) for edge cases, data augmentation'\n",
    "# }\n",
    "# print(\"\\n Sample Answer:\", sample_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### Dataset Importance\n",
    "- **Quality > Quantity:** Diverse, accurately labeled data beats large but biased datasets\n",
    "- **No perfect dataset:** Each has strengths and weaknesses\n",
    "- **Combine sources:** Use multiple datasets to improve coverage\n",
    "\n",
    "### Major Datasets Compared\n",
    "- **KITTI:** Pioneer, still used for benchmarks, limited diversity\n",
    "- **nuScenes:** Best sensor variety, good weather/time diversity, 360Â° coverage\n",
    "- **Waymo Open:** Largest, highest quality, but USA-centric\n",
    "- **BDD100K:** Most geographic diversity, but camera-only (no 3D)\n",
    "\n",
    "### Safety Implications\n",
    "- **Class imbalance:** Models worse on rare but critical objects\n",
    "- **Long-tail distribution:** Rare scenarios cause accidents\n",
    "- **Geographic bias:** Models fail in new locations\n",
    "- **Weather gaps:** Missing conditions lead to failures\n",
    "\n",
    "### Best Practices\n",
    "1. **Understand ODD:** Choose datasets matching deployment environment\n",
    "2. **Identify gaps:** What scenarios are missing?\n",
    "3. **Collect targeted data:** Fill critical gaps\n",
    "4. **Use simulation:** Generate edge cases (more in Session 2!)\n",
    "5. **Validate thoroughly:** Test on held-out data from different sources\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”œ Next: Sensor Fusion Basics\n",
    "\n",
    "Now that we understand datasets, let's learn how to fuse data from multiple sensors!\n",
    "\n",
    "**Open Notebook 5:** `05_Sensor_Fusion_Basics.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created by Milin Patel | Hochschule Kempten*  \n",
    "*Last updated: 2025-01-17*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

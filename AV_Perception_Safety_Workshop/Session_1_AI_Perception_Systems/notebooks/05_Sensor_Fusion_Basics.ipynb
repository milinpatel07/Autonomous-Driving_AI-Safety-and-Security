{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 5: Sensor Fusion Basics for Autonomous Vehicles\n",
    "\n",
    "**Session 1: AI-based Perception Systems in Autonomous Vehicles**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/AV_Perception_Safety_Workshop/Session_1_AI_Perception_Systems/notebooks/05_Sensor_Fusion_Basics.ipynb)\n",
    "\n",
    "**Author:** Milin Patel  \n",
    "**Duration:** ~15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand why sensor fusion is critical for AV safety\n",
    "- ‚úÖ Learn three fusion approaches: early, late, and deep fusion\n",
    "- ‚úÖ Implement simple late fusion for camera + LiDAR detections\n",
    "- ‚úÖ Visualize fused detection results\n",
    "- ‚úÖ Analyze fusion performance vs. single-sensor\n",
    "- ‚úÖ Understand fusion challenges and failure modes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Check if running on Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\\n\")\n",
    "    !pip install -q matplotlib numpy scipy pandas seaborn\n",
    "    print(\"‚úÖ Setup complete!\\n\")\n",
    "else:\n",
    "    print(\"üíª Running locally\\n\")\n",
    "\n",
    "print(\"‚úÖ Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Why Sensor Fusion?\n",
    "\n",
    "**Problem:** No single sensor is perfect!\n",
    "\n",
    "- **Camera:** Good for classification, bad in rain/night\n",
    "- **LiDAR:** Accurate 3D, expensive, affected by fog\n",
    "- **Radar:** All-weather, but low resolution\n",
    "\n",
    "**Solution:** Combine (fuse) multiple sensors to get the best of all worlds!\n",
    "\n",
    "### Benefits of Sensor Fusion:\n",
    "1. **Redundancy:** If one sensor fails, others compensate\n",
    "2. **Complementary strengths:** Camera sees colors, LiDAR measures depth\n",
    "3. **Improved accuracy:** Reduces false positives/negatives\n",
    "4. **All-weather operation:** Some sensor always works\n",
    "5. **Safety compliance:** Required by ISO 26262 for ASIL-D systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate sensor performance in different conditions\n",
    "conditions = ['Clear\\nDay', 'Rain', 'Fog', 'Night', 'Snow']\n",
    "camera_acc = [95, 40, 30, 25, 35]\n",
    "lidar_acc = [95, 75, 50, 95, 60]\n",
    "radar_acc = [85, 95, 85, 90, 90]\n",
    "fusion_acc = [98, 90, 75, 92, 80]  # Fusion improves overall\n",
    "\n",
    "x = np.arange(len(conditions))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars1 = ax.bar(x - 1.5*width, camera_acc, width, label='üì∑ Camera', color='#FF6B6B')\n",
    "bars2 = ax.bar(x - 0.5*width, lidar_acc, width, label='üåê LiDAR', color='#4ECDC4')\n",
    "bars3 = ax.bar(x + 0.5*width, radar_acc, width, label='üì° Radar', color='#45B7D1')\n",
    "bars4 = ax.bar(x + 1.5*width, fusion_acc, width, label='üîó Fusion', color='#95E1D3')\n",
    "\n",
    "ax.set_xlabel('Weather Condition', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Detection Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Sensor Fusion Improves Robustness Across Conditions', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(conditions)\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "               f'{int(height)}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: Fusion never worse than best individual sensor!\")\n",
    "print(\"   In fact, often BETTER due to complementary information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Three Fusion Approaches\n",
    "\n",
    "### 1. Early Fusion (Data-Level)\n",
    "Combine **raw sensor data** before processing.\n",
    "\n",
    "```\n",
    "Camera Image + LiDAR Points ‚Üí Fused Input ‚Üí Detection Model ‚Üí Objects\n",
    "```\n",
    "\n",
    "**Pros:** Preserves all information  \n",
    "**Cons:** Complex, computationally expensive, requires precise calibration\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Late Fusion (Decision-Level)\n",
    "Process sensors **independently**, then combine detections.\n",
    "\n",
    "```\n",
    "Camera ‚Üí Detection1 ‚Üò\n",
    "                      ‚Üí Fusion ‚Üí Final Objects\n",
    "LiDAR ‚Üí Detection2  ‚Üó\n",
    "```\n",
    "\n",
    "**Pros:** Simple, modular, can use existing models  \n",
    "**Cons:** Information loss, harder to resolve conflicts\n",
    "\n",
    "**Most common in industry!** (We'll implement this today)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Deep Fusion (Feature-Level)\n",
    "Fuse **learned features** from neural networks.\n",
    "\n",
    "```\n",
    "Camera ‚Üí CNN Features ‚Üò\n",
    "                        ‚Üí Fusion Network ‚Üí Objects\n",
    "LiDAR ‚Üí PointNet Features ‚Üó\n",
    "```\n",
    "\n",
    "**Pros:** Learns optimal fusion, state-of-the-art accuracy  \n",
    "**Cons:** Requires large paired datasets, end-to-end training\n",
    "\n",
    "**Research frontier!** (Used in modern systems like Tesla FSD, Waymo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fusion approaches\n",
    "fusion_comparison = pd.DataFrame({\n",
    "    'Approach': ['Early Fusion', 'Late Fusion', 'Deep Fusion'],\n",
    "    'Fusion Level': ['Raw Data', 'Detections', 'Features'],\n",
    "    'Complexity': ['High', 'Low', 'Very High'],\n",
    "    'Accuracy': ['Good', 'Medium', 'Best'],\n",
    "    'Computational Cost': ['High', 'Low', 'Very High'],\n",
    "    'Calibration Sensitivity': ['Very High', 'Medium', 'High'],\n",
    "    'Modularity': ['Low', 'High', 'Medium'],\n",
    "    'Industry Use': ['Rare', 'Common', 'Growing']\n",
    "})\n",
    "\n",
    "display(fusion_comparison)\n",
    "\n",
    "print(\"\\nüí° Today we implement Late Fusion (simplest, most practical)\")\n",
    "print(\"   But deep fusion is the future (BEVFusion, TransFusion, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Implement Simple Late Fusion\n",
    "\n",
    "**Task:** Fuse camera and LiDAR detections\n",
    "\n",
    "**Algorithm:**\n",
    "1. Run camera detector ‚Üí get bounding boxes\n",
    "2. Run LiDAR detector ‚Üí get 3D boxes\n",
    "3. Project 3D boxes to image plane\n",
    "4. Match camera and LiDAR detections (IoU-based)\n",
    "5. Combine matched detections (weighted by confidence)\n",
    "6. Keep unmatched high-confidence detections\n",
    "\n",
    "Let's implement step-by-step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for late fusion\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) between two 2D boxes.\n",
    "    \n",
    "    Args:\n",
    "        box1, box2: [x1, y1, x2, y2]\n",
    "    \n",
    "    Returns:\n",
    "        iou: float between 0 and 1\n",
    "    \"\"\"\n",
    "    x1_max = max(box1[0], box2[0])\n",
    "    y1_max = max(box1[1], box2[1])\n",
    "    x2_min = min(box1[2], box2[2])\n",
    "    y2_min = min(box1[3], box2[3])\n",
    "    \n",
    "    # Intersection area\n",
    "    inter_width = max(0, x2_min - x1_max)\n",
    "    inter_height = max(0, y2_min - y1_max)\n",
    "    inter_area = inter_width * inter_height\n",
    "    \n",
    "    # Union area\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    if union_area == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def match_detections(camera_dets, lidar_dets, iou_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Match camera and LiDAR detections using Hungarian algorithm.\n",
    "    \n",
    "    Args:\n",
    "        camera_dets: List of dicts with 'bbox', 'confidence', 'class'\n",
    "        lidar_dets: List of dicts with 'bbox', 'confidence', 'class'\n",
    "        iou_threshold: Minimum IoU for valid match\n",
    "    \n",
    "    Returns:\n",
    "        matches: List of (camera_idx, lidar_idx) pairs\n",
    "        unmatched_camera: List of camera indices\n",
    "        unmatched_lidar: List of lidar indices\n",
    "    \"\"\"\n",
    "    if len(camera_dets) == 0 or len(lidar_dets) == 0:\n",
    "        return [], list(range(len(camera_dets))), list(range(len(lidar_dets)))\n",
    "    \n",
    "    # Compute IoU matrix\n",
    "    iou_matrix = np.zeros((len(camera_dets), len(lidar_dets)))\n",
    "    for i, cam_det in enumerate(camera_dets):\n",
    "        for j, lid_det in enumerate(lidar_dets):\n",
    "            # Only match same class\n",
    "            if cam_det['class'] == lid_det['class']:\n",
    "                iou_matrix[i, j] = compute_iou(cam_det['bbox'], lid_det['bbox'])\n",
    "    \n",
    "    # Hungarian algorithm for optimal matching\n",
    "    row_ind, col_ind = linear_sum_assignment(-iou_matrix)  # Maximize IoU\n",
    "    \n",
    "    # Filter by threshold\n",
    "    matches = []\n",
    "    matched_cam = set()\n",
    "    matched_lid = set()\n",
    "    \n",
    "    for i, j in zip(row_ind, col_ind):\n",
    "        if iou_matrix[i, j] >= iou_threshold:\n",
    "            matches.append((i, j))\n",
    "            matched_cam.add(i)\n",
    "            matched_lid.add(j)\n",
    "    \n",
    "    unmatched_camera = [i for i in range(len(camera_dets)) if i not in matched_cam]\n",
    "    unmatched_lidar = [j for j in range(len(lidar_dets)) if j not in matched_lid]\n",
    "    \n",
    "    return matches, unmatched_camera, unmatched_lidar\n",
    "\n",
    "\n",
    "def fuse_detections(camera_dets, lidar_dets, camera_weight=0.5):\n",
    "    \"\"\"\n",
    "    Fuse camera and LiDAR detections using late fusion.\n",
    "    \n",
    "    Args:\n",
    "        camera_dets: List of camera detections\n",
    "        lidar_dets: List of LiDAR detections\n",
    "        camera_weight: Weight for camera confidence (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        fused_dets: List of fused detections\n",
    "    \"\"\"\n",
    "    # Match detections\n",
    "    matches, unmatched_cam, unmatched_lid = match_detections(camera_dets, lidar_dets)\n",
    "    \n",
    "    fused = []\n",
    "    \n",
    "    # Fuse matched detections (weighted average)\n",
    "    for cam_idx, lid_idx in matches:\n",
    "        cam_det = camera_dets[cam_idx]\n",
    "        lid_det = lidar_dets[lid_idx]\n",
    "        \n",
    "        # Average bounding box\n",
    "        fused_bbox = [\n",
    "            camera_weight * cam_det['bbox'][i] + (1 - camera_weight) * lid_det['bbox'][i]\n",
    "            for i in range(4)\n",
    "        ]\n",
    "        \n",
    "        # Combine confidence (average)\n",
    "        fused_conf = camera_weight * cam_det['confidence'] + (1 - camera_weight) * lid_det['confidence']\n",
    "        \n",
    "        fused.append({\n",
    "            'bbox': fused_bbox,\n",
    "            'confidence': fused_conf,\n",
    "            'class': cam_det['class'],\n",
    "            'source': 'fused'\n",
    "        })\n",
    "    \n",
    "    # Add high-confidence unmatched detections\n",
    "    for idx in unmatched_cam:\n",
    "        if camera_dets[idx]['confidence'] > 0.7:  # High confidence threshold\n",
    "            det = camera_dets[idx].copy()\n",
    "            det['source'] = 'camera_only'\n",
    "            fused.append(det)\n",
    "    \n",
    "    for idx in unmatched_lid:\n",
    "        if lidar_dets[idx]['confidence'] > 0.7:\n",
    "            det = lidar_dets[idx].copy()\n",
    "            det['source'] = 'lidar_only'\n",
    "            fused.append(det)\n",
    "    \n",
    "    return fused\n",
    "\n",
    "print(\"‚úÖ Fusion functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Test Fusion on Simulated Detections\n",
    "\n",
    "Let's simulate a driving scene with camera and LiDAR detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate detections from camera and LiDAR\n",
    "np.random.seed(42)\n",
    "\n",
    "# Camera detections (good for classification, some false positives)\n",
    "camera_detections = [\n",
    "    {'bbox': [100, 200, 250, 350], 'confidence': 0.92, 'class': 'car'},\n",
    "    {'bbox': [400, 180, 520, 320], 'confidence': 0.88, 'class': 'car'},\n",
    "    {'bbox': [650, 220, 720, 380], 'confidence': 0.65, 'class': 'pedestrian'},  # Low conf\n",
    "    {'bbox': [800, 150, 900, 280], 'confidence': 0.45, 'class': 'bicycle'},     # False positive?\n",
    "]\n",
    "\n",
    "# LiDAR detections (accurate 3D, may miss small objects)\n",
    "lidar_detections = [\n",
    "    {'bbox': [105, 205, 248, 348], 'confidence': 0.95, 'class': 'car'},        # Matches cam[0]\n",
    "    {'bbox': [395, 185, 525, 325], 'confidence': 0.90, 'class': 'car'},        # Matches cam[1]\n",
    "    {'bbox': [300, 250, 380, 360], 'confidence': 0.85, 'class': 'truck'},      # Unmatched\n",
    "    # Note: LiDAR missed the pedestrian (too small/far)\n",
    "]\n",
    "\n",
    "print(\"üì∑ Camera Detections:\")\n",
    "for i, det in enumerate(camera_detections):\n",
    "    print(f\"   {i+1}. {det['class']} (conf: {det['confidence']:.2f}): {det['bbox']}\")\n",
    "\n",
    "print(\"\\nüåê LiDAR Detections:\")\n",
    "for i, det in enumerate(lidar_detections):\n",
    "    print(f\"   {i+1}. {det['class']} (conf: {det['confidence']:.2f}): {det['bbox']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fusion\n",
    "fused_detections = fuse_detections(camera_detections, lidar_detections, camera_weight=0.5)\n",
    "\n",
    "print(\"\\nüîó Fused Detections:\")\n",
    "for i, det in enumerate(fused_detections):\n",
    "    print(f\"   {i+1}. {det['class']} (conf: {det['confidence']:.2f}, source: {det['source']}): {[int(x) for x in det['bbox']]}\")\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Camera: {len(camera_detections)} detections\")\n",
    "print(f\"   LiDAR: {len(lidar_detections)} detections\")\n",
    "print(f\"   Fused: {len(fused_detections)} detections\")\n",
    "print(f\"\\nüí° Fusion filtered out low-confidence false positives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Visualize Fusion Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def visualize_detections(camera_dets, lidar_dets, fused_dets, img_size=(1000, 400)):\n",
    "    \"\"\"\n",
    "    Visualize camera, LiDAR, and fused detections side by side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Create dummy background\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(0, img_size[0])\n",
    "        ax.set_ylim(img_size[1], 0)  # Flip y-axis for image coordinates\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Camera detections\n",
    "    for det in camera_dets:\n",
    "        x1, y1, x2, y2 = det['bbox']\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        color = 'green' if det['confidence'] > 0.7 else 'orange' if det['confidence'] > 0.5 else 'red'\n",
    "        rect = Rectangle((x1, y1), width, height, linewidth=2, \n",
    "                        edgecolor=color, facecolor='none')\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].text(x1, y1-5, f\"{det['class']}\\n{det['confidence']:.2f}\", \n",
    "                    fontsize=9, color=color, fontweight='bold')\n",
    "    axes[0].set_title(f'üì∑ Camera Detections ({len(camera_dets)})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # LiDAR detections\n",
    "    for det in lidar_dets:\n",
    "        x1, y1, x2, y2 = det['bbox']\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        color = 'blue'\n",
    "        rect = Rectangle((x1, y1), width, height, linewidth=2, \n",
    "                        edgecolor=color, facecolor='none')\n",
    "        axes[1].add_patch(rect)\n",
    "        axes[1].text(x1, y1-5, f\"{det['class']}\\n{det['confidence']:.2f}\", \n",
    "                    fontsize=9, color=color, fontweight='bold')\n",
    "    axes[1].set_title(f'üåê LiDAR Detections ({len(lidar_dets)})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Fused detections\n",
    "    for det in fused_dets:\n",
    "        x1, y1, x2, y2 = det['bbox']\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        if det['source'] == 'fused':\n",
    "            color = 'purple'\n",
    "        elif det['source'] == 'camera_only':\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'blue'\n",
    "        rect = Rectangle((x1, y1), width, height, linewidth=2, \n",
    "                        edgecolor=color, facecolor='none', linestyle='--' if 'only' in det['source'] else '-')\n",
    "        axes[2].add_patch(rect)\n",
    "        axes[2].text(x1, y1-5, f\"{det['class']}\\n{det['confidence']:.2f}\\n({det['source']})\", \n",
    "                    fontsize=8, color=color, fontweight='bold')\n",
    "    axes[2].set_title(f'üîó Fused Detections ({len(fused_dets)})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "visualize_detections(camera_detections, lidar_detections, fused_detections)\n",
    "\n",
    "print(\"\\nüí° Notice:\")\n",
    "print(\"   - Purple boxes: Fused from both sensors (highest confidence)\")\n",
    "print(\"   - Green dashed: Camera-only (e.g., small objects LiDAR missed)\")\n",
    "print(\"   - Blue dashed: LiDAR-only (e.g., objects camera misclassified)\")\n",
    "print(\"   - Low-confidence detections filtered out!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Analyze Fusion Performance\n",
    "\n",
    "Let's compare precision and recall for single-sensor vs. fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate ground truth and compute metrics\n",
    "ground_truth = [\n",
    "    {'class': 'car', 'bbox': [100, 200, 250, 350]},\n",
    "    {'class': 'car', 'bbox': [400, 180, 520, 320]},\n",
    "    {'class': 'truck', 'bbox': [300, 250, 380, 360]},\n",
    "    {'class': 'pedestrian', 'bbox': [650, 220, 720, 380]},\n",
    "]\n",
    "\n",
    "def compute_metrics(detections, ground_truth, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute precision and recall.\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    \n",
    "    for det in detections:\n",
    "        for gt in ground_truth:\n",
    "            if det['class'] == gt['class']:\n",
    "                iou = compute_iou(det['bbox'], gt['bbox'])\n",
    "                if iou >= iou_threshold:\n",
    "                    true_positives += 1\n",
    "                    break\n",
    "    \n",
    "    precision = true_positives / len(detections) if len(detections) > 0 else 0\n",
    "    recall = true_positives / len(ground_truth) if len(ground_truth) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Compute metrics for each approach\n",
    "cam_p, cam_r, cam_f1 = compute_metrics(camera_detections, ground_truth)\n",
    "lid_p, lid_r, lid_f1 = compute_metrics(lidar_detections, ground_truth)\n",
    "fus_p, fus_r, fus_f1 = compute_metrics(fused_detections, ground_truth)\n",
    "\n",
    "# Visualize comparison\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Method': ['Camera', 'LiDAR', 'Fusion'],\n",
    "    'Precision': [cam_p, lid_p, fus_p],\n",
    "    'Recall': [cam_r, lid_r, fus_r],\n",
    "    'F1-Score': [cam_f1, lid_f1, fus_f1]\n",
    "})\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(3)\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, metrics_df['Precision'], width, label='Precision', color='#FF6B6B')\n",
    "ax.bar(x, metrics_df['Recall'], width, label='Recall', color='#4ECDC4')\n",
    "ax.bar(x + width, metrics_df['F1-Score'], width, label='F1-Score', color='#95E1D3')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Fusion Improves Detection Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_df['Method'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i in range(3):\n",
    "    for j, metric in enumerate(['Precision', 'Recall', 'F1-Score']):\n",
    "        val = metrics_df.iloc[i][metric]\n",
    "        ax.text(i + (j-1)*width, val + 0.02, f'{val:.2f}', \n",
    "               ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   Camera: Precision={cam_p:.2f}, Recall={cam_r:.2f}, F1={cam_f1:.2f}\")\n",
    "print(f\"   LiDAR:  Precision={lid_p:.2f}, Recall={lid_r:.2f}, F1={lid_f1:.2f}\")\n",
    "print(f\"   Fusion: Precision={fus_p:.2f}, Recall={fus_r:.2f}, F1={fus_f1:.2f}\")\n",
    "print(f\"\\n‚úÖ Fusion achieves best F1-score by balancing precision and recall!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Fusion Challenges and Failure Modes\n",
    "\n",
    "**Sensor fusion is not a silver bullet!** It has challenges:\n",
    "\n",
    "### 1. Calibration Errors\n",
    "- Sensors must be precisely aligned\n",
    "- Misalignment ‚Üí wrong associations\n",
    "- Example: Camera thinks object at X, LiDAR says Y ‚Üí no match!\n",
    "\n",
    "### 2. Temporal Synchronization\n",
    "- Sensors capture at different times\n",
    "- Fast-moving objects ‚Üí position mismatch\n",
    "- Need timestamp alignment (typically 100ms tolerance)\n",
    "\n",
    "### 3. Conflicting Information\n",
    "- Camera: \"It's a car\"\n",
    "- LiDAR: \"It's a truck\"\n",
    "- How to resolve? (Usually: trust higher-confidence sensor)\n",
    "\n",
    "### 4. Sensor Degradation\n",
    "- Dirty lens, water droplets, mud\n",
    "- System must detect degraded sensors\n",
    "- Fallback to other sensors\n",
    "\n",
    "### 5. Computational Cost\n",
    "- Processing multiple sensors is expensive\n",
    "- Real-time requirement: < 100ms\n",
    "- Need efficient algorithms + GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate calibration error impact\n",
    "calibration_errors = [0, 10, 20, 30, 40, 50]  # pixels\n",
    "fusion_accuracy = []\n",
    "\n",
    "for error in calibration_errors:\n",
    "    # Shift LiDAR detections to simulate calibration error\n",
    "    shifted_lidar = []\n",
    "    for det in lidar_detections:\n",
    "        shifted = det.copy()\n",
    "        shifted['bbox'] = [det['bbox'][i] + error for i in range(4)]\n",
    "        shifted_lidar.append(shifted)\n",
    "    \n",
    "    # Fuse with shifted LiDAR\n",
    "    fused = fuse_detections(camera_detections, shifted_lidar)\n",
    "    _, _, f1 = compute_metrics(fused, ground_truth)\n",
    "    fusion_accuracy.append(f1)\n",
    "\n",
    "# Plot impact\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(calibration_errors, fusion_accuracy, 'o-', linewidth=2, markersize=8, color='#FF6B6B')\n",
    "plt.xlabel('Calibration Error (pixels)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Fusion F1-Score', fontsize=12, fontweight='bold')\n",
    "plt.title('Impact of Calibration Error on Fusion Performance', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=cam_f1, color='green', linestyle='--', label='Camera-only baseline')\n",
    "plt.axhline(y=lid_f1, color='blue', linestyle='--', label='LiDAR-only baseline')\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Calibration Error Impact:\")\n",
    "print(f\"   0 pixels error: F1 = {fusion_accuracy[0]:.2f}\")\n",
    "print(f\"   50 pixels error: F1 = {fusion_accuracy[-1]:.2f}\")\n",
    "print(f\"\\nüí° With large calibration errors, fusion can be WORSE than single sensor!\")\n",
    "print(\"   ‚Üí Regular calibration checks are critical for safety\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úèÔ∏è Exercise: Design a Fusion Strategy\n",
    "\n",
    "**Scenario:** You're designing sensor fusion for a Level 4 autonomous shuttle.\n",
    "\n",
    "**Available sensors:**\n",
    "- 4 cameras (front, back, left, right)\n",
    "- 1 LiDAR (360¬∞)\n",
    "- 2 radars (front, back)\n",
    "\n",
    "**Questions:**\n",
    "1. Which fusion approach would you use? (early/late/deep)\n",
    "2. How would you handle sensor failures?\n",
    "3. What's your strategy for conflicting detections?\n",
    "4. How would you validate fusion performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in your fusion strategy\n",
    "your_strategy = {\n",
    "    'fusion_approach': '',  # early/late/deep\n",
    "    'reasoning': '',\n",
    "    'failure_handling': '',\n",
    "    'conflict_resolution': '',\n",
    "    'validation_plan': ''\n",
    "}\n",
    "\n",
    "print(\"üí° Consider:\")\n",
    "print(\"   - Computational constraints (real-time)\")\n",
    "print(\"   - Safety criticality (ASIL-D)\")\n",
    "print(\"   - Sensor failure modes\")\n",
    "print(\"   - Environmental conditions\")\n",
    "\n",
    "# Sample answer (uncomment to see)\n",
    "# sample_answer = {\n",
    "#     'fusion_approach': 'Late fusion with deep fusion for critical zones',\n",
    "#     'reasoning': 'Late fusion is proven and modular. Deep fusion for pedestrian-rich areas.',\n",
    "#     'failure_handling': 'Detect sensor health, degrade gracefully, use redundancy',\n",
    "#     'conflict_resolution': 'Weighted voting based on sensor confidence + environmental conditions',\n",
    "#     'validation_plan': 'Test on nuScenes + custom campus data + simulation + field tests'\n",
    "# }\n",
    "# print(\"\\nSample Answer:\", sample_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### Why Fusion?\n",
    "- **No single sensor is perfect** - each has strengths and weaknesses\n",
    "- **Complementary information** - camera for semantics, LiDAR for 3D, radar for weather\n",
    "- **Redundancy** - critical for safety (ISO 26262 requirement)\n",
    "\n",
    "### Three Fusion Approaches\n",
    "1. **Early fusion:** Fuse raw data (complex, best info preservation)\n",
    "2. **Late fusion:** Fuse detections (simple, modular, industry standard)\n",
    "3. **Deep fusion:** Fuse learned features (state-of-the-art, requires large data)\n",
    "\n",
    "### Fusion Benefits\n",
    "- ‚úÖ Improved accuracy (higher precision AND recall)\n",
    "- ‚úÖ Robustness to weather (always have working sensor)\n",
    "- ‚úÖ Reduced false positives/negatives\n",
    "- ‚úÖ Safety through redundancy\n",
    "\n",
    "### Challenges\n",
    "- ‚ö†Ô∏è Calibration errors degrade fusion\n",
    "- ‚ö†Ô∏è Temporal synchronization needed\n",
    "- ‚ö†Ô∏è Computational cost (real-time constraint)\n",
    "- ‚ö†Ô∏è Conflicting information resolution\n",
    "\n",
    "### Best Practices\n",
    "1. **Regular calibration** - check alignment frequently\n",
    "2. **Timestamp alignment** - synchronize sensor data\n",
    "3. **Sensor health monitoring** - detect degradation\n",
    "4. **Graceful degradation** - fallback strategies\n",
    "5. **Extensive validation** - test all weather/lighting conditions\n",
    "\n",
    "---\n",
    "\n",
    "## üîú Next: Pedestrian Detection Case Study\n",
    "\n",
    "Now let's apply what we learned to a safety-critical task: **pedestrian detection**!\n",
    "\n",
    "**Open Notebook 6:** `06_Pedestrian_Detection_Case_Study.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created by Milin Patel | Hochschule Kempten*  \n",
    "*Last updated: 2025-01-17*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

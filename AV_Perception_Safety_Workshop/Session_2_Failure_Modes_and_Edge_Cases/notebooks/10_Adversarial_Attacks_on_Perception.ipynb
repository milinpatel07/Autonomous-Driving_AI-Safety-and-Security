{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright (c) 2025 Milin Patel\n",
    "Hochschule Kempten - University of Applied Sciences\n",
    "\n",
    "Autonomous Driving: AI Safety and Security Workshop\n",
    "This project is licensed under the MIT License.\n",
    "See LICENSE file in the root directory for full license text.\n",
    "-->\n",
    "\n",
    "*Copyright ¬© 2025 Milin Patel. All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: Adversarial Attacks on AV Perception Systems\n",
    "\n",
    "**Session 2: Failure Modes and Edge Cases**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/Autonomous-Driving_AI-Safety-and-Security/blob/main/AV_Perception_Safety_Workshop/Session_2_Failure_Modes_and_Edge_Cases/notebooks/10_Adversarial_Attacks_on_Perception.ipynb)\n",
    "\n",
    "**Author:** Milin Patel  \n",
    "**Duration:** ~25 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand adversarial examples and their threat to AVs\n",
    "- ‚úÖ Implement FGSM and PGD digital attacks\n",
    "- ‚úÖ Analyze physical adversarial patches on traffic signs\n",
    "- ‚úÖ Explore sensor spoofing attacks (LiDAR, camera)\n",
    "- ‚úÖ Implement defense mechanisms (adversarial training, input validation)\n",
    "- ‚úÖ Connect adversarial robustness to ISO/SAE 21434 cybersecurity\n",
    "- ‚úÖ Design security requirements for AV systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ What are Adversarial Examples?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Adversarial examples** are inputs intentionally designed to cause a machine learning model to make a mistake.\n",
    "\n",
    "**Key property:** Small, **imperceptible** perturbations cause **large** changes in model output\n",
    "\n",
    "$$x_{adv} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x L(x, y))$$\n",
    "\n",
    "Where:\n",
    "- $x$ = original input\n",
    "- $x_{adv}$ = adversarial example\n",
    "- $\\epsilon$ = perturbation magnitude (small!)\n",
    "- $\\nabla_x L$ = gradient of loss\n",
    "\n",
    "### Example\n",
    "\n",
    "**Original:** Stop sign ‚Üí Classified as \"stop sign\" (99% confidence)  \n",
    "**Adversarial:** Stop sign + tiny noise ‚Üí Classified as \"speed limit 45\" (95% confidence)\n",
    "\n",
    "**Danger for AVs:**\n",
    "- Attacker can cause misclassification\n",
    "- Imperceptible to humans\n",
    "- Can be physical (stickers on real signs)\n",
    "\n",
    "### Why do adversarial examples exist?\n",
    "\n",
    "1. **High dimensionality:** Images have millions of pixels\n",
    "2. **Linear nature:** Neural networks locally linear\n",
    "3. **Transferability:** Attacks transfer across models\n",
    "4. **Optimization:** Networks optimize for average case, not worst case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adversarial example concept\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Simulate an example\n",
    "np.random.seed(42)\n",
    "original = np.random.rand(64, 64, 3) * 0.5 + 0.25  # Original image\n",
    "perturbation = (np.random.rand(64, 64, 3) - 0.5) * 0.1  # Small noise\n",
    "adversarial = np.clip(original + perturbation, 0, 1)\n",
    "difference = np.abs(adversarial - original) * 10  # Amplified for visibility\n",
    "\n",
    "axes[0].imshow(original)\n",
    "axes[0].set_title('Original Image\\n\"Stop Sign\"\\nConfidence: 99%', fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(perturbation + 0.5, cmap='seismic', vmin=0, vmax=1)\n",
    "axes[1].set_title('Perturbation\\n(Amplified for visibility)', fontsize=11, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(adversarial)\n",
    "axes[2].set_title('Adversarial Image\\n\"Speed Limit 45\"\\nConfidence: 95%', \n",
    "                  fontsize=11, fontweight='bold', color='red')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(difference)\n",
    "axes[3].set_title('Difference\\n(Amplified 10x)', fontsize=11, fontweight='bold')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ö†Ô∏è Danger: Tiny, imperceptible changes cause misclassification!\")\n",
    "print(\"   Human sees: Stop sign\")\n",
    "print(\"   AV sees: Speed limit 45\")\n",
    "print(\"   Result: Vehicle doesn't stop ‚Üí Collision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Digital Attack 1: FGSM (Fast Gradient Sign Method)\n",
    "\n",
    "**Idea:** Perturb image in direction of gradient\n",
    "\n",
    "**Algorithm:**\n",
    "1. Compute loss $L(x, y_{true})$\n",
    "2. Compute gradient $\\nabla_x L$\n",
    "3. Add perturbation: $x_{adv} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x L)$\n",
    "\n",
    "**Properties:**\n",
    "- **Fast:** Single gradient computation\n",
    "- **Simple:** One-step attack\n",
    "- **Effective:** Often succeeds\n",
    "\n",
    "**Reference:** Goodfellow et al. (2014) - \"Explaining and Harnessing Adversarial Examples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simple model for demonstration\n",
    "model = resnet18(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ImageNet normalization\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - mean) / std\n",
    "\n",
    "def denormalize(x):\n",
    "    return x * std + mean\n",
    "\n",
    "print(\"‚úÖ Model loaded (ResNet-18 pre-trained on ImageNet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM Attack Implementation\n",
    "def fgsm_attack(image, epsilon, model, target=None):\n",
    "    \"\"\"\n",
    "    Fast Gradient Sign Method attack\n",
    "    \n",
    "    Args:\n",
    "        image: Input image tensor [1, 3, H, W]\n",
    "        epsilon: Perturbation magnitude\n",
    "        model: Target model\n",
    "        target: Target class (if None, untargeted attack)\n",
    "    \n",
    "    Returns:\n",
    "        adversarial image\n",
    "    \"\"\"\n",
    "    # Ensure image requires grad\n",
    "    image = image.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(normalize(image))\n",
    "    \n",
    "    # Compute loss\n",
    "    if target is None:\n",
    "        # Untargeted: maximize loss for true class\n",
    "        pred = output.argmax(dim=1)\n",
    "        loss = F.cross_entropy(output, pred)\n",
    "    else:\n",
    "        # Targeted: minimize loss for target class\n",
    "        loss = -F.cross_entropy(output, torch.tensor([target]).to(device))\n",
    "    \n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradient sign\n",
    "    grad_sign = image.grad.sign()\n",
    "    \n",
    "    # Create adversarial example\n",
    "    adversarial = image + epsilon * grad_sign\n",
    "    adversarial = torch.clamp(adversarial, 0, 1)  # Keep in valid range\n",
    "    \n",
    "    return adversarial.detach()\n",
    "\n",
    "print(\"‚úÖ FGSM attack function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FGSM attack on a sample image\n",
    "# Create a synthetic \"traffic sign\" image for demo\n",
    "torch.manual_seed(42)\n",
    "test_image = torch.rand(1, 3, 224, 224).to(device) * 0.5 + 0.25\n",
    "\n",
    "# Original prediction\n",
    "with torch.no_grad():\n",
    "    orig_output = model(normalize(test_image))\n",
    "    orig_pred = orig_output.argmax(dim=1).item()\n",
    "    orig_conf = F.softmax(orig_output, dim=1).max().item()\n",
    "\n",
    "print(f\"Original prediction: Class {orig_pred}, Confidence: {orig_conf:.3f}\")\n",
    "\n",
    "# Generate adversarial examples with different epsilon values\n",
    "epsilons = [0.0, 0.01, 0.03, 0.05, 0.1]\n",
    "results = []\n",
    "\n",
    "fig, axes = plt.subplots(1, len(epsilons), figsize=(16, 3))\n",
    "\n",
    "for i, eps in enumerate(epsilons):\n",
    "    if eps == 0:\n",
    "        adv_image = test_image\n",
    "    else:\n",
    "        adv_image = fgsm_attack(test_image, eps, model)\n",
    "    \n",
    "    # Prediction on adversarial example\n",
    "    with torch.no_grad():\n",
    "        adv_output = model(normalize(adv_image))\n",
    "        adv_pred = adv_output.argmax(dim=1).item()\n",
    "        adv_conf = F.softmax(adv_output, dim=1).max().item()\n",
    "    \n",
    "    success = adv_pred != orig_pred\n",
    "    results.append({'Epsilon': eps, 'Success': success, 'Pred': adv_pred, 'Conf': adv_conf})\n",
    "    \n",
    "    # Visualize\n",
    "    img_np = adv_image.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    axes[i].imshow(img_np)\n",
    "    color = 'red' if success else 'green'\n",
    "    axes[i].set_title(f'Œµ={eps}\\nClass {adv_pred}\\nConf: {adv_conf:.2f}', \n",
    "                      fontsize=10, fontweight='bold', color=color)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    if success:\n",
    "        axes[i].add_patch(plt.Rectangle((0, 0), 224, 224, fill=False, \n",
    "                                        edgecolor='red', linewidth=4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\nüìä Attack Success Rate:\")\n",
    "success_rate = results_df[results_df['Epsilon'] > 0]['Success'].mean() * 100\n",
    "print(f\"   {success_rate:.1f}% of adversarial examples caused misclassification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Digital Attack 2: PGD (Projected Gradient Descent)\n",
    "\n",
    "**Idea:** Iterative version of FGSM (stronger attack)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Start from original image\n",
    "2. Repeat N iterations:\n",
    "   - Apply FGSM with small step size\n",
    "   - Project back to epsilon-ball around original\n",
    "3. Return final adversarial example\n",
    "\n",
    "**Properties:**\n",
    "- **Stronger:** More iterations find better perturbations\n",
    "- **Slower:** Multiple gradient computations\n",
    "- **More robust:** Better success rate\n",
    "\n",
    "**Reference:** Madry et al. (2018) - \"Towards Deep Learning Models Resistant to Adversarial Attacks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD Attack Implementation\n",
    "def pgd_attack(image, epsilon, model, alpha=0.01, num_iter=10, target=None):\n",
    "    \"\"\"\n",
    "    Projected Gradient Descent attack\n",
    "    \n",
    "    Args:\n",
    "        image: Input image tensor\n",
    "        epsilon: Maximum perturbation\n",
    "        model: Target model\n",
    "        alpha: Step size per iteration\n",
    "        num_iter: Number of iterations\n",
    "        target: Target class (if None, untargeted)\n",
    "    \"\"\"\n",
    "    original_image = image.clone().detach()\n",
    "    adversarial = image.clone().detach()\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        adversarial.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(normalize(adversarial))\n",
    "        \n",
    "        # Compute loss\n",
    "        if target is None:\n",
    "            pred = output.argmax(dim=1)\n",
    "            loss = F.cross_entropy(output, pred)\n",
    "        else:\n",
    "            loss = -F.cross_entropy(output, torch.tensor([target]).to(device))\n",
    "        \n",
    "        # Backward\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update adversarial example\n",
    "        grad_sign = adversarial.grad.sign()\n",
    "        adversarial = adversarial.detach() + alpha * grad_sign\n",
    "        \n",
    "        # Project back to epsilon-ball\n",
    "        perturbation = adversarial - original_image\n",
    "        perturbation = torch.clamp(perturbation, -epsilon, epsilon)\n",
    "        adversarial = original_image + perturbation\n",
    "        adversarial = torch.clamp(adversarial, 0, 1)\n",
    "    \n",
    "    return adversarial.detach()\n",
    "\n",
    "print(\"‚úÖ PGD attack function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare FGSM vs PGD\n",
    "epsilon = 0.03\n",
    "\n",
    "# Generate attacks\n",
    "fgsm_adv = fgsm_attack(test_image, epsilon, model)\n",
    "pgd_adv = pgd_attack(test_image, epsilon, model, alpha=0.007, num_iter=10)\n",
    "\n",
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    fgsm_output = model(normalize(fgsm_adv))\n",
    "    fgsm_pred = fgsm_output.argmax(dim=1).item()\n",
    "    fgsm_conf = F.softmax(fgsm_output, dim=1).max().item()\n",
    "    \n",
    "    pgd_output = model(normalize(pgd_adv))\n",
    "    pgd_pred = pgd_output.argmax(dim=1).item()\n",
    "    pgd_conf = F.softmax(pgd_output, dim=1).max().item()\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "images = [test_image, fgsm_adv, pgd_adv]\n",
    "titles = [\n",
    "    f'Original\\nClass {orig_pred}\\nConf: {orig_conf:.2f}',\n",
    "    f'FGSM (Œµ={epsilon})\\nClass {fgsm_pred}\\nConf: {fgsm_conf:.2f}',\n",
    "    f'PGD (Œµ={epsilon})\\nClass {pgd_pred}\\nConf: {pgd_conf:.2f}'\n",
    "]\n",
    "colors = ['green', 'orange' if fgsm_pred != orig_pred else 'green', \n",
    "          'red' if pgd_pred != orig_pred else 'green']\n",
    "\n",
    "for i, (img, title, color) in enumerate(zip(images, titles, colors)):\n",
    "    img_np = img.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    axes[i].imshow(img_np)\n",
    "    axes[i].set_title(title, fontsize=11, fontweight='bold', color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(f\"   Original: Class {orig_pred}\")\n",
    "print(f\"   FGSM: Class {fgsm_pred} {'‚úó Failed' if fgsm_pred == orig_pred else '‚úì Success'}\")\n",
    "print(f\"   PGD: Class {pgd_pred} {'‚úó Failed' if pgd_pred == orig_pred else '‚úì Success'}\")\n",
    "print(\"\\nüí° PGD typically has higher attack success rate than FGSM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Physical Adversarial Attacks\n",
    "\n",
    "**Challenge:** Digital attacks work on raw pixels, but real world is different!\n",
    "\n",
    "### Physical Attack Considerations\n",
    "1. **Printing:** Perturbations must survive printer limitations\n",
    "2. **Viewing angle:** Must work from multiple angles\n",
    "3. **Distance:** Must work at various distances\n",
    "4. **Lighting:** Must work under different lighting conditions\n",
    "5. **Weather:** Rain, fog, etc. affect appearance\n",
    "\n",
    "### Adversarial Patch Attack\n",
    "\n",
    "**Idea:** Instead of perturbing entire image, place a **patch** (sticker) on object\n",
    "\n",
    "**Famous Example:** Adversarial stickers on stop sign\n",
    "- Paper: Eykholt et al. (2018) - \"Robust Physical-World Attacks on Deep Learning Visual Classification\"\n",
    "- Result: Stop sign misclassified as \"Speed Limit 45\"\n",
    "- Method: Small stickers placed on sign\n",
    "\n",
    "**Danger for AVs:**\n",
    "- Attacker can physically modify traffic signs\n",
    "- AV misinterprets sign\n",
    "- Could cause vehicle to not stop ‚Üí collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate adversarial patch attack\ndef create_adversarial_patch(size=(50, 50, 3)):\n    \"\"\"Create a random adversarial patch\"\"\"\n    # In practice, this would be optimized via gradient descent\n    # Here we simulate with random pattern\n    np.random.seed(42)\n    patch = np.random.rand(*size)\n    return patch\n\ndef apply_patch(image, patch, position):\n    \"\"\"Apply patch to image at given position\"\"\"\n    image_with_patch = image.copy()\n    x, y = position\n    h, w = patch.shape[:2]\n    \n    # Check bounds\n    if x + w <= image.shape[1] and y + h <= image.shape[0]:\n        image_with_patch[y:y+h, x:x+w] = patch\n    \n    return image_with_patch\n\n# Create synthetic traffic sign (convert to uint8 for cv2)\nsign_image = (np.ones((224, 224, 3)) * 0.8 * 255).astype(np.uint8)\n# Draw red octagon (stop sign shape)\ncv2.circle(sign_image, (112, 112), 80, (200, 25, 25), -1)\ncv2.putText(sign_image, 'STOP', (70, 125), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 4)\n\n# Convert back to float for display\nsign_image = sign_image.astype(np.float64) / 255.0\n\n# Create adversarial patch\nadv_patch = create_adversarial_patch((40, 40, 3))\n\n# Apply patch at different positions\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\npositions = [(20, 20), (160, 20), (90, 140), (90, 90)]\nfor i, pos in enumerate(positions):\n    patched_sign = apply_patch(sign_image, adv_patch, pos)\n    \n    axes[i].imshow(patched_sign)\n    axes[i].set_title(f'Patch at {pos}\\n\"Speed Limit 45\" (simulated)', \n                      fontsize=10, fontweight='bold', color='red')\n    axes[i].axis('off')\n    \n    # Highlight patch\n    rect = plt.Rectangle(pos, 40, 40, fill=False, edgecolor='yellow', linewidth=3)\n    axes[i].add_patch(rect)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"‚ö†Ô∏è Physical Attack Scenario:\")\nprint(\"   Attacker places small stickers on stop sign\")\nprint(\"   AV perception system misclassifies sign\")\nprint(\"   Vehicle fails to stop ‚Üí Potential collision!\")\nprint(\"\\nüí° Defense: Anomaly detection, multiple viewpoints, sensor fusion\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Sensor Spoofing Attacks\n",
    "\n",
    "Beyond camera attacks, adversaries can target other sensors:\n",
    "\n",
    "### LiDAR Spoofing\n",
    "- **Attack:** Use laser to inject fake returns\n",
    "- **Result:** Phantom objects appear\n",
    "- **Example:** Make AV \"see\" pedestrian that doesn't exist ‚Üí Emergency brake\n",
    "\n",
    "### Camera Blinding\n",
    "- **Attack:** Use laser to saturate camera sensor\n",
    "- **Result:** Temporary blindness\n",
    "- **Danger:** Critical objects not detected\n",
    "\n",
    "### GPS Spoofing\n",
    "- **Attack:** Broadcast fake GPS signals\n",
    "- **Result:** Vehicle thinks it's in wrong location\n",
    "- **Danger:** Wrong map data, navigation errors\n",
    "\n",
    "### Radar Jamming\n",
    "- **Attack:** Broadcast interference on radar frequency\n",
    "- **Result:** Radar sees noise instead of objects\n",
    "- **Danger:** Miss vehicles, pedestrians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensor spoofing attacks\n",
    "sensor_attacks = pd.DataFrame({\n",
    "    'Sensor': ['Camera', 'Camera', 'LiDAR', 'LiDAR', 'Radar', 'GPS'],\n",
    "    'Attack_Type': ['Adversarial patch', 'Laser blinding', 'Spoofing', 'Interference', 'Jamming', 'Spoofing'],\n",
    "    'Difficulty': ['Medium', 'Easy', 'Hard', 'Medium', 'Medium', 'Hard'],\n",
    "    'Impact': ['Misclassification', 'Temporary blindness', 'Phantom objects', 'Lost data', 'Lost data', 'Wrong location'],\n",
    "    'Risk': ['High', 'Critical', 'Critical', 'High', 'High', 'High'],\n",
    "    'Countermeasure': [\n",
    "        'Adversarial training, ensemble',\n",
    "        'Filter detection, redundancy',\n",
    "        'Signal authentication, anomaly detection',\n",
    "        'Frequency hopping, shielding',\n",
    "        'Frequency diversity, sensor fusion',\n",
    "        'INS backup, plausibility checks'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(sensor_attacks)\n",
    "\n",
    "# Visualize attack surface\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sensors = sensor_attacks['Sensor'].unique()\n",
    "attack_counts = sensor_attacks.groupby('Sensor').size()\n",
    "risk_levels = sensor_attacks.groupby('Sensor')['Risk'].apply(lambda x: (x == 'Critical').sum())\n",
    "\n",
    "x = np.arange(len(sensors))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, attack_counts, width, label='Total Attack Types', alpha=0.8, color='orange')\n",
    "bars2 = ax.bar(x + width/2, risk_levels, width, label='Critical Risk Attacks', alpha=0.8, color='red')\n",
    "\n",
    "ax.set_xlabel('Sensor Type', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Number of Attack Types', fontsize=12, fontweight='bold')\n",
    "ax.set_title('AV Sensor Attack Surface', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sensors)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Key Insight: All sensors vulnerable to attacks!\")\n",
    "print(\"   Defense: Multi-sensor fusion + anomaly detection + authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Defense Mechanisms\n",
    "\n",
    "### Defense 1: Adversarial Training\n",
    "\n",
    "**Idea:** Train model on adversarial examples\n",
    "\n",
    "**Procedure:**\n",
    "1. Generate adversarial examples during training\n",
    "2. Include in training dataset\n",
    "3. Model learns to be robust\n",
    "\n",
    "**Pros:**\n",
    "- Effective against known attacks\n",
    "- Improves model robustness\n",
    "\n",
    "**Cons:**\n",
    "- Slower training\n",
    "- May reduce accuracy on clean data\n",
    "- Doesn't defend against novel attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate adversarial training (conceptual)\n",
    "def adversarial_training_epoch(model, train_loader, optimizer, epsilon=0.03):\n",
    "    \"\"\"\n",
    "    One epoch of adversarial training\n",
    "    \n",
    "    In each batch:\n",
    "    1. Generate adversarial examples\n",
    "    2. Train on both clean and adversarial\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        adv_images = fgsm_attack(images, epsilon, model)\n",
    "        \n",
    "        # Combine clean and adversarial\n",
    "        combined_images = torch.cat([images, adv_images], dim=0)\n",
    "        combined_labels = torch.cat([labels, labels], dim=0)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(normalize(combined_images))\n",
    "        loss = F.cross_entropy(outputs, combined_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "print(\"‚úÖ Adversarial training procedure defined\")\n",
    "print(\"\\nüí° In production:\")\n",
    "print(\"   - Train for multiple epochs with adversarial examples\")\n",
    "print(\"   - Use stronger attacks (PGD) for training\")\n",
    "print(\"   - Balance clean and adversarial accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense 2: Input Validation\n",
    "\n",
    "**Idea:** Detect adversarial inputs before feeding to model\n",
    "\n",
    "**Methods:**\n",
    "1. **Statistical tests:** Check for unusual pixel patterns\n",
    "2. **Reconstruction:** Use autoencoder to \"clean\" input\n",
    "3. **Ensemble disagreement:** Multiple models should agree\n",
    "4. **Feature squeezing:** Reduce color depth, spatial resolution\n",
    "\n",
    "**Example: Simple pixel statistics check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple input validation detector\n",
    "def detect_adversarial_simple(image, clean_images):\n",
    "    \"\"\"\n",
    "    Simple adversarial detector based on pixel statistics\n",
    "    \n",
    "    Check if image statistics deviate from clean distribution\n",
    "    \"\"\"\n",
    "    # Compute statistics\n",
    "    img_mean = image.mean().item()\n",
    "    img_std = image.std().item()\n",
    "    img_max = image.max().item()\n",
    "    img_min = image.min().item()\n",
    "    \n",
    "    # Compare to clean distribution\n",
    "    clean_mean = clean_images.mean().item()\n",
    "    clean_std = clean_images.std().item()\n",
    "    \n",
    "    # Simple threshold-based detection\n",
    "    mean_diff = abs(img_mean - clean_mean)\n",
    "    std_diff = abs(img_std - clean_std)\n",
    "    \n",
    "    # Flag if statistics deviate significantly\n",
    "    threshold = 0.1\n",
    "    is_adversarial = (mean_diff > threshold) or (std_diff > threshold)\n",
    "    \n",
    "    return is_adversarial, {'mean_diff': mean_diff, 'std_diff': std_diff}\n",
    "\n",
    "# Test detector\n",
    "clean_batch = torch.rand(10, 3, 224, 224).to(device) * 0.5 + 0.25\n",
    "\n",
    "# Test on clean vs adversarial\n",
    "is_adv_clean, stats_clean = detect_adversarial_simple(test_image, clean_batch)\n",
    "is_adv_fgsm, stats_fgsm = detect_adversarial_simple(fgsm_adv, clean_batch)\n",
    "is_adv_pgd, stats_pgd = detect_adversarial_simple(pgd_adv, clean_batch)\n",
    "\n",
    "print(\"Input Validation Results:\\n\" + \"=\"*50)\n",
    "print(f\"Clean image: {'‚ö†Ô∏è Flagged' if is_adv_clean else '‚úÖ Passed'}\")\n",
    "print(f\"  Stats: {stats_clean}\")\n",
    "print(f\"\\nFGSM adversarial: {'‚ö†Ô∏è Flagged' if is_adv_fgsm else '‚úÖ Passed'}\")\n",
    "print(f\"  Stats: {stats_fgsm}\")\n",
    "print(f\"\\nPGD adversarial: {'‚ö†Ô∏è Flagged' if is_adv_pgd else '‚úÖ Passed'}\")\n",
    "print(f\"  Stats: {stats_pgd}\")\n",
    "\n",
    "print(\"\\nüí° Note: Real detectors use more sophisticated methods:\")\n",
    "print(\"   - Deep learning-based detectors\")\n",
    "print(\"   - Autoencoder reconstruction error\")\n",
    "print(\"   - Ensemble model agreement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense 3: Sensor Fusion\n",
    "\n",
    "**Idea:** Multiple sensors provide redundancy\n",
    "\n",
    "**Strategy:**\n",
    "- Camera may be fooled by adversarial patch\n",
    "- BUT LiDAR still detects stop sign correctly\n",
    "- Fusion logic: Trust LiDAR when disagreement\n",
    "\n",
    "**Benefit:** Attacking multiple sensors simultaneously is much harder\n",
    "\n",
    "### Defense 4: Certified Defenses\n",
    "\n",
    "**Idea:** Provable robustness guarantees\n",
    "\n",
    "**Methods:**\n",
    "- **Randomized smoothing:** Add noise, average predictions\n",
    "- **Interval bound propagation:** Compute guaranteed bounds\n",
    "- **Lipschitz constraints:** Limit model sensitivity\n",
    "\n",
    "**Trade-off:** Often reduce model accuracy for guaranteed robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ ISO/SAE 21434: Cybersecurity Standard\n",
    "\n",
    "**ISO/SAE 21434:2021** - Road vehicles ‚Äî Cybersecurity engineering\n",
    "\n",
    "### Key Requirements\n",
    "\n",
    "**1. Threat Analysis and Risk Assessment (TARA):**\n",
    "- Identify attack vectors\n",
    "- Assess likelihood and impact\n",
    "- Prioritize mitigation\n",
    "\n",
    "**2. Security-by-Design:**\n",
    "- Security considered from concept phase\n",
    "- Secure architecture\n",
    "- Regular security reviews\n",
    "\n",
    "**3. Security Testing:**\n",
    "- Penetration testing\n",
    "- Vulnerability scanning\n",
    "- Adversarial attack simulation\n",
    "\n",
    "**4. Incident Response:**\n",
    "- Monitor for attacks in deployment\n",
    "- Rapid response to discovered vulnerabilities\n",
    "- OTA updates for patches\n",
    "\n",
    "### Application to Adversarial Attacks\n",
    "\n",
    "**Threat:** Adversarial examples on traffic signs\n",
    "\n",
    "**Risk Assessment:**\n",
    "- **Likelihood:** Medium (requires physical access)\n",
    "- **Impact:** Critical (collision)\n",
    "- **Risk:** HIGH\n",
    "\n",
    "**Mitigation:**\n",
    "1. Adversarial training (design)\n",
    "2. Input validation (runtime)\n",
    "3. Sensor fusion (redundancy)\n",
    "4. Anomaly detection (monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISO/SAE 21434 Threat Analysis Example\n",
    "threats = pd.DataFrame({\n",
    "    'Threat': [\n",
    "        'Adversarial patch on stop sign',\n",
    "        'LiDAR spoofing',\n",
    "        'Camera laser blinding',\n",
    "        'GPS spoofing',\n",
    "        'CAN bus injection',\n",
    "        'OTA update compromise'\n",
    "    ],\n",
    "    'Attack_Vector': [\n",
    "        'Physical modification',\n",
    "        'Laser injection',\n",
    "        'Laser attack',\n",
    "        'RF spoofing',\n",
    "        'Physical access to bus',\n",
    "        'Network attack'\n",
    "    ],\n",
    "    'Likelihood': ['Medium', 'Low', 'Medium', 'Low', 'Low', 'Low'],\n",
    "    'Impact': ['Critical', 'Critical', 'Critical', 'High', 'Critical', 'Critical'],\n",
    "    'Risk_Level': ['HIGH', 'MEDIUM', 'HIGH', 'MEDIUM', 'MEDIUM', 'MEDIUM'],\n",
    "    'Mitigation_Status': ['‚ö†Ô∏è Partial', '‚ùå None', '‚ö†Ô∏è Partial', '‚úÖ Yes', '‚úÖ Yes', '‚úÖ Yes'],\n",
    "    'Required_Actions': [\n",
    "        'Deploy adversarial training',\n",
    "        'Add signal authentication',\n",
    "        'Add bright light detection',\n",
    "        'Strengthen INS backup',\n",
    "        'Already secured',\n",
    "        'Already secured'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(threats)\n",
    "\n",
    "# Priority matrix\n",
    "high_risk = threats[threats['Risk_Level'] == 'HIGH']\n",
    "not_mitigated = high_risk[high_risk['Mitigation_Status'] != '‚úÖ Yes']\n",
    "\n",
    "print(\"\\nüö® HIGH PRIORITY SECURITY ACTIONS:\\n\" + \"=\"*60)\n",
    "for idx, row in not_mitigated.iterrows():\n",
    "    print(f\"\\nThreat: {row['Threat']}\")\n",
    "    print(f\"  Risk Level: {row['Risk_Level']}\")\n",
    "    print(f\"  Current Status: {row['Mitigation_Status']}\")\n",
    "    print(f\"  Required Action: {row['Required_Actions']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° ISO/SAE 21434 requires documented risk assessment and mitigation plan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úèÔ∏è Exercise: Design Security Requirements\n",
    "\n",
    "**Task:** For the campus shuttle, design security requirements against adversarial attacks.\n",
    "\n",
    "**Consider:**\n",
    "1. What are the attack vectors?\n",
    "2. Which attacks are most feasible?\n",
    "3. What defenses would you implement?\n",
    "4. How would you test adversarial robustness?\n",
    "5. What monitoring is needed in deployment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete your security design\n",
    "\n",
    "your_security_design = pd.DataFrame({\n",
    "    'Threat': [\n",
    "        # Identify threats for campus shuttle\n",
    "        # Example: 'Adversarial sticker on stop sign at campus entrance'\n",
    "    ],\n",
    "    'Likelihood': [],  # Low, Medium, High\n",
    "    'Impact': [],      # Low, Medium, High, Critical\n",
    "    'Defense': [],     # Your proposed defense\n",
    "    'Testing': []      # How to test this defense\n",
    "})\n",
    "\n",
    "if len(your_security_design) > 0:\n",
    "    display(your_security_design)\n",
    "else:\n",
    "    print(\"Add your security analysis above!\")\n",
    "\n",
    "print(\"\\nü§î Reflection Questions:\")\n",
    "print(\"   1. How do you balance security and performance?\")\n",
    "print(\"   2. What's the cost of false positives (flagging clean inputs)?\")\n",
    "print(\"   3. How do you update defenses as new attacks emerge?\")\n",
    "print(\"   4. What role does sensor fusion play in security?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### Adversarial Attacks are Real\n",
    "- **Digital attacks:** FGSM, PGD cause misclassification with tiny perturbations\n",
    "- **Physical attacks:** Adversarial patches work in real world\n",
    "- **Sensor attacks:** All sensors (camera, LiDAR, radar, GPS) vulnerable\n",
    "\n",
    "### Attack Characteristics\n",
    "- **Imperceptible:** Humans can't see perturbations\n",
    "- **Transferable:** Attacks work across different models\n",
    "- **Feasible:** Physical attacks demonstrated in research\n",
    "- **Safety-critical:** Can cause accidents in AVs\n",
    "\n",
    "### Defense Strategies\n",
    "1. **Adversarial training:** Train on adversarial examples\n",
    "2. **Input validation:** Detect adversarial inputs\n",
    "3. **Sensor fusion:** Multiple sensors provide redundancy\n",
    "4. **Certified defenses:** Provable robustness guarantees\n",
    "\n",
    "### ISO/SAE 21434 Compliance\n",
    "- **Threat analysis:** Identify and assess attack vectors\n",
    "- **Security-by-design:** Build in defenses from start\n",
    "- **Testing:** Simulate adversarial attacks\n",
    "- **Monitoring:** Detect attacks in deployment\n",
    "\n",
    "### Best Practices\n",
    "1. ‚úÖ **Defense-in-depth:** Multiple layers of security\n",
    "2. ‚úÖ **Sensor diversity:** Don't rely on single sensor\n",
    "3. ‚úÖ **Continuous testing:** Regular penetration testing\n",
    "4. ‚úÖ **Rapid response:** OTA updates for vulnerabilities\n",
    "5. ‚úÖ **Monitoring:** Detect anomalous inputs in deployment\n",
    "\n",
    "### Open Research Questions\n",
    "- How to guarantee robustness without sacrificing accuracy?\n",
    "- Can we detect all adversarial examples?\n",
    "- What about attacks we haven't thought of yet?\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Session 2 Complete!\n",
    "\n",
    "**Congratulations!** You've completed Session 2 on Failure Modes and Edge Cases.\n",
    "\n",
    "**What we covered:**\n",
    "1. ‚úÖ Real-world AV failure case studies\n",
    "2. ‚úÖ Out-of-distribution detection methods\n",
    "3. ‚úÖ Corner cases and long-tail scenarios\n",
    "4. ‚úÖ Adversarial attacks and defenses\n",
    "\n",
    "**Next:** Session 3 - Safety Standards and Verification\n",
    "- ISO 26262 (Functional Safety)\n",
    "- ISO 21448 (SOTIF)\n",
    "- ISO/SAE 21434 (Cybersecurity)\n",
    "- Verification and validation methods\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created by Milin Patel | Hochschule Kempten*  \n",
    "*Last updated: 2025-01-18*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}